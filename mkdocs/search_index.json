{
    "docs": [
        {
            "location": "/", 
            "text": "Copperlight Writes\n\n\n\n  \n2017-10-13\n\n  \nDiscuss\n\n\n\n\n\n\n\nMy name is \nMatthew Johnson\n and I am currently a Sr. DevOps Engineer at \nNetflix\n; I have been working in the DevOps space since I joined \nBlackboard\n in 2004.  The purpose of this site is to share notes and neat ways of solving problems.\n\n\nIf you have questions or comments on this site, file a GitHub issue and discussion will take place there.\n\n\nPopular Pages\n\n\n\n\nInstall the Latest Python Versions on Mac OSX\n\n\nHow I Built My Site\n\n\nNetflix Atlas Telemetry - A Platform Begets an Ecosystem\n\n\nAWS Credential Files for Java and Python\n\n\nUsing the Scala REPL to Configure Amazon SES Notifications\n\n\nAnsible Vault and SSH Key Distribution\n\n\nRunning Ansible Playbooks on Windows", 
            "title": "Home"
        }, 
        {
            "location": "/#copperlight-writes", 
            "text": "2017-10-13 \n   Discuss    My name is  Matthew Johnson  and I am currently a Sr. DevOps Engineer at  Netflix ; I have been working in the DevOps space since I joined  Blackboard  in 2004.  The purpose of this site is to share notes and neat ways of solving problems.  If you have questions or comments on this site, file a GitHub issue and discussion will take place there.", 
            "title": "Copperlight Writes"
        }, 
        {
            "location": "/#popular-pages", 
            "text": "Install the Latest Python Versions on Mac OSX  How I Built My Site  Netflix Atlas Telemetry - A Platform Begets an Ecosystem  AWS Credential Files for Java and Python  Using the Scala REPL to Configure Amazon SES Notifications  Ansible Vault and SSH Key Distribution  Running Ansible Playbooks on Windows", 
            "title": "Popular Pages"
        }, 
        {
            "location": "/ansible/ansible-vault-and-ssh-key-distribution/", 
            "text": "Ansible Vault and SSH Key Distribution\n\n\n\n  \n2014-06-30\n\n  \nDiscuss\n\n\n\n\n\n\n\nThere are two types of SSH key distribution discussed in this post: private keys on local hosts and\npublic keys on remote hosts.  SSH private key distribution is best used for setting up your own\nworkstation or possibly an \nAnsible Tower\n server.  In general, you\nshould not be distributing private keys widely; with a good SSH tunneling configuration and SSH\npublic key distribution, there should be no need for the private keys to be installed in more than\nfew places.  This configuration will show off a technique for configuring an SSH jump host bastion\nthat allows you to keep your private key on your own workstation; there is no need to have the SSH\nprivate key on the bastion host.\n\n\nFor the purpose of this post, I have generated a new SSH key pair to demonstrate this technique;\nthis keypair is used nowhere.  Part of the trick to making this work is that the private key needs\nto be base64 encoded so that line breaks are preserved when it is stored as a yaml string in the\n\nvars_files\n.  Template files are created for the public and private keys to preserve file change\ndetection.\n\n\nGenerate a new key pair:\n\n\n$ ssh-keygen -b \n2048\n -f junk_key -C junk\nGenerating public/private rsa key pair.\nEnter passphrase \n(\nempty \nfor\n no passphrase\n)\n:\nEnter same passphrase again:\nYour identification has been saved in junk_key.\nYour public key has been saved in junk_key.pub.\nThe key fingerprint is:\n\n94\n:94:ae:ac:c3:5e:ee:7d:fa:2c:cb:0f:ae:19:c8:99 junk\nThe key\ns randomart image is:\n+--\n[\n RSA \n2048\n]\n----+\n\n|\n        ..       \n|\n\n\n|\n       ...       \n|\n\n\n|\n       .o        \n|\n\n\n|\n       ..        \n|\n\n\n|\n     . .S        \n|\n\n\n|\n   . +o          \n|\n\n\n|\n   .E.o .        \n|\n\n\n|\n    +o *.o.      \n|\n\n\n|\n   ..o\n=\n.*B+      \n|\n\n+-----------------+\n\n/pre\n\n\n\n\n\n\nBase64 encode the private key:\n\n\nbase64 -i junk_key \n junk_key.b64\n\n\n\n\n\nCreate an Ansible vars_files yaml data file named \nssh_keys/ssh_key_vault.yml\n.  The\n\nssh_private_key\n variable should contain the base64 encoded private key and the \nssh_public_key\n\nvariable should contain the public key. Encrypt the file with \nansible-vault\n:\n\n\nssh_private_key\n:\n \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\n\n\n\nssh_public_key\n:\n \nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk\n\n\n\n\n\n\nansible-vault encrypt ssh_keys/ssh_key_vault.yml\nVault password:\nConfirm Vault password:\nEncryption successful\n\n\n\n\n\nCreate an inventory file named \ninventory\n, showing off the SSH jump host connection capability:\n\n\n[localhost]\n\n\nlocalhost ansible_connection\n=\nlocal\n\n\n\n[group-all:children]\n\n\ngroup-01\n\n\ngroup-02\n\n\n\n[group-01]\n\n\ni-00000001 ansible_ssh_host\n=\nbastion+192.168.1.1 ansible_ssh_user=remoteuser\n\n\ni-00000002 ansible_ssh_host\n=\nbastion+192.168.1.2 ansible_ssh_user=remoteuser\n\n\n\n[group-02]\n\n\ni-00000003 ansible_ssh_host\n=\nbastion+192.168.1.3 ansible_ssh_user=remoteuser\n\n\ni-00000004 ansible_ssh_host\n=\nbastion+192.168.1.4 ansible_ssh_user=remoteuser\n\n\n\n\n\n\nCreate a template file for the private key named \ntemplates/HOME_.ssh_junk\n:\n\n\n{{\nssh_private_key_decoded.stdout\n}}\n\n\n\n\n\n\nCreate a template file for the public key named \ntemplates/HOME_.ssh_junk.pub\n:\n\n\n{{\nssh_public_key\n}}\n\n\n\n\n\n\nCreate a template file for the SSH jump host configuration named \ntemplates/HOME_.ssh_config\n:\n\n\nHost *\n\n\n    ServerAliveInterval 30\n\n\n    ServerAliveCountMax 5\n\n\n\nHost bastion\n\n\n    User \n{{\nremote_user\n}}\n\n\n    IdentityFile ~/.ssh/junk\n\n\n    Hostname bastion\n\n\n\nHost bastion+*\n\n\n    User \n{{\nremote_user\n}}\n\n\n    IdentityFile ~/.ssh/junk\n\n\n    ProxyCommand ssh -T -a bastion nc $(echo %h |cut -d+ -f2) %p 2\n/dev/null\n\n\n    StrictHostKeyChecking no\n\n\n\n\n\n\nThis configuration assumes that you have a consistent remote username defined on the bastion server\nand your protected hosts.\n\n\nWrite a playbook to install the SSH key and configuration on your local workstation named\n\nconfig_local-ssh.yml\n:\n\n\n-\n \nname\n:\n \nconfigure local ssh\n\n  \nhosts\n:\n\n  \n-\n \nlocalhost\n\n  \ngather_facts\n:\n \nfalse\n\n  \nsudo\n:\n \nfalse\n\n  \nvars\n:\n\n    \nlocal_home\n:\n \n{{\n \nlookup(\nenv\n,\nHOME\n)\n \n}}\n\n    \nlocal_user\n:\n \n{{\n \nlookup(\nenv\n,\nUSER\n)\n \n}}\n\n    \nremote_user\n:\n \nremoteuser\n\n  \nvars_files\n:\n\n  \n-\n \nssh_keys/ssh_key_vault.yml\n\n  \ntasks\n:\n\n  \n-\n \nfile\n:\n \npath={{local_home}}/.ssh state=directory mode=0700 owner={{local_user}}\n\n\n  \n-\n \ntemplate\n:\n \nsrc=templates/HOME_.ssh_config dest={{local_home}}/.ssh/config mode=0644 owner={{local_user}} backup=yes\n\n\n  \n-\n \nshell\n:\n \necho {{ssh_private_key}} |base64 --decode\n\n    \nregister\n:\n \nssh_private_key_decoded\n\n\n  \n-\n \ntemplate\n:\n \nsrc=templates/HOME_.ssh_junk dest={{local_home}}/.ssh/junk mode=0600 owner={{local_user}}\n\n\n  \n-\n \ntemplate\n:\n \nsrc=templates/HOME_.ssh_junk.pub dest={{local_home}}/.ssh/junk.pub mode=0644 owner={{local_user}}\n\n\n\n\n\n\nRun the playbook to setup your local workstation with SSH keys and configuration:\n\n\nansible-playbook -i inventory config_local-ssh.yml --ask-vault-pass\nVault password:\n\n\n\n\n\nTest your SSH tunneling access to a remote host behind the bastion server:\n\n\nssh bastion+192.168.1.1\nssh bastion+192.168.1.1 \ndate; date \n /tmp/date.out\n\nscp bastion+192.168.1.1:/tmp/date.out .\n\n\n\n\n\nNotice that the hosts behind the bastion server are referenced in the SSH command the same way that\nthey are referenced in the Ansible inventory file.  The \"+\" character used as a separator was\nselected explicitly for its ability to be used interchangeably at the command line and in the\nAnsible inventory.  IP addresses are being used on the right hand side of the expression since the\nsecondary connection to the protected host relies on the name resolution capabilities of the first\nhost in the tunnel.  If you had a reliable dynamic DNS service that was keeping up with changes to\nthe protected hosts and was accessible to the bastion host, then you could use host names instead,\nsuch as \nbastion+webserver01\n.  This host selection technique can be extended to an Ansible dynamic\ninventory script, if you were running instances at a cloud provider such as AWS.  When you write a\n\ndynamic inventory script\n, the data format\nshould look like this:\n\n\n{\n\n    \ngroup-01\n:\n \n{\n\n        \nhosts\n:\n \n[\n\n            \ni-00000001\n,\n\n            \ni-00000002\n\n        \n]\n\n    \n},\n\n    \ngroup-02\n:\n \n{\n\n        \nhosts\n:\n \n[\n\n            \ni-00000003\n,\n\n            \ni-00000004\n\n        \n]\n\n    \n},\n\n    \ngroup-all\n:\n \n{\n\n        \nchildren\n:\n \n[\n\n            \ngroup-01\n,\n\n            \ngroup-02\n\n        \n]\n\n    \n},\n\n    \nlocalhost\n:\n \n[\n\n        \nlocalhost\n\n    \n],\n\n    \n_meta\n:\n \n{\n\n        \nhostvars\n:\n \n{\n\n            \ni-00000001\n:\n \n{\n\n                \nansible_ssh_host\n:\n \nbastion+192.168.1.1\n,\n\n                \nansible_ssh_user\n:\n \nremoteuser\n\n            \n},\n\n            \ni-00000002\n:\n \n{\n\n                \nansible_ssh_host\n:\n \nbastion+192.168.1.2\n,\n\n                \nansible_ssh_user\n:\n \nremoteuser\n\n            \n},\n\n            \ni-00000003\n:\n \n{\n\n                \nansible_ssh_host\n:\n \nbastion+192.168.1.3\n,\n\n                \nansible_ssh_user\n:\n \nremoteuser\n\n            \n},\n\n            \ni-00000004\n:\n \n{\n\n                \nansible_ssh_host\n:\n \nbastion+192.168.1.4\n,\n\n                \nansible_ssh_user\n:\n \nremoteuser\n\n            \n},\n\n            \nlocalhost\n:\n \n{\n\n                \nansible_connection\n:\n \nlocal\n\n            \n},\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nThe nice thing about this style of SSH configuration is that you can have multiple bastion hosts in\ndifferent locations and target the hosts behind each of them, provided that you give your bastion\nhosts different names.  The method of accessing them is the same between direct SSH connections and\nAnsible execution.  Once you have this infrastructure in place, you can start distributing public\nSSH keys to your protected hosts.\n\n\nWrite a \ntemplates/etc_sudoers\n file that grants NOPASSWD access to the sudo group:\n\n\nDefaults\n    \nenv_reset\n\n\nDefaults\n    \nmail_badpass\n\n\nDefaults\n    \nsecure_path\n=\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n\n\n# Host alias specification\n\n\n\n# User alias specification\n\n\n\n# Cmnd alias specification\n\n\n\n# User privilege specification\n\n\nroot\n    \nALL\n=\n(\nALL\n:\nALL\n)\n \nALL\n\n\n\n# Allow members of group sudo to execute any command\n\n\n%sudo\n   \nALL\n=\nNOPASSWD\n:\n \nALL\n\n\n\n#include\ndir /etc/sudoers.d\n\n\n\n\n\n\nWrite a playbook \nupdate_remote-ssh.yml\n to configure NOPASSWD sudo access for your remote user and\ndistribute SSH public keys on your remote hosts.  This will allow subsequent playbook execution to\noperate more easily against your remote hosts.  In order for this to work, the paramiko connection\ntype must be used initially, so that the password can be requested once and re-used across all hosts.\n\n\n-\n \nname\n:\n \nupdate remote ssh\n\n  \nhosts\n:\n\n  \n-\n \ngroup-all\n\n  \ngather_facts\n:\n \nfalse\n\n  \nsudo\n:\n \ntrue\n\n  \nconnection\n:\n \nparamiko\n\n  \nvars_files\n:\n\n  \n-\n \nssh_keys/ssh_key_vault.yml\n\n  \ntasks\n:\n\n  \n-\n \ncopy\n:\n \nsrc=templates/etc_sudoers dest=/etc/sudoers mode=0440 owner=root group=root\n\n\n  \n-\n \nuser\n:\n \nname=remoteuser groups=sudo shell=/bin/bash state=present\n\n\n  \n-\n \nauthorized_key\n:\n \nuser=remoteuser state=present key={{ssh_public_key}}\n\n\n\n\n\n\nRun an SSH configuration playbook against remote hosts through the SSH tunnel, providing the SSH\npassword, sudo password and vault password:\n\n\nansible-playbook -i inventory update_remote-ssh.yml --ask-pass --ask-sudo-pass --ask-vault-pass\nSSH password:\nsudo password \n[\ndefaults to SSH password\n]\n:\nVault password:\n\nPLAY \n[\nupdate ssh\n]\n *************************************************************\n\nTASK: \n[\ncopy \nsrc\n=\ntemplates/etc_sudoers \ndest\n=\n/etc/sudoers \nmode\n=\n0440\n \nowner\n=\nroot \ngroup\n=\nroot\n]\n ***\nok: \n[\ni-00000001\n]\n\nok: \n[\ni-00000002\n]\n\nok: \n[\ni-00000003\n]\n\nok: \n[\ni-00000004\n]\n\n\nTASK: \n[\nuser \nname\n=\nremoteuser \ngroups\n=\nsudo \nshell\n=\n/bin/bash \nstate\n=\npresent\n]\n ***\nok: \n[\ni-00000001\n]\n\nok: \n[\ni-00000002\n]\n\nok: \n[\ni-00000003\n]\n\nok: \n[\ni-00000004\n]\n\n\nTASK: \n[\nauthorized_key \nuser\n=\nremoteuser \nstate\n=\npresent \nkey\n=\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk\n]\n ***\nok: \n[\ni-00000001\n]\n\nok: \n[\ni-00000002\n]\n\nok: \n[\ni-00000003\n]\n\nok: \n[\ni-00000004\n]\n\n\n\nPLAY RECAP ********************************************************************\ni-00000001               : \nok\n=\n3\n    \nchanged\n=\n0\n    \nunreachable\n=\n0\n    \nfailed\n=\n0\n\ni-00000002               : \nok\n=\n3\n    \nchanged\n=\n0\n    \nunreachable\n=\n0\n    \nfailed\n=\n0\n\ni-00000003               : \nok\n=\n3\n    \nchanged\n=\n0\n    \nunreachable\n=\n0\n    \nfailed\n=\n0\n\ni-00000004               : \nok\n=\n3\n    \nchanged\n=\n0\n    \nunreachable\n=\n0\n    \nfailed\n=\n0\n\n\n\n\n\n\nThe best way to keep Ansible output concise is to run without verbosity -- only crank this up if you\nneed it to diagnose a problem.", 
            "title": "Ansible Vault and SSH Key Distribution"
        }, 
        {
            "location": "/ansible/ansible-vault-and-ssh-key-distribution/#ansible-vault-and-ssh-key-distribution", 
            "text": "2014-06-30 \n   Discuss    There are two types of SSH key distribution discussed in this post: private keys on local hosts and\npublic keys on remote hosts.  SSH private key distribution is best used for setting up your own\nworkstation or possibly an  Ansible Tower  server.  In general, you\nshould not be distributing private keys widely; with a good SSH tunneling configuration and SSH\npublic key distribution, there should be no need for the private keys to be installed in more than\nfew places.  This configuration will show off a technique for configuring an SSH jump host bastion\nthat allows you to keep your private key on your own workstation; there is no need to have the SSH\nprivate key on the bastion host.  For the purpose of this post, I have generated a new SSH key pair to demonstrate this technique;\nthis keypair is used nowhere.  Part of the trick to making this work is that the private key needs\nto be base64 encoded so that line breaks are preserved when it is stored as a yaml string in the vars_files .  Template files are created for the public and private keys to preserve file change\ndetection.  Generate a new key pair:  $ ssh-keygen -b  2048  -f junk_key -C junk\nGenerating public/private rsa key pair.\nEnter passphrase  ( empty  for  no passphrase ) :\nEnter same passphrase again:\nYour identification has been saved in junk_key.\nYour public key has been saved in junk_key.pub.\nThe key fingerprint is: 94 :94:ae:ac:c3:5e:ee:7d:fa:2c:cb:0f:ae:19:c8:99 junk\nThe key s randomart image is:\n+-- [  RSA  2048 ] ----+ |         ..        |  |        ...        |  |        .o         |  |        ..         |  |      . .S         |  |    . +o           |  |    .E.o .         |  |     +o *.o.       |  |    ..o = .*B+       | \n+-----------------+ /pre   Base64 encode the private key:  base64 -i junk_key   junk_key.b64  Create an Ansible vars_files yaml data file named  ssh_keys/ssh_key_vault.yml .  The ssh_private_key  variable should contain the base64 encoded private key and the  ssh_public_key \nvariable should contain the public key. Encrypt the file with  ansible-vault :  ssh_private_key :   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  ssh_public_key :   ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk   ansible-vault encrypt ssh_keys/ssh_key_vault.yml\nVault password:\nConfirm Vault password:\nEncryption successful  Create an inventory file named  inventory , showing off the SSH jump host connection capability:  [localhost]  localhost ansible_connection = local  [group-all:children]  group-01  group-02  [group-01]  i-00000001 ansible_ssh_host = bastion+192.168.1.1 ansible_ssh_user=remoteuser  i-00000002 ansible_ssh_host = bastion+192.168.1.2 ansible_ssh_user=remoteuser  [group-02]  i-00000003 ansible_ssh_host = bastion+192.168.1.3 ansible_ssh_user=remoteuser  i-00000004 ansible_ssh_host = bastion+192.168.1.4 ansible_ssh_user=remoteuser   Create a template file for the private key named  templates/HOME_.ssh_junk :  {{ ssh_private_key_decoded.stdout }}   Create a template file for the public key named  templates/HOME_.ssh_junk.pub :  {{ ssh_public_key }}   Create a template file for the SSH jump host configuration named  templates/HOME_.ssh_config :  Host *      ServerAliveInterval 30      ServerAliveCountMax 5  Host bastion      User  {{ remote_user }}      IdentityFile ~/.ssh/junk      Hostname bastion  Host bastion+*      User  {{ remote_user }}      IdentityFile ~/.ssh/junk      ProxyCommand ssh -T -a bastion nc $(echo %h |cut -d+ -f2) %p 2 /dev/null      StrictHostKeyChecking no   This configuration assumes that you have a consistent remote username defined on the bastion server\nand your protected hosts.  Write a playbook to install the SSH key and configuration on your local workstation named config_local-ssh.yml :  -   name :   configure local ssh \n   hosts : \n   -   localhost \n   gather_facts :   false \n   sudo :   false \n   vars : \n     local_home :   {{   lookup( env , HOME )   }} \n     local_user :   {{   lookup( env , USER )   }} \n     remote_user :   remoteuser \n   vars_files : \n   -   ssh_keys/ssh_key_vault.yml \n   tasks : \n   -   file :   path={{local_home}}/.ssh state=directory mode=0700 owner={{local_user}} \n\n   -   template :   src=templates/HOME_.ssh_config dest={{local_home}}/.ssh/config mode=0644 owner={{local_user}} backup=yes \n\n   -   shell :   echo {{ssh_private_key}} |base64 --decode \n     register :   ssh_private_key_decoded \n\n   -   template :   src=templates/HOME_.ssh_junk dest={{local_home}}/.ssh/junk mode=0600 owner={{local_user}} \n\n   -   template :   src=templates/HOME_.ssh_junk.pub dest={{local_home}}/.ssh/junk.pub mode=0644 owner={{local_user}}   Run the playbook to setup your local workstation with SSH keys and configuration:  ansible-playbook -i inventory config_local-ssh.yml --ask-vault-pass\nVault password:  Test your SSH tunneling access to a remote host behind the bastion server:  ssh bastion+192.168.1.1\nssh bastion+192.168.1.1  date; date   /tmp/date.out \nscp bastion+192.168.1.1:/tmp/date.out .  Notice that the hosts behind the bastion server are referenced in the SSH command the same way that\nthey are referenced in the Ansible inventory file.  The \"+\" character used as a separator was\nselected explicitly for its ability to be used interchangeably at the command line and in the\nAnsible inventory.  IP addresses are being used on the right hand side of the expression since the\nsecondary connection to the protected host relies on the name resolution capabilities of the first\nhost in the tunnel.  If you had a reliable dynamic DNS service that was keeping up with changes to\nthe protected hosts and was accessible to the bastion host, then you could use host names instead,\nsuch as  bastion+webserver01 .  This host selection technique can be extended to an Ansible dynamic\ninventory script, if you were running instances at a cloud provider such as AWS.  When you write a dynamic inventory script , the data format\nshould look like this:  { \n     group-01 :   { \n         hosts :   [ \n             i-00000001 , \n             i-00000002 \n         ] \n     }, \n     group-02 :   { \n         hosts :   [ \n             i-00000003 , \n             i-00000004 \n         ] \n     }, \n     group-all :   { \n         children :   [ \n             group-01 , \n             group-02 \n         ] \n     }, \n     localhost :   [ \n         localhost \n     ], \n     _meta :   { \n         hostvars :   { \n             i-00000001 :   { \n                 ansible_ssh_host :   bastion+192.168.1.1 , \n                 ansible_ssh_user :   remoteuser \n             }, \n             i-00000002 :   { \n                 ansible_ssh_host :   bastion+192.168.1.2 , \n                 ansible_ssh_user :   remoteuser \n             }, \n             i-00000003 :   { \n                 ansible_ssh_host :   bastion+192.168.1.3 , \n                 ansible_ssh_user :   remoteuser \n             }, \n             i-00000004 :   { \n                 ansible_ssh_host :   bastion+192.168.1.4 , \n                 ansible_ssh_user :   remoteuser \n             }, \n             localhost :   { \n                 ansible_connection :   local \n             }, \n         } \n     }  }   The nice thing about this style of SSH configuration is that you can have multiple bastion hosts in\ndifferent locations and target the hosts behind each of them, provided that you give your bastion\nhosts different names.  The method of accessing them is the same between direct SSH connections and\nAnsible execution.  Once you have this infrastructure in place, you can start distributing public\nSSH keys to your protected hosts.  Write a  templates/etc_sudoers  file that grants NOPASSWD access to the sudo group:  Defaults      env_reset  Defaults      mail_badpass  Defaults      secure_path = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin  # Host alias specification  # User alias specification  # Cmnd alias specification  # User privilege specification  root      ALL = ( ALL : ALL )   ALL  # Allow members of group sudo to execute any command  %sudo     ALL = NOPASSWD :   ALL  #include dir /etc/sudoers.d   Write a playbook  update_remote-ssh.yml  to configure NOPASSWD sudo access for your remote user and\ndistribute SSH public keys on your remote hosts.  This will allow subsequent playbook execution to\noperate more easily against your remote hosts.  In order for this to work, the paramiko connection\ntype must be used initially, so that the password can be requested once and re-used across all hosts.  -   name :   update remote ssh \n   hosts : \n   -   group-all \n   gather_facts :   false \n   sudo :   true \n   connection :   paramiko \n   vars_files : \n   -   ssh_keys/ssh_key_vault.yml \n   tasks : \n   -   copy :   src=templates/etc_sudoers dest=/etc/sudoers mode=0440 owner=root group=root \n\n   -   user :   name=remoteuser groups=sudo shell=/bin/bash state=present \n\n   -   authorized_key :   user=remoteuser state=present key={{ssh_public_key}}   Run an SSH configuration playbook against remote hosts through the SSH tunnel, providing the SSH\npassword, sudo password and vault password:  ansible-playbook -i inventory update_remote-ssh.yml --ask-pass --ask-sudo-pass --ask-vault-pass\nSSH password:\nsudo password  [ defaults to SSH password ] :\nVault password:\n\nPLAY  [ update ssh ]  *************************************************************\n\nTASK:  [ copy  src = templates/etc_sudoers  dest = /etc/sudoers  mode = 0440   owner = root  group = root ]  ***\nok:  [ i-00000001 ] \nok:  [ i-00000002 ] \nok:  [ i-00000003 ] \nok:  [ i-00000004 ] \n\nTASK:  [ user  name = remoteuser  groups = sudo  shell = /bin/bash  state = present ]  ***\nok:  [ i-00000001 ] \nok:  [ i-00000002 ] \nok:  [ i-00000003 ] \nok:  [ i-00000004 ] \n\nTASK:  [ authorized_key  user = remoteuser  state = present  key = ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk ]  ***\nok:  [ i-00000001 ] \nok:  [ i-00000002 ] \nok:  [ i-00000003 ] \nok:  [ i-00000004 ] \n\n\nPLAY RECAP ********************************************************************\ni-00000001               :  ok = 3      changed = 0      unreachable = 0      failed = 0 \ni-00000002               :  ok = 3      changed = 0      unreachable = 0      failed = 0 \ni-00000003               :  ok = 3      changed = 0      unreachable = 0      failed = 0 \ni-00000004               :  ok = 3      changed = 0      unreachable = 0      failed = 0   The best way to keep Ansible output concise is to run without verbosity -- only crank this up if you\nneed it to diagnose a problem.", 
            "title": "Ansible Vault and SSH Key Distribution"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/", 
            "text": "Running Ansible Playbooks on Windows\n\n\n\n  \n2014-06-29\n\n  \nDiscuss\n\n\n\n\n\n\n\nBut First, Some History\n\n\nIn early 2006, running almost a thousand servers for Blackboard Product Development that were evenly\ndistributed across Windows, Linux and Solaris, we needed an automation tool that would allow us to\nquickly deploy and configure the Blackboard Learning Management System (LMS).  The real sticking\npoint for us was managing the Windows ecosystem.\n\n\nThe \nfirst commit\n\nof PuppetLabs Puppet occurred in April 2005 and the \nfirst tagged release\n\nwas on Jan 3, 2006.  The \nfirst commit\n\nof OpsCode Chef occurred in March 2008 and the \nfirst tagged release\n\nwas on Jan 31, 2009.  Needless to say, the modern configuration management ecosystem was much sparser\nin 2006 then as compared to today.  Puppet was still very new and in the process of gaining mind share\nand adding functionality; it did not support Windows at first.  \nCFEngine\n was\navailable, but it also did not support Windows in the open source version.\n\n\nI worked with Dave Carter at the beginning of 2006 to develop our own in-house configuration\nmanagement system that we called Fusion.  It was based on \nAnt\n and\n\nAnt-Contrib\n.  Since Blackboard was a\nJava based application, we always had one or more versions of JDKs installed on our systems as a\npart of the imaging or virtual machine cloning process, so picking a tool that ran on the JDK made\nsense and offered us the platform independence we needed.  Dave developed a state machine with a\nsocket listener that would accept XML-formatted messages and then kick off various tasks.  I\ndeveloped a library and property inheritance hierarchy for the the system, along with a parallel job\nexecution client and added the set of scripts that deployed and configured the Blackboard LMS.  I\nfigured out that by cherry-picking a few key utilities out of the \nUnxUtils distribution\n,\nI could write Windows batch scripts in a manner similar to Linux bash scripts and this lent to a\nsomewhat manageable level of consistency between the disparate operating systems without completely\nabandoning the hooks we needed for Windows.\n\n\nMotivation\n\n\nAnyone who has spoken with me in the past year about my work knows that \nAnsible\n\nis hands-down my favorite piece of software tooling.  Using Ansible, I was able to effectively\nmanage 500-odd Ubuntu Linux systems at Blackboard, half of which were deployed in a VPC at AWS and\nhalf of which were deployed in the Blackboard Managed Hosting data centers.  Part of the challenge\nthat we had at Blackboard in the Product Development department is that 30-40% of the several\nthousand servers used for development and testing were Windows, which made it difficult to choose a\nsingle configuration management system to rule them all.  For the better part of a year, I kept\nsaying that it was a terrible shame that Ansible did not support Windows and that Opscode Chef would\nprobably be the best choice for configuration management of all systems, since it arguably had the\nbest support for that platform in 2013.  After meeting with \nMichael DeHaan\n,\ncreator and CTO of Ansible, at Blackboard headquarters, we talked through their rough plans for\nWindows support.  In short, it was something they wanted to be thoughtful about.  Some time later,\nwe had a hack day organized at Blackboard and I decided that I would attempt to develop an Ansible\nplaybook that could install and uninstall a JDK on Windows, using my previous experience with\nbuilding the Fusion configuration management system.\n\n\nIt turns out that this works. Quite well. Even though it wasn't intended to.\n\n\nAnsible Roadmap Update\n\n\nOn June 19, 2014, Michael DeHaan announced \nWindows Is Coming\n.\nPowerShell remoting is a far cleaner solution and I am looking forward to seeing it hit the release\nbranch, although I don't have to worry about Windows machines so much these days.  I learned this\nnifty fact from \nAnsible Weekly Issue 38\n;\nthis is not a bad way to keep up on the latest Ansible news.\n\n\nPre-Requisites\n\n\nWindows + Cygwin + SSHd + Python\n\n\nPlaybooks\n\n\nAnsible inventory file \nhosts\n:\n\n\n[w7x64-jf]\n\n\nw7x64-jf.pd.local\n\n\n\n\n\n\nJDK7 installation playbook \njdk7_install.yml\n:\n\n\n-\n \nname\n:\n \ninstall oracle jdk7\n\n  \nhosts\n:\n\n  \n-\n \nw7x64-jf\n\n  \nuser\n:\n \nadministrator\n\n  \ngather_facts\n:\n \nfalse\n\n  \nvars\n:\n\n    \nversion\n:\n \n7u45\n\n    \nbuild\n:\n \nb18\n\n    \nversion_padded\n:\n \n1.7.0_45\n\n    \ndodrootca2\n:\n \nc:\\\\jdk\\{{version_padded}}\\\\jre\\\\lib\\\\security\\\\dod.root-ca-2.pem\n\n    \ncacerts\n:\n \nc:\\\\jdk{{version_padded}}\\\\jre\\\\lib\\\\security\\\\cacerts\n\n  \ntasks\n:\n\n  \n-\n \ncommand\n:\n \nwget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/{{version}}-{{build}}/jdk-{{version}}-windows-x64.exe creates=/usr/local/src/jdk-{{version}}-windows-x64.exe\n\n\n  \n-\n \nfile\n:\n \npath=/usr/local/src/jdk-{{version}}-windows-x64.exe mode=0755\n\n\n  \n-\n \nshell\n:\n \n/usr/local/src/jdk-{{version}}-windows-x64.exe /s INSTALLDIR=c:\\\\jdk{{version_padded}} /INSTALLDIRPUBJRE=c:\\\\jre{{version_padded}} REBOOT=Suppress ADDLOCAL=ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE=0 SYSTRAY=0 SYSTRAY=0 /L c:\\\\cygwin\\\\usr\\\\local\\\\src\\\\jdk-install-log.txt creates=/cygdrive/c/jdk{{version_padded}}\n\n\n  \n-\n \ncopy\n:\n \nsrc=../certs/dod.root-ca-2.pem dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/dod.root-ca-2.pem\n\n\n  \n-\n \ncommand\n:\n \n/cygdrive/c/jdk{{version_padded}}/bin/keytool -import -trustcacerts -alias dodrootca2 -file {{dodrootca2}} -keystore $cacerts -storepass changeit -noprompt\n\n    \nignore_errors\n:\n \ntrue\n\n\n  \n-\n \ncopy\n:\n \nsrc=files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/US_export_policy.jar\n\n\n  \n-\n \ncopy\n:\n \nsrc=files/cygdrive_c_java_jre_lib_security_local_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/local_policy.jar\n\n\n  \n-\n \nshell\n:\n \n/cygdrive/c/jdk{{version_padded}}/bin/java -version 2\n1 |head -1 |awk \n{print $3}\n |sed -e \ns/\n//g\n\n    \nregister\n:\n \njava_version\n\n\n  \n-\n \nfail\n:\n \nmsg=\nThe Java version does not match the expected value {{ version_padded }}.\n\n    \nwhen\n:\n \n{{\n \njava_version.stdout\n \n}}\n \n!=\n \n{{\n \nversion_padded\n \n}}\n\n\n\n\n\n\nJDK7 uninstallation playbook \njdk7_uninstall.yml\n:\n\n\n-\n \nname\n:\n \nuninstall oracle jdk7\n\n  \nhosts\n:\n\n  \n-\n \nw7x64-jf\n\n  \nuser\n:\n \nadministrator\n\n  \ngather_facts\n:\n \nfalse\n\n  \nvars\n:\n\n    \nversion\n:\n \n7u45\n\n    \nversion_padded\n:\n \n1.7.0_45\n\n    \nversion_text\n:\n \n7\n \nUpdate\n \n45\n\n  \ntasks\n:\n\n  \n-\n \ntemplate\n:\n \nsrc=templates/usr_local_src_remove-programs.vbs dest=/usr/local/src/remove-programs.vbs\n\n\n  \n-\n \nshell\n:\n \ncscript remove-programs.vbs |grep \nJava {{version_text}} (64-bit)\n chdir=/usr/local/src\n\n    \nregister\n:\n \nresult\n\n    \nignore_errors\n:\n \ntrue\n\n\n  \n-\n \ncommand\n:\n \ncscript remove-programs.vbs /uninstall \nJava {{version_text}} (64-bit)\n chdir=/usr/local/src\n\n    \nwhen\n:\n \nresult|success\n\n\n  \n-\n \nshell\n:\n \ncscript remove-programs.vbs |grep \nJava SE Development Kit {{version_text}} (64-bit)\n chdir=/usr/local/src\n\n    \nregister\n:\n \nresult\n\n    \nignore_errors\n:\n \ntrue\n\n\n  \n-\n \ncommand\n:\n \ncscript remove-programs.vbs /uninstall \nJava SE Development Kit {{version_text}} (64-bit)\n chdir=/usr/local/src\n\n    \nwhen\n:\n \nresult|success\n\n\n  \n-\n \nfile\n:\n \npath={{item}} state=absent\n\n    \nwith_items\n:\n\n    \n-\n \n/usr/local/src/jdk-{{version}}-windows-x64.exe\n\n    \n-\n \n/usr/local/src/jdk-install-log.txt\n\n    \n-\n \n/usr/local/src/remove-programs.vbs\n\n\n\n\n\n\nThe \nremove-programs.vbs\n helper script:\n\n\nIf Wscript.Arguments.Count = 0 Then\n  inventory_software()\nElseIf Wscript.Arguments.Count = 2 Then\n  If Wscript.Arguments(0) = \n/uninstall\n Then\n    \nExpecting: cscript remove-programs.vbs /uninstall \nJava(TM) 6 Update 26\n\n    \nExpecting: cscript remove-programs.vbs /uninstall \nJava(TM) SE Development Kit 6 Update 26\n\n    uninstall_software(Wscript.Arguments(1))\n  Else\n    Wscript.Echo \nUsage: remove-programs.vbs [/uninstall \nsoftware\n]\n\n  End If\nElse\n  Wscript.Echo \nUsage: remove-programs.vbs [/uninstall \nsoftware\n]\n\nEnd If\n\nSub inventory_software()\n  strComputer = \n.\n\n\n  Set objWMIService = GetObject(\nwinmgmts:\n _\n    \n \n{impersonationLevel=impersonate}!\\\\\n _\n    \n strComputer \n \n\\root\\cimv2\n)\n  Set colSoftware = objWMIService.ExecQuery _\n    (\nSelect * from Win32_Product\n)\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo \nName: \n \n objSoftware.Name\n    \nWscript.Echo \nVersion: \n \n objSoftware.Version\n  Next\nEnd Sub\n\nSub inventory_java_software()\n  strComputer = \n.\n\n\n  Set objWMIService = GetObject(\nwinmgmts:\n _\n    \n \n{impersonationLevel=impersonate}!\\\\\n _\n    \n strComputer \n \n\\root\\cimv2\n)\n  Set colSoftware = objWMIService.ExecQuery _\n    (\nSelect * from Win32_Product \n _\n        \n \nWhere Name Like \nJava%\n)\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo \nName: \n \n objSoftware.Name\n    Wscript.Echo \nVersion: \n \n objSoftware.Version\n  Next\nEnd Sub\n\nSub uninstall_software(strApplicationName)\n  \nMake sure to run this with Administrator privileges\n  strComputer = \n.\n\n\n  Set objWMIService = GetObject(\nwinmgmts:\n _\n    \n \n{impersonationLevel=impersonate}!\\\\\n _\n    \n strComputer \n \n\\root\\cimv2\n)\n  Set colSoftware = objWMIService.ExecQuery _\n    (\nSelect * From Win32_Product Where Name = \n _\n    \n strApplicationName \n \n)\n\n  For Each objSoftware in colSoftware\n    objSoftware.Uninstall()\n  Next\nEnd Sub\n\n\n\n\n\nDemonstration\n\n\nFile system state before install:\n\n\nadministrator@W7X64-JF~\n$ ls -l /cygdrive/c \n|\negrep \njdk|jre\n\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Jun \n24\n \n2010\n jdk5\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk6\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk7\n\n\n\n\n\nInstall JDK7 on Windows:\n\n\ncopperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_install.yml\n\nPLAY \n[\ninstall oracle jdk7\n]\n ****************************************************\n\nTASK: \n[\ncommand\n wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24\n=\nhttp%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/7u45-b18/jdk-7u45-windows-x64.exe \ncreates\n=\n/usr/local/src/jdk-7u45-windows-x64.exe\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nfile \npath\n=\n/usr/local/src/jdk-7u45-windows-x64.exe \nmode\n=\n0755\n]\n ***********\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nshell /usr/local/src/jdk-7u45-windows-x64.exe /s \nINSTALLDIR\n=\nc:\n\\\\\njdk1.7.0_45 /INSTALLDIRPUBJRE\n=\nc:\n\\\\\njre1.7.0_45 \nREBOOT\n=\nSuppress \nADDLOCAL\n=\nToolsFeature,SourceFeature,PublicjreFeature \nAUTOUPDATE\n=\n0\n \nSYSTRAY\n=\n0\n \nSYSTRAY\n=\n0\n /L c:\n\\\\\ncygwin\n\\\\\nusr\n\\\\\nlocal\n\\\\\nsrc\n\\\\\njdk-install-log.txt \ncreates\n=\n/cygdrive/c/jdk1.7.0_45\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncopy \nsrc\n=\n../certs/dod.root-ca-2.pem \ndest\n=\n/cygdrive/c/jdk1.7.0_45/jre/lib/security/dod.root-ca-2.pem\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncommand\n /cygdrive/c/jdk1.7.0_45/bin/keytool -import -trustcacerts -alias dodrootca2 -file c:\n\\\\\njdk1.7.0_45\n\\\\\njre\n\\\\\nlib\n\\\\\nsecurity\n\\\\\ndod.root-ca-2.pem -keystore c:\n\\\\\njdk1.7.0_45\n\\\\\njre\n\\\\\nlib\n\\\\\nsecurity\n\\\\\ncacerts -storepass changeit -noprompt\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncopy \nsrc\n=\nfiles/cygdrive_c_java_jre_lib_security_US_export_policy.jar \ndest\n=\n/cygdrive/c/jdk1.7.0_45/jre/lib/security/US_export_policy.jar\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncopy \nsrc\n=\nfiles/cygdrive_c_java_jre_lib_security_local_policy.jar \ndest\n=\n/cygdrive/c/jdk1.7.0_45/jre/lib/security/local_policy.jar\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nshell /cygdrive/c/jdk1.7.0_45/bin/java -version \n2\n1\n \n|\nhead -1 \n|\nawk \n{print $3}\n \n|\nsed -e \ns/\n//g\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nfail \nmsg\n=\nThe Java version does not match the expected value 1.7.0_45.\n]\n ***\nskipping: \n[\nw7x64-jf.pd.local\n]\n\n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local : \nok\n=\n8\n \nchanged\n=\n8\n \nunreachable\n=\n0\n \nfailed\n=\n0\n\n\n\n\n\n\nFile system state after install and before uninstall:\n\n\nadministrator@W7X64-JF ~\n$ ls -l /cygdrive/c \n|\negrep \njdk|jre\n\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Dec \n12\n \n23\n:17 jdk1.7.0_45\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Jun \n24\n \n2010\n jdk5\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk6\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk7\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Dec \n12\n \n23\n:17 jre1.7.0_45\n\n\n\n\n\nUninstall JDK7 on Windows:\n\n\ncopperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_uninstall.yml\n\nPLAY \n[\nuninstall oracle jdk7\n]\n **************************************************\n\nTASK: \n[\ntemplate \nsrc\n=\ntemplates/usr_local_src_remove-programs.vbs \ndest\n=\n/usr/local/src/remove-programs.vbs\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nshell cscript remove-programs.vbs \n|\ngrep \nJava 7 Update 45 (64-bit)\n \nchdir\n=\n/usr/local/src\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncommand\n cscript remove-programs.vbs /uninstall \nJava 7 Update 45 (64-bit)\n \nchdir\n=\n/usr/local/src\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nshell cscript remove-programs.vbs \n|\ngrep \nJava SE Development Kit 7 Update 45 (64-bit)\n \nchdir\n=\n/usr/local/src\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncommand\n cscript remove-programs.vbs /uninstall \nJava SE Development Kit 7 Update 45 (64-bit)\n \nchdir\n=\n/usr/local/src\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nfile \npath\n=\n$item\n \nstate\n=\nabsent\n]\n ******************************************\nchanged: \n[\nw7x64-jf.pd.local\n]\n \n=\n \n(\nitem\n=\n/usr/local/src/jdk-7u45-windows-x64.exe\n)\n\nchanged: \n[\nw7x64-jf.pd.local\n]\n \n=\n \n(\nitem\n=\n/usr/local/src/jdk-install-log.txt\n)\n\nchanged: \n[\nw7x64-jf.pd.local\n]\n \n=\n \n(\nitem\n=\n/usr/local/src/remove-programs.vbs\n)\n\n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local : \nok\n=\n6\n \nchanged\n=\n6\n \nunreachable\n=\n0\n \nfailed\n=\n0\n\n\n\n\n\n\nFile system state after uninstall:\n\n\nadministrator@W7X64-JF ~\n$ ls -l /cygdrive/c \n|\negrep \njdk|jre\n\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Jun \n24\n \n2010\n jdk5\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk6\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk7", 
            "title": "Running Ansible Playbooks on Windows"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#running-ansible-playbooks-on-windows", 
            "text": "2014-06-29 \n   Discuss", 
            "title": "Running Ansible Playbooks on Windows"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#but-first-some-history", 
            "text": "In early 2006, running almost a thousand servers for Blackboard Product Development that were evenly\ndistributed across Windows, Linux and Solaris, we needed an automation tool that would allow us to\nquickly deploy and configure the Blackboard Learning Management System (LMS).  The real sticking\npoint for us was managing the Windows ecosystem.  The  first commit \nof PuppetLabs Puppet occurred in April 2005 and the  first tagged release \nwas on Jan 3, 2006.  The  first commit \nof OpsCode Chef occurred in March 2008 and the  first tagged release \nwas on Jan 31, 2009.  Needless to say, the modern configuration management ecosystem was much sparser\nin 2006 then as compared to today.  Puppet was still very new and in the process of gaining mind share\nand adding functionality; it did not support Windows at first.   CFEngine  was\navailable, but it also did not support Windows in the open source version.  I worked with Dave Carter at the beginning of 2006 to develop our own in-house configuration\nmanagement system that we called Fusion.  It was based on  Ant  and Ant-Contrib .  Since Blackboard was a\nJava based application, we always had one or more versions of JDKs installed on our systems as a\npart of the imaging or virtual machine cloning process, so picking a tool that ran on the JDK made\nsense and offered us the platform independence we needed.  Dave developed a state machine with a\nsocket listener that would accept XML-formatted messages and then kick off various tasks.  I\ndeveloped a library and property inheritance hierarchy for the the system, along with a parallel job\nexecution client and added the set of scripts that deployed and configured the Blackboard LMS.  I\nfigured out that by cherry-picking a few key utilities out of the  UnxUtils distribution ,\nI could write Windows batch scripts in a manner similar to Linux bash scripts and this lent to a\nsomewhat manageable level of consistency between the disparate operating systems without completely\nabandoning the hooks we needed for Windows.", 
            "title": "But First, Some History"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#motivation", 
            "text": "Anyone who has spoken with me in the past year about my work knows that  Ansible \nis hands-down my favorite piece of software tooling.  Using Ansible, I was able to effectively\nmanage 500-odd Ubuntu Linux systems at Blackboard, half of which were deployed in a VPC at AWS and\nhalf of which were deployed in the Blackboard Managed Hosting data centers.  Part of the challenge\nthat we had at Blackboard in the Product Development department is that 30-40% of the several\nthousand servers used for development and testing were Windows, which made it difficult to choose a\nsingle configuration management system to rule them all.  For the better part of a year, I kept\nsaying that it was a terrible shame that Ansible did not support Windows and that Opscode Chef would\nprobably be the best choice for configuration management of all systems, since it arguably had the\nbest support for that platform in 2013.  After meeting with  Michael DeHaan ,\ncreator and CTO of Ansible, at Blackboard headquarters, we talked through their rough plans for\nWindows support.  In short, it was something they wanted to be thoughtful about.  Some time later,\nwe had a hack day organized at Blackboard and I decided that I would attempt to develop an Ansible\nplaybook that could install and uninstall a JDK on Windows, using my previous experience with\nbuilding the Fusion configuration management system.  It turns out that this works. Quite well. Even though it wasn't intended to.", 
            "title": "Motivation"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#ansible-roadmap-update", 
            "text": "On June 19, 2014, Michael DeHaan announced  Windows Is Coming .\nPowerShell remoting is a far cleaner solution and I am looking forward to seeing it hit the release\nbranch, although I don't have to worry about Windows machines so much these days.  I learned this\nnifty fact from  Ansible Weekly Issue 38 ;\nthis is not a bad way to keep up on the latest Ansible news.", 
            "title": "Ansible Roadmap Update"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#pre-requisites", 
            "text": "Windows + Cygwin + SSHd + Python", 
            "title": "Pre-Requisites"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#playbooks", 
            "text": "Ansible inventory file  hosts :  [w7x64-jf]  w7x64-jf.pd.local   JDK7 installation playbook  jdk7_install.yml :  -   name :   install oracle jdk7 \n   hosts : \n   -   w7x64-jf \n   user :   administrator \n   gather_facts :   false \n   vars : \n     version :   7u45 \n     build :   b18 \n     version_padded :   1.7.0_45 \n     dodrootca2 :   c:\\\\jdk\\{{version_padded}}\\\\jre\\\\lib\\\\security\\\\dod.root-ca-2.pem \n     cacerts :   c:\\\\jdk{{version_padded}}\\\\jre\\\\lib\\\\security\\\\cacerts \n   tasks : \n   -   command :   wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/{{version}}-{{build}}/jdk-{{version}}-windows-x64.exe creates=/usr/local/src/jdk-{{version}}-windows-x64.exe \n\n   -   file :   path=/usr/local/src/jdk-{{version}}-windows-x64.exe mode=0755 \n\n   -   shell :   /usr/local/src/jdk-{{version}}-windows-x64.exe /s INSTALLDIR=c:\\\\jdk{{version_padded}} /INSTALLDIRPUBJRE=c:\\\\jre{{version_padded}} REBOOT=Suppress ADDLOCAL=ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE=0 SYSTRAY=0 SYSTRAY=0 /L c:\\\\cygwin\\\\usr\\\\local\\\\src\\\\jdk-install-log.txt creates=/cygdrive/c/jdk{{version_padded}} \n\n   -   copy :   src=../certs/dod.root-ca-2.pem dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/dod.root-ca-2.pem \n\n   -   command :   /cygdrive/c/jdk{{version_padded}}/bin/keytool -import -trustcacerts -alias dodrootca2 -file {{dodrootca2}} -keystore $cacerts -storepass changeit -noprompt \n     ignore_errors :   true \n\n   -   copy :   src=files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/US_export_policy.jar \n\n   -   copy :   src=files/cygdrive_c_java_jre_lib_security_local_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/local_policy.jar \n\n   -   shell :   /cygdrive/c/jdk{{version_padded}}/bin/java -version 2 1 |head -1 |awk  {print $3}  |sed -e  s/ //g \n     register :   java_version \n\n   -   fail :   msg= The Java version does not match the expected value {{ version_padded }}. \n     when :   {{   java_version.stdout   }}   !=   {{   version_padded   }}   JDK7 uninstallation playbook  jdk7_uninstall.yml :  -   name :   uninstall oracle jdk7 \n   hosts : \n   -   w7x64-jf \n   user :   administrator \n   gather_facts :   false \n   vars : \n     version :   7u45 \n     version_padded :   1.7.0_45 \n     version_text :   7   Update   45 \n   tasks : \n   -   template :   src=templates/usr_local_src_remove-programs.vbs dest=/usr/local/src/remove-programs.vbs \n\n   -   shell :   cscript remove-programs.vbs |grep  Java {{version_text}} (64-bit)  chdir=/usr/local/src \n     register :   result \n     ignore_errors :   true \n\n   -   command :   cscript remove-programs.vbs /uninstall  Java {{version_text}} (64-bit)  chdir=/usr/local/src \n     when :   result|success \n\n   -   shell :   cscript remove-programs.vbs |grep  Java SE Development Kit {{version_text}} (64-bit)  chdir=/usr/local/src \n     register :   result \n     ignore_errors :   true \n\n   -   command :   cscript remove-programs.vbs /uninstall  Java SE Development Kit {{version_text}} (64-bit)  chdir=/usr/local/src \n     when :   result|success \n\n   -   file :   path={{item}} state=absent \n     with_items : \n     -   /usr/local/src/jdk-{{version}}-windows-x64.exe \n     -   /usr/local/src/jdk-install-log.txt \n     -   /usr/local/src/remove-programs.vbs   The  remove-programs.vbs  helper script:  If Wscript.Arguments.Count = 0 Then\n  inventory_software()\nElseIf Wscript.Arguments.Count = 2 Then\n  If Wscript.Arguments(0) =  /uninstall  Then\n     Expecting: cscript remove-programs.vbs /uninstall  Java(TM) 6 Update 26 \n     Expecting: cscript remove-programs.vbs /uninstall  Java(TM) SE Development Kit 6 Update 26 \n    uninstall_software(Wscript.Arguments(1))\n  Else\n    Wscript.Echo  Usage: remove-programs.vbs [/uninstall  software ] \n  End If\nElse\n  Wscript.Echo  Usage: remove-programs.vbs [/uninstall  software ] \nEnd If\n\nSub inventory_software()\n  strComputer =  . \n\n  Set objWMIService = GetObject( winmgmts:  _\n       {impersonationLevel=impersonate}!\\\\  _\n      strComputer    \\root\\cimv2 )\n  Set colSoftware = objWMIService.ExecQuery _\n    ( Select * from Win32_Product )\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo  Name:     objSoftware.Name\n     Wscript.Echo  Version:     objSoftware.Version\n  Next\nEnd Sub\n\nSub inventory_java_software()\n  strComputer =  . \n\n  Set objWMIService = GetObject( winmgmts:  _\n       {impersonationLevel=impersonate}!\\\\  _\n      strComputer    \\root\\cimv2 )\n  Set colSoftware = objWMIService.ExecQuery _\n    ( Select * from Win32_Product   _\n           Where Name Like  Java% )\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo  Name:     objSoftware.Name\n    Wscript.Echo  Version:     objSoftware.Version\n  Next\nEnd Sub\n\nSub uninstall_software(strApplicationName)\n   Make sure to run this with Administrator privileges\n  strComputer =  . \n\n  Set objWMIService = GetObject( winmgmts:  _\n       {impersonationLevel=impersonate}!\\\\  _\n      strComputer    \\root\\cimv2 )\n  Set colSoftware = objWMIService.ExecQuery _\n    ( Select * From Win32_Product Where Name =   _\n      strApplicationName    )\n\n  For Each objSoftware in colSoftware\n    objSoftware.Uninstall()\n  Next\nEnd Sub", 
            "title": "Playbooks"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#demonstration", 
            "text": "File system state before install:  administrator@W7X64-JF~\n$ ls -l /cygdrive/c  | egrep  jdk|jre \ndrwx------+  1  SYSTEM SYSTEM  0  Jun  24   2010  jdk5\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk6\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk7  Install JDK7 on Windows:  copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_install.yml\n\nPLAY  [ install oracle jdk7 ]  ****************************************************\n\nTASK:  [ command  wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24 = http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/7u45-b18/jdk-7u45-windows-x64.exe  creates = /usr/local/src/jdk-7u45-windows-x64.exe ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ file  path = /usr/local/src/jdk-7u45-windows-x64.exe  mode = 0755 ]  ***********\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ shell /usr/local/src/jdk-7u45-windows-x64.exe /s  INSTALLDIR = c: \\\\ jdk1.7.0_45 /INSTALLDIRPUBJRE = c: \\\\ jre1.7.0_45  REBOOT = Suppress  ADDLOCAL = ToolsFeature,SourceFeature,PublicjreFeature  AUTOUPDATE = 0   SYSTRAY = 0   SYSTRAY = 0  /L c: \\\\ cygwin \\\\ usr \\\\ local \\\\ src \\\\ jdk-install-log.txt  creates = /cygdrive/c/jdk1.7.0_45 ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ copy  src = ../certs/dod.root-ca-2.pem  dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/dod.root-ca-2.pem ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ command  /cygdrive/c/jdk1.7.0_45/bin/keytool -import -trustcacerts -alias dodrootca2 -file c: \\\\ jdk1.7.0_45 \\\\ jre \\\\ lib \\\\ security \\\\ dod.root-ca-2.pem -keystore c: \\\\ jdk1.7.0_45 \\\\ jre \\\\ lib \\\\ security \\\\ cacerts -storepass changeit -noprompt ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ copy  src = files/cygdrive_c_java_jre_lib_security_US_export_policy.jar  dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/US_export_policy.jar ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ copy  src = files/cygdrive_c_java_jre_lib_security_local_policy.jar  dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/local_policy.jar ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ shell /cygdrive/c/jdk1.7.0_45/bin/java -version  2 1   | head -1  | awk  {print $3}   | sed -e  s/ //g ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ fail  msg = The Java version does not match the expected value 1.7.0_45. ]  ***\nskipping:  [ w7x64-jf.pd.local ] \n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local :  ok = 8   changed = 8   unreachable = 0   failed = 0   File system state after install and before uninstall:  administrator@W7X64-JF ~\n$ ls -l /cygdrive/c  | egrep  jdk|jre \ndrwx------+  1  SYSTEM SYSTEM  0  Dec  12   23 :17 jdk1.7.0_45\ndrwx------+  1  SYSTEM SYSTEM  0  Jun  24   2010  jdk5\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk6\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk7\ndrwx------+  1  SYSTEM SYSTEM  0  Dec  12   23 :17 jre1.7.0_45  Uninstall JDK7 on Windows:  copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_uninstall.yml\n\nPLAY  [ uninstall oracle jdk7 ]  **************************************************\n\nTASK:  [ template  src = templates/usr_local_src_remove-programs.vbs  dest = /usr/local/src/remove-programs.vbs ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ shell cscript remove-programs.vbs  | grep  Java 7 Update 45 (64-bit)   chdir = /usr/local/src ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ command  cscript remove-programs.vbs /uninstall  Java 7 Update 45 (64-bit)   chdir = /usr/local/src ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ shell cscript remove-programs.vbs  | grep  Java SE Development Kit 7 Update 45 (64-bit)   chdir = /usr/local/src ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ command  cscript remove-programs.vbs /uninstall  Java SE Development Kit 7 Update 45 (64-bit)   chdir = /usr/local/src ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ file  path = $item   state = absent ]  ******************************************\nchanged:  [ w7x64-jf.pd.local ]   =   ( item = /usr/local/src/jdk-7u45-windows-x64.exe ) \nchanged:  [ w7x64-jf.pd.local ]   =   ( item = /usr/local/src/jdk-install-log.txt ) \nchanged:  [ w7x64-jf.pd.local ]   =   ( item = /usr/local/src/remove-programs.vbs ) \n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local :  ok = 6   changed = 6   unreachable = 0   failed = 0   File system state after uninstall:  administrator@W7X64-JF ~\n$ ls -l /cygdrive/c  | egrep  jdk|jre \ndrwx------+  1  SYSTEM SYSTEM  0  Jun  24   2010  jdk5\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk6\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk7", 
            "title": "Demonstration"
        }, 
        {
            "location": "/ansible/testing-ansible-galaxy-roles-with-docker/", 
            "text": "Testing Ansible Galaxy Roles with Docker\n\n\n\n  \n2014-10-14\n\n  \nDiscuss\n\n\n\n\n\n\n\nI learned a neat trick for testing \nAnsible Galaxy\n\n\nroles\n at\n\nAnsibleFest 2014\n.\n\n\nTo get started with Docker on your Mac, install \nVirtualBox\n and then\ninstall \nboot2docker\n, using \nHomebrew\n.\nThe boot2docker package will install docker as a dependency and it runs a small (24MB) Linux\nVirtualBox virtual machine that provides a platform for running Docker images:\n\n\nbrew install boot2docker\nboot2docker init\nboot2docker up\n$(boot2docker shellinit)\n\n\n\n\n\nThe Ansible Team (thanks, \nToshio!\n) has\n\nreleased\n\nDocker images that are included in the \nDocker Hub Registry\n,\nwhich can be used for testing:\n\n\ndocker search ansible |grep ^ansible\nansible/ubuntu14.04-ansible                    Ubuntu 14.04 LTS with ansible                   12\nansible/centos7-ansible                        Ansible on Centos7                              11\n\n\n\n\n\nThere are two tags associated with each of these images: latest and devel.  The latest contains a\nlayered Ansible stable and devel contains a layered Ansible HEAD.  Pull one of these images from the\nDocker Hub:\n\n\ndocker pull ansible/ubuntu14.04-ansible\n\n\n\n\n\nThis technique will allow you to rinse and repeat installs with different roles, which will help you\nfigure out which ones deliver the most suitable functionality for your use cases.\n\n\nLaunch the docker image and leave a shell process running:\n\n\ndocker run -i -t ansible/ubuntu14.04-ansible:stable /bin/bash\n\n\n\n\n\nDownload a role from Ansible Galaxy:\n\n\nansible-galaxy install geerlingguy.memcached\n\n\n\n\n\nCreate a local site.yml playbook to run the role:\n\n\n- hosts: localhost\n  roles:\n      - role: geerlingguy.memcached\n\n\n\n\n\nExecute the playbook:\n\n\nroot@ccca9e7f365f:/# ansible-playbook site.yml -c local\n\nPLAY [localhost] **************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nskipping: [localhost]\n\nTASK: [geerlingguy.memcached | Update apt cache.] *****************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Copy Memcached configuration.] *****************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Ensure Memcached is started and set to run on startup.] ***\nchanged: [localhost]\n\nNOTIFIED: [geerlingguy.memcached | restart memcached] *************************\nchanged: [localhost]\n\nPLAY RECAP ********************************************************************\nlocalhost                  : ok=6    changed=4    unreachable=0    failed=0\n\nroot@ccca9e7f365f:/#", 
            "title": "Testing Ansible Galaxy Roles with Docker"
        }, 
        {
            "location": "/ansible/testing-ansible-galaxy-roles-with-docker/#testing-ansible-galaxy-roles-with-docker", 
            "text": "2014-10-14 \n   Discuss    I learned a neat trick for testing  Ansible Galaxy  roles  at AnsibleFest 2014 .  To get started with Docker on your Mac, install  VirtualBox  and then\ninstall  boot2docker , using  Homebrew .\nThe boot2docker package will install docker as a dependency and it runs a small (24MB) Linux\nVirtualBox virtual machine that provides a platform for running Docker images:  brew install boot2docker\nboot2docker init\nboot2docker up\n$(boot2docker shellinit)  The Ansible Team (thanks,  Toshio! ) has released \nDocker images that are included in the  Docker Hub Registry ,\nwhich can be used for testing:  docker search ansible |grep ^ansible\nansible/ubuntu14.04-ansible                    Ubuntu 14.04 LTS with ansible                   12\nansible/centos7-ansible                        Ansible on Centos7                              11  There are two tags associated with each of these images: latest and devel.  The latest contains a\nlayered Ansible stable and devel contains a layered Ansible HEAD.  Pull one of these images from the\nDocker Hub:  docker pull ansible/ubuntu14.04-ansible  This technique will allow you to rinse and repeat installs with different roles, which will help you\nfigure out which ones deliver the most suitable functionality for your use cases.  Launch the docker image and leave a shell process running:  docker run -i -t ansible/ubuntu14.04-ansible:stable /bin/bash  Download a role from Ansible Galaxy:  ansible-galaxy install geerlingguy.memcached  Create a local site.yml playbook to run the role:  - hosts: localhost\n  roles:\n      - role: geerlingguy.memcached  Execute the playbook:  root@ccca9e7f365f:/# ansible-playbook site.yml -c local\n\nPLAY [localhost] **************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nskipping: [localhost]\n\nTASK: [geerlingguy.memcached | Update apt cache.] *****************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Copy Memcached configuration.] *****************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Ensure Memcached is started and set to run on startup.] ***\nchanged: [localhost]\n\nNOTIFIED: [geerlingguy.memcached | restart memcached] *************************\nchanged: [localhost]\n\nPLAY RECAP ********************************************************************\nlocalhost                  : ok=6    changed=4    unreachable=0    failed=0\n\nroot@ccca9e7f365f:/#", 
            "title": "Testing Ansible Galaxy Roles with Docker"
        }, 
        {
            "location": "/aws/assume-role-with-awscli/", 
            "text": "Assume Role with AWSCLI\n\n\n\n  \n2016-05-19\n\n  \nDiscuss\n\n\n\n\n\n\n\nIf you have either static or instance profile credentials that grant you STS permissions, then you\ncan gather a set of time-limited role credentials as follows:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n#!/bin/bash\n\n\n\nTEST_CREDENTIALS\n=\n$(\n \n\\\n\n  aws sts assume-role \n\\\n\n  --role-arn arn:aws:iam::\n$AWS_ACCOUNT_ID_1\n:role/\n$ROLE_NAME\n \n\\\n\n  --role-session-name \n$USER\n \n\\\n\n  \n|\njq \n.Credentials\n\n\n)\n\n\n\nPROD_CREDENTIALS\n=\n$(\n \n\\\n\n  aws sts assume-role \n\\\n\n  --role-arn arn:aws:iam::\n$AWS_ACCOUNT_ID_2\n:role/\n$ROLE_NAME\n \n\\\n\n  --role-session-name \n$USER\n \n\\\n\n  \n|\njq \n.Credentials\n\n\n)\n\n\ncat \n$HOME\n/.aws/credentials \nEOF\n\n\n[test-$ROLE_NAME]\n\n\naws_access_key_id=$(echo $TEST_CREDENTIALS |jq -r \n.AccessKeyId\n)\n\n\naws_secret_access_key=$(echo $TEST_CREDENTIALS |jq -r \n.SecretAccessKey\n)\n\n\naws_session_token=$(echo $TEST_CREDENTIALS |jq -r \n.SessionToken\n)\n\n\nexpiration=$(echo $TEST_CREDENTIALS |jq -r \n.Expiration\n)\n\n\n\n[prod-$ROLE_NAME]\n\n\naws_access_key_id=$(echo $PROD_CREDENTIALS |jq -r \n.AccessKeyId\n)\n\n\naws_secret_access_key=$(echo $PROD_CREDENTIALS |jq -r \n.SecretAccessKey\n)\n\n\naws_session_token=$(echo $PROD_CREDENTIALS |jq -r \n.SessionToken\n)\n\n\nexpiration=$(echo $PROD_CREDENTIALS |jq -r \n.Expiration\n)\n\n\nEOF", 
            "title": "Assume Role with AWSCLI"
        }, 
        {
            "location": "/aws/assume-role-with-awscli/#assume-role-with-awscli", 
            "text": "2016-05-19 \n   Discuss    If you have either static or instance profile credentials that grant you STS permissions, then you\ncan gather a set of time-limited role credentials as follows:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29 #!/bin/bash  TEST_CREDENTIALS = $(   \\ \n  aws sts assume-role  \\ \n  --role-arn arn:aws:iam:: $AWS_ACCOUNT_ID_1 :role/ $ROLE_NAME   \\ \n  --role-session-name  $USER   \\ \n   | jq  .Credentials  )  PROD_CREDENTIALS = $(   \\ \n  aws sts assume-role  \\ \n  --role-arn arn:aws:iam:: $AWS_ACCOUNT_ID_2 :role/ $ROLE_NAME   \\ \n  --role-session-name  $USER   \\ \n   | jq  .Credentials  ) \n\ncat  $HOME /.aws/credentials  EOF  [test-$ROLE_NAME]  aws_access_key_id=$(echo $TEST_CREDENTIALS |jq -r  .AccessKeyId )  aws_secret_access_key=$(echo $TEST_CREDENTIALS |jq -r  .SecretAccessKey )  aws_session_token=$(echo $TEST_CREDENTIALS |jq -r  .SessionToken )  expiration=$(echo $TEST_CREDENTIALS |jq -r  .Expiration )  [prod-$ROLE_NAME]  aws_access_key_id=$(echo $PROD_CREDENTIALS |jq -r  .AccessKeyId )  aws_secret_access_key=$(echo $PROD_CREDENTIALS |jq -r  .SecretAccessKey )  aws_session_token=$(echo $PROD_CREDENTIALS |jq -r  .SessionToken )  expiration=$(echo $PROD_CREDENTIALS |jq -r  .Expiration )  EOF", 
            "title": "Assume Role with AWSCLI"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/", 
            "text": "AWS Credential Files for Java and Python\n\n\n\n  \n2014-07-14\n\n  \nDiscuss\n\n\n\n\n\n\n\nEach of the AWS tools has slightly different expectations about the location and naming of the\ncredentials file and the various properties within it.  It seems like the Python tools are moving\ncloser to the Java standard as they iterate through releases, but it is still necessary to use a\npatchwork solution to be able to have a unified credentials file.\n\n\nJava SDK\n\n\n\n    \n \nVersion: \n\"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\"\n    \n \nInstallation: \nsbt\n    \n \nLink: \nAWS Java SDK Class ProfilesConfigFile\n\n\n\n\n\nThe standard location for the credentials file is \n~/.aws/credentials\n, which can be overridden with\nthe \nAWS_CREDENTIAL_PROFILES_FILE\n environment variable or by specifying an alternate file location\nin the constructor.  The format of this file is described below:\n\n\n[default]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\n\n[test-user]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\n\n\n\n\nJava Command Line Tools\n\n\n\n    \n \nVersion: \n1.6.13.0\n    \n \nInstallation: \nbrew install ec2-api-tools\n    \n \nLink: \nSetting Up the Amazon EC2 Command Line Interface Tools on Linux/Unix and Mac OS X\n\n\n\n\n\nThe standard configuration is to use environment variables, since these tools have not been updated\nto read the standard AWS credentials file.  Add the following to your \n~/.bash_profile\n, to link the\nrequired data to the standard credentials file and allow for session tokens:\n\n\nexport AWS_CREDENTIAL_FILE=\n$HOME/.aws/credentials\n\nexport AWS_ACCESS_KEY=\n$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\nexport AWS_SECRET_KEY=\n$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\nexport AWS_DELEGATION_TOKEN=\n$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\n\n\n\n\n\nPython Boto\n\n\n\n    \n \nVersion: \nboto==2.31.1 \n botocore==0.56.0\n    \n \nInstallation: \npip install boto\n    \n \nLink: \nBoto Config\n\n\n\n\n\nThe latest version of boto needs to have \naws_security_token\n defined, rather than\n\naws_session_token\n, in the credentials file.  The simplest solution for this is to duplicate the\ntoken between both names; the Java SDK will throw the following log message when reading the extra\nproperty, but will work as expected: \nINFO: Skip unsupported property name aws_security_token in\nprofile [default].\n  Boto will not throw log messages about the existence of the \naws_session_token\n\nproperty.\n\n\nAWS CLI\n\n\n\n    \n \nVersion: \naws-cli/1.3.22\n    \n \nInstallation: \npip install awscli\n    \n \nLink: \nConfiguring the AWS Command Line Interface\n\n\n\n\n\nThe standard location for the credentials file is \n~/.aws/config\n, which can be overridden with the\n\nAWS_CREDENTIAL_FILE\n environment variable.  The latest version of this tool accepts the Java SDK\ncredential file format as-is, including the use of \naws_session_token\n, whereas previous versions\nwanted \naws_security_token\n instead.  When you have multiple profiles in the credentials file, you\ncan select a profile with the tool like so:\n\n\naws --profile test-user s3 ls\n\n\n\n\n\nUnified Solution\n\n\nThe best approach for creating a unified credentials file is to follow the Java credentials file\nformat as closely as possible, while redirecting the Python tools to that file and adding properties\nto cover the corner cases.\n\n\nTo do this, create a \n~/.aws/credentials\n file that duplicates the necessary properties:\n\n\n[default]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\naws_security_token\n=\ntestSessionToken\n\n\n\n[test-user]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\naws_security_token\n=\ntestSessionToken\n\n\n\n[prod-user]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\naws_security_token\n=\ntestSessionToken\n\n\n\n\n\n\nAnd add a section to your \n~/.bash_profile\n:\n\n\nexport AWS_CREDENTIAL_FILE=\n$HOME/.aws/credentials\n\nexport AWS_ACCESS_KEY=\n$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\nexport AWS_SECRET_KEY=\n$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\nexport AWS_DELEGATION_TOKEN=\n$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\n\n\n\n\n\nWith this configuration, you should be able to move seamlessly between the various Java and Python\ntools available for AWS.", 
            "title": "AWS Credential Files for Java and Python"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#aws-credential-files-for-java-and-python", 
            "text": "2014-07-14 \n   Discuss    Each of the AWS tools has slightly different expectations about the location and naming of the\ncredentials file and the various properties within it.  It seems like the Python tools are moving\ncloser to the Java standard as they iterate through releases, but it is still necessary to use a\npatchwork solution to be able to have a unified credentials file.", 
            "title": "AWS Credential Files for Java and Python"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#java-sdk", 
            "text": "Version:  \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\"\n       Installation:  sbt\n       Link:  AWS Java SDK Class ProfilesConfigFile   The standard location for the credentials file is  ~/.aws/credentials , which can be overridden with\nthe  AWS_CREDENTIAL_PROFILES_FILE  environment variable or by specifying an alternate file location\nin the constructor.  The format of this file is described below:  [default]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken  [test-user]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken", 
            "title": "Java SDK"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#java-command-line-tools", 
            "text": "Version:  1.6.13.0\n       Installation:  brew install ec2-api-tools\n       Link:  Setting Up the Amazon EC2 Command Line Interface Tools on Linux/Unix and Mac OS X   The standard configuration is to use environment variables, since these tools have not been updated\nto read the standard AWS credentials file.  Add the following to your  ~/.bash_profile , to link the\nrequired data to the standard credentials file and allow for session tokens:  export AWS_CREDENTIAL_FILE= $HOME/.aws/credentials \nexport AWS_ACCESS_KEY= $(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2) \nexport AWS_SECRET_KEY= $(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2) \nexport AWS_DELEGATION_TOKEN= $(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)", 
            "title": "Java Command Line Tools"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#python-boto", 
            "text": "Version:  boto==2.31.1   botocore==0.56.0\n       Installation:  pip install boto\n       Link:  Boto Config   The latest version of boto needs to have  aws_security_token  defined, rather than aws_session_token , in the credentials file.  The simplest solution for this is to duplicate the\ntoken between both names; the Java SDK will throw the following log message when reading the extra\nproperty, but will work as expected:  INFO: Skip unsupported property name aws_security_token in\nprofile [default].   Boto will not throw log messages about the existence of the  aws_session_token \nproperty.", 
            "title": "Python Boto"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#aws-cli", 
            "text": "Version:  aws-cli/1.3.22\n       Installation:  pip install awscli\n       Link:  Configuring the AWS Command Line Interface   The standard location for the credentials file is  ~/.aws/config , which can be overridden with the AWS_CREDENTIAL_FILE  environment variable.  The latest version of this tool accepts the Java SDK\ncredential file format as-is, including the use of  aws_session_token , whereas previous versions\nwanted  aws_security_token  instead.  When you have multiple profiles in the credentials file, you\ncan select a profile with the tool like so:  aws --profile test-user s3 ls", 
            "title": "AWS CLI"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#unified-solution", 
            "text": "The best approach for creating a unified credentials file is to follow the Java credentials file\nformat as closely as possible, while redirecting the Python tools to that file and adding properties\nto cover the corner cases.  To do this, create a  ~/.aws/credentials  file that duplicates the necessary properties:  [default]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken  aws_security_token = testSessionToken  [test-user]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken  aws_security_token = testSessionToken  [prod-user]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken  aws_security_token = testSessionToken   And add a section to your  ~/.bash_profile :  export AWS_CREDENTIAL_FILE= $HOME/.aws/credentials \nexport AWS_ACCESS_KEY= $(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2) \nexport AWS_SECRET_KEY= $(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2) \nexport AWS_DELEGATION_TOKEN= $(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)   With this configuration, you should be able to move seamlessly between the various Java and Python\ntools available for AWS.", 
            "title": "Unified Solution"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/", 
            "text": "Build a Fake Instance Metadata Server for Ubuntu on Vagrant\n\n\n\n  \n2014-07-22\n\n  \nDiscuss\n\n\n\n\n\n\n\nLet's say that you have a proper build/bake/deploy pipeline in place for running appications in AWS.\nThis is a reliable way to deploy applications at scale and move traffic between different versions\nof applications as a part of the deployment pipeline.  However, the whole process may take 20-40\nminutes or so to complete for any particular build.  If you want to iterate more rapidly on your\ndevelopment efforts, you could skip the full process with a quickpatch ssh/rsync deployment to a\nsingle server or you could stand up a local Vagrant base image and iterate on that.  Now let's say\nthat the application you are working on is intended to work with instance metadata, particularly for\nthe purpose of obtaining a rotating set of access and secret keys.  It might be nice to have a fake\nmetadata service running on your local Vagrant image so that you can test your application in a\nmanner similar to how it will be running in the cloud.  In this post, I describe how to build and\nconfigure a fake metadata service for an Ubuntu image running on Vagrant.\n\n\nPackage Layout\n\n\nThis layout assumes that you will be installing custom packages to the \n/apps\n directory, and there\nis a \ndaemontools\n service hierarchy located at \n/service\n.  The\n\nfake-metadata/service/run\n file is a script suitable for use with daemontools.\n\n\nroot/\n\u251c\u2500\u2500 apps\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 app.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 service\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 run\n\u2514\u2500\u2500 etc\n    \u251c\u2500\u2500 init.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u251c\u2500\u2500 logrotate.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u2514\u2500\u2500 network\n        \u2514\u2500\u2500 iptables.rules\n\n\n\n\n\nBuilding and Packaging\n\n\nFor cross-platform build packaging, it will be easiest to use the\n\nnebula ospackage\n plugin with Gradle.\nWith this plugin available, your build script will look something like this:\n\n\napply\n \nplugin:\n \nnebula-ospackage\n\n\n\nospackage\n \n{\n\n    \nversion\n=\n1.0\n\n    \npackageName\n=\nfake-metadata\n\n\n    \nrequires\n(\npython-flask\n)\n\n\n    \nlink\n(\n/apps/fake-metadata/logs\n,\n \n/mnt/logs/fake-metadata\n)\n\n    \nlink\n(\n/service/fake-metadata\n,\n \n/apps/fake-metadata/service\n)\n\n\n}\n\n\n\nbuildDeb\n \n{\n\n    \npostInstall\n \nfile\n(\nscripts/postInstall.sh\n)\n\n    \npreUninstall\n \nsvc -d /service/fake-metadata\n\n    \npostUninstall\n \nfile\n(\nscripts/postUninstall.sh\n)\n\n\n}\n\n\n\ntask\n \nbuild\n(\ndependsOn:\n \n[\nbuildDeb\n])\n\n\n\n\n\n\nFake Metadata Application\n\n\nThe simplest approach to building the service is to create a \nFlask\n\nservice and have it run bare on the Vagrant instance.  Given how small it will be and limited amount\nof traffic it will need to serve, there is no need to run this behind a dedicated static webserver\nlike nginx or Apache.  The nice thing about using Flask and having a basic structure in place is\nthat it is then easy to extend the application to add other endpoints when needed.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n#!/usr/bin/env python\n\n\n\nfrom\n \nflask\n \nimport\n \nFlask\n,\n \njsonify\n,\n \nabort\n,\n \nmake_response\n,\n \nrequest\n\n\nimport\n \nos\n\n\nimport\n \nsys\n\n\n\n\napp\n \n=\n \nFlask\n(\n__name__\n)\n\n\n\n\nBaseIAMRole\n \n=\n \n{\n\n  \nCode\n:\n \nSuccess\n,\n\n  \nLastUpdated\n \n:\n \n,\n\n  \nType\n:\n \nAWS-HMAC\n,\n\n  \nAccessKeyId\n:\n \n,\n\n  \nSecretAccessKey\n:\n \n,\n\n  \nToken\n:\n \n,\n\n  \nExpiration\n:\n \n\n\n}\n\n\n\n\n@app.route\n(\n/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nindex\n():\n\n    \nreturn\n \nlatest\n\n\n\n\n@app.route\n(\n/latest/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nlatest\n():\n\n    \nreturn\n \nmeta-data\n\n\n\n\n@app.route\n(\n/latest/meta-data/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nmeta_data\n():\n\n    \nendpoints\n \n=\n \n[\n\n        \niam\n,\n\n        \npublic-hostname\n,\n\n        \npublic-ipv4\n\n    \n]\n\n    \nreturn\n \n(\n\\n\n)\n.\njoin\n(\nendpoints\n)\n\n\n\n\n@app.route\n(\n/latest/meta-data/iam/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \niam\n():\n\n    \nreturn\n \nsecurity-credentials\n\n\n\n\n@app.route\n(\n/latest/meta-data/iam/security-credentials/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nsecurity_credentials\n():\n\n    \nreturn\n \nBaseIAMRole\n\n\n\n\n@app.route\n(\n/latest/meta-data/iam/security-credentials/BaseIAMRole\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nbase_iam_role\n():\n\n    \n# update the payload to contain a current set of accesss and secrey keys\n\n    \nreturn\n \njsonify\n(\nBaseIAMRole\n)\n\n\n\n\n@app.route\n(\n/latest/meta-data/public-hostname\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \npublic_hostname\n():\n\n    \nreturn\n \nos\n.\nenviron\n[\nEC2_LOCAL_HOSTNAME\n]\n\n\n\n\n@app.route\n(\n/latest/meta-data/public-ipv4\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \npublic_ipv4\n():\n\n    \nreturn\n \nos\n.\nenviron\n[\nEC2_LOCAL_IPV4\n]\n\n\n\n\n@app.errorhandler\n(\n400\n)\n\n\ndef\n \nnot_found\n(\nerror\n):\n\n    \nreturn\n \nmake_response\n(\njsonify\n(\n \n{\n \nerror\n:\n \nbad request\n \n}\n \n),\n \n400\n)\n\n\n\n\n@app.errorhandler\n(\n404\n)\n\n\ndef\n \nnot_found\n(\nerror\n):\n\n    \nreturn\n \nmake_response\n(\njsonify\n(\n \n{\n \nerror\n:\n \nnot found\n \n}\n \n),\n \n404\n)\n\n\n\n\n@app.errorhandler\n(\n500\n)\n\n\ndef\n \nnot_found\n(\nerror\n):\n\n    \nreturn\n \nmake_response\n(\njsonify\n(\n \n{\n \nerror\n:\n \ninternal server error\n \n}\n \n),\n \n500\n)\n\n\n\n\nif\n \n__name__\n \n==\n \n__main__\n:\n\n    \nif\n \nlen\n(\nsys\n.\nargv\n)\n \n \n1\n:\n\n        \nif\n \n:\n \nin\n \nsys\n.\nargv\n[\n1\n]:\n\n            \nhost\n=\nsys\n.\nargv\n[\n1\n]\n.\nsplit\n(\n:\n)[\n0\n]\n\n            \nport\n=\nint\n(\nsys\n.\nargv\n[\n1\n]\n.\nsplit\n(\n:\n)[\n1\n])\n\n            \napp\n.\nrun\n(\nhost\n=\nhost\n,\n \nport\n=\nport\n)\n\n        \nelse\n:\n\n            \napp\n.\nrun\n(\nhost\n=\nsys\n.\nargv\n[\n1\n])\n\n    \nelse\n:\n\n        \napp\n.\nrun\n(\ndebug\n=\nTrue\n)\n\n\n\n\n\n\n\nDaemontools Run Script\n\n\nThis script is watched by the supervise process, which then starts (or restarts) the application if\nit is not running.  Switching to a non-root user and redirecting output to the log file occurs here.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n#!/bin/bash\n\n\n\nulimit\n -n \n32768\n\n\n\nsource\n /etc/profile.d/environment.sh\n\n\nexport\n \nPATH\n=\n/command:/usr/local/bin:/usr/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/scripts\n\n\nif\n \n[\n ! -d \n/mnt/logs/fake-metadata\n \n]\n;\n \nthen\n\n  mkdir -p /mnt/logs/fake-metadata\n  chmod \n0777\n /mnt/logs/fake-metadata\n\nfi\n\n\n\nUSER\n=\nsomeuser\n\nPYTHON\n=\n/usr/bin/python\n\nAPP\n=\n/apps/fake-metadata/app.py\n\nOPTS\n=\n127\n.0.0.1:8000\n\nLOG\n=\n/mnt/logs/fake-metadata/server.log\n\n\necho\n \nstarting fake-metadata\n\n\nexec\n setuidgid \n$USER\n \n$PYTHON\n \n$APP\n \n$OPTS\n \n \n$LOG\n \n2\n1\n\n\n\n\n\n\n\nPostInstall Script\n\n\nThis is where most of the trickiness occurs.\n\n\nThe post install script is responsible for modifying the \n/etc/network/interfaces\n file, adding the\nmetadata server IP address and configuring iptables in an idempotent manner.  When the post install\nscript is packaged by the nebula ospackage plugin into a Debian package, it gets a \n#!/bin/sh -e\n\nshebang invocation, which means that the script will halt execution at any point where it evaluates\na non-zero return code.  This means that the script needs to be written such a way that the\nenvironment state testing being done always returns a true value so that the script does not fail,\nhence the \n||true\n constructs.\n\n\nWe are attaching an extra IP address to the loopback interface, so we need to redirect traffic from\n169.254.169.254:80 to the location where the fake metadata server is running.  We are dealing with\nthe loopback interface, which means that the PREROUTING nat table is never hit and we must use the\nOUTPUT table instead.  You cannot DNAT packets destined for the loopback interface, because the\nkernel will treat them as martians and drop them, so you must REDIRECT the packets to the desired\nport.  When performing the redirection from port 80 to 8000 on the loopback interface, it sends the\npackets to 127.0.0.1:8000, not 169.254.169.254:8000, so the fake metadata server must be listening\non localhost port 8000.\n\n\nIf for some reason, you need to troubleshoot the post install script, it can be found at\n\n/var/lib/dpkg/info/fake-metadata.postinst\n following an attempted package installation.\n\n\nLINE\n=\n$(\n grep \n169\n.254.169.254/32 /etc/network/interfaces \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n ! \n$LINE\n \n==\n *169.254.169.254/32*  \n]]\n;\n \nthen\n\n    sed -i \n/iface lo inet loopback/a up ip addr add 169.254.169.254/32 dev lo scope host\n /etc/network/interfaces\n\nfi\n\n\n\nLINE\n=\n$(\n /sbin/ip addr \n|\ngrep \n169\n.254.169.254/32 \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n ! \n$LINE\n \n==\n *169.254.169.254/32*  \n]]\n;\n \nthen\n\n    /sbin/ip addr add \n169\n.254.169.254/32 dev lo scope host\n\nfi\n\n\n\nLINE\n=\n$(\n grep iptables-restore /etc/network/interfaces \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n ! \n$LINE\n \n==\n *iptables-restore*  \n]]\n;\n \nthen\n\n    sed -i \n/up ip addr add/a pre-up iptables-restore \n /etc/network/iptables.rules\n /etc/network/interfaces\n\nfi\n\n\n\nLINE\n=\n$(\n iptables -t nat -L \n|\ngrep \n8000\n \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n ! \n$LINE\n \n==\n *8000*  \n]]\n;\n \nthen\n\n    iptables -t nat -A OUTPUT -p tcp -d \n169\n.254.169.254/32 --dport \n80\n -j REDIRECT --to-ports \n8000\n\n\nfi\n\n\n/usr/sbin/update-rc.d fake-metadata defaults\n\n\n\n\n\nPostUninstall Script\n\n\nThis script is the inverse of the post install script; it returns the system to its previous state.\n\n\nLINE\n=\n$(\n grep \n169\n.254.169.254/32 /etc/network/interfaces \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n \n$LINE\n \n==\n *169.254.169.254/32*  \n]]\n;\n \nthen\n\n    sed -i \n/up ip addr add 169.254.169.254\\/32 dev lo scope host/d\n /etc/network/interfaces\n\nfi\n\n\n\nLINE\n=\n$(\n /sbin/ip addr \n|\ngrep \n169\n.254.169.254/32 \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n \n$LINE\n \n==\n *169.254.169.254/32*  \n]]\n;\n \nthen\n\n    /sbin/ip addr delete \n169\n.254.169.254/32 dev lo scope host\n\nfi\n\n\n\nLINE\n=\n$(\n grep iptables-restore /etc/network/interfaces \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n \n$LINE\n \n==\n *iptables-restore*  \n]]\n;\n \nthen\n\n    sed -i \n/pre-up iptables-restore \n \\/etc\\/network\\/iptables.rules/d\n /etc/network/interfaces\n\nfi\n\n\n\nLINE\n=\n$(\n iptables -t nat -L \n|\ngrep \n8000\n \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n \n$LINE\n \n==\n *8000*  \n]]\n;\n \nthen\n\n    iptables -t nat -D OUTPUT -p tcp -d \n169\n.254.169.254/32 --dport \n80\n -j REDIRECT --to-ports \n8000\n\n\nfi\n\n\n/usr/sbin/update-rc.d -f fake-metadata remove\nrm -rf /apps/fake-metadata\npkill -f \nsupervise fake-metadata\n\n\n\n\n\n\nLog Rotation\n\n\nTo keep the local Vagrant instance clean, it is useful to configure log rotation.  Sending a HUP\nsignal to the service allows it to continue writing to the new logfile.\n\n\n/mnt/logs/fake-metadata/server.log \n{\n\n    daily\n    rotate \n7\n\n    compress\n    missingok\n    notifempty\n    create \n644\n root root\n    postrotate\n        svc -h /service/fake-metadata\n    endscript\n\n}", 
            "title": "Build a Fake Instance Metadata Server for Ubuntu on Vagrant"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant", 
            "text": "2014-07-22 \n   Discuss    Let's say that you have a proper build/bake/deploy pipeline in place for running appications in AWS.\nThis is a reliable way to deploy applications at scale and move traffic between different versions\nof applications as a part of the deployment pipeline.  However, the whole process may take 20-40\nminutes or so to complete for any particular build.  If you want to iterate more rapidly on your\ndevelopment efforts, you could skip the full process with a quickpatch ssh/rsync deployment to a\nsingle server or you could stand up a local Vagrant base image and iterate on that.  Now let's say\nthat the application you are working on is intended to work with instance metadata, particularly for\nthe purpose of obtaining a rotating set of access and secret keys.  It might be nice to have a fake\nmetadata service running on your local Vagrant image so that you can test your application in a\nmanner similar to how it will be running in the cloud.  In this post, I describe how to build and\nconfigure a fake metadata service for an Ubuntu image running on Vagrant.", 
            "title": "Build a Fake Instance Metadata Server for Ubuntu on Vagrant"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#package-layout", 
            "text": "This layout assumes that you will be installing custom packages to the  /apps  directory, and there\nis a  daemontools  service hierarchy located at  /service .  The fake-metadata/service/run  file is a script suitable for use with daemontools.  root/\n\u251c\u2500\u2500 apps\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 app.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 service\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 run\n\u2514\u2500\u2500 etc\n    \u251c\u2500\u2500 init.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u251c\u2500\u2500 logrotate.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u2514\u2500\u2500 network\n        \u2514\u2500\u2500 iptables.rules", 
            "title": "Package Layout"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#building-and-packaging", 
            "text": "For cross-platform build packaging, it will be easiest to use the nebula ospackage  plugin with Gradle.\nWith this plugin available, your build script will look something like this:  apply   plugin:   nebula-ospackage  ospackage   { \n     version = 1.0 \n     packageName = fake-metadata \n\n     requires ( python-flask ) \n\n     link ( /apps/fake-metadata/logs ,   /mnt/logs/fake-metadata ) \n     link ( /service/fake-metadata ,   /apps/fake-metadata/service )  }  buildDeb   { \n     postInstall   file ( scripts/postInstall.sh ) \n     preUninstall   svc -d /service/fake-metadata \n     postUninstall   file ( scripts/postUninstall.sh )  }  task   build ( dependsOn:   [ buildDeb ])", 
            "title": "Building and Packaging"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#fake-metadata-application", 
            "text": "The simplest approach to building the service is to create a  Flask \nservice and have it run bare on the Vagrant instance.  Given how small it will be and limited amount\nof traffic it will need to serve, there is no need to run this behind a dedicated static webserver\nlike nginx or Apache.  The nice thing about using Flask and having a basic structure in place is\nthat it is then easy to extend the application to add other endpoints when needed.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92 #!/usr/bin/env python  from   flask   import   Flask ,   jsonify ,   abort ,   make_response ,   request  import   os  import   sys  app   =   Flask ( __name__ )  BaseIAMRole   =   { \n   Code :   Success , \n   LastUpdated   :   , \n   Type :   AWS-HMAC , \n   AccessKeyId :   , \n   SecretAccessKey :   , \n   Token :   , \n   Expiration :    }  @app.route ( / ,   methods   =   [ GET ])  def   index (): \n     return   latest  @app.route ( /latest/ ,   methods   =   [ GET ])  def   latest (): \n     return   meta-data  @app.route ( /latest/meta-data/ ,   methods   =   [ GET ])  def   meta_data (): \n     endpoints   =   [ \n         iam , \n         public-hostname , \n         public-ipv4 \n     ] \n     return   ( \\n ) . join ( endpoints )  @app.route ( /latest/meta-data/iam/ ,   methods   =   [ GET ])  def   iam (): \n     return   security-credentials  @app.route ( /latest/meta-data/iam/security-credentials/ ,   methods   =   [ GET ])  def   security_credentials (): \n     return   BaseIAMRole  @app.route ( /latest/meta-data/iam/security-credentials/BaseIAMRole ,   methods   =   [ GET ])  def   base_iam_role (): \n     # update the payload to contain a current set of accesss and secrey keys \n     return   jsonify ( BaseIAMRole )  @app.route ( /latest/meta-data/public-hostname ,   methods   =   [ GET ])  def   public_hostname (): \n     return   os . environ [ EC2_LOCAL_HOSTNAME ]  @app.route ( /latest/meta-data/public-ipv4 ,   methods   =   [ GET ])  def   public_ipv4 (): \n     return   os . environ [ EC2_LOCAL_IPV4 ]  @app.errorhandler ( 400 )  def   not_found ( error ): \n     return   make_response ( jsonify (   {   error :   bad request   }   ),   400 )  @app.errorhandler ( 404 )  def   not_found ( error ): \n     return   make_response ( jsonify (   {   error :   not found   }   ),   404 )  @app.errorhandler ( 500 )  def   not_found ( error ): \n     return   make_response ( jsonify (   {   error :   internal server error   }   ),   500 )  if   __name__   ==   __main__ : \n     if   len ( sys . argv )     1 : \n         if   :   in   sys . argv [ 1 ]: \n             host = sys . argv [ 1 ] . split ( : )[ 0 ] \n             port = int ( sys . argv [ 1 ] . split ( : )[ 1 ]) \n             app . run ( host = host ,   port = port ) \n         else : \n             app . run ( host = sys . argv [ 1 ]) \n     else : \n         app . run ( debug = True )", 
            "title": "Fake Metadata Application"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#daemontools-run-script", 
            "text": "This script is watched by the supervise process, which then starts (or restarts) the application if\nit is not running.  Switching to a non-root user and redirecting output to the log file occurs here.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 #!/bin/bash  ulimit  -n  32768  source  /etc/profile.d/environment.sh export   PATH = /command:/usr/local/bin:/usr/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/scripts if   [  ! -d  /mnt/logs/fake-metadata   ] ;   then \n  mkdir -p /mnt/logs/fake-metadata\n  chmod  0777  /mnt/logs/fake-metadata fi  USER = someuser PYTHON = /usr/bin/python APP = /apps/fake-metadata/app.py OPTS = 127 .0.0.1:8000 LOG = /mnt/logs/fake-metadata/server.log echo   starting fake-metadata  exec  setuidgid  $USER   $PYTHON   $APP   $OPTS     $LOG   2 1", 
            "title": "Daemontools Run Script"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#postinstall-script", 
            "text": "This is where most of the trickiness occurs.  The post install script is responsible for modifying the  /etc/network/interfaces  file, adding the\nmetadata server IP address and configuring iptables in an idempotent manner.  When the post install\nscript is packaged by the nebula ospackage plugin into a Debian package, it gets a  #!/bin/sh -e \nshebang invocation, which means that the script will halt execution at any point where it evaluates\na non-zero return code.  This means that the script needs to be written such a way that the\nenvironment state testing being done always returns a true value so that the script does not fail,\nhence the  ||true  constructs.  We are attaching an extra IP address to the loopback interface, so we need to redirect traffic from\n169.254.169.254:80 to the location where the fake metadata server is running.  We are dealing with\nthe loopback interface, which means that the PREROUTING nat table is never hit and we must use the\nOUTPUT table instead.  You cannot DNAT packets destined for the loopback interface, because the\nkernel will treat them as martians and drop them, so you must REDIRECT the packets to the desired\nport.  When performing the redirection from port 80 to 8000 on the loopback interface, it sends the\npackets to 127.0.0.1:8000, not 169.254.169.254:8000, so the fake metadata server must be listening\non localhost port 8000.  If for some reason, you need to troubleshoot the post install script, it can be found at /var/lib/dpkg/info/fake-metadata.postinst  following an attempted package installation.  LINE = $(  grep  169 .254.169.254/32 /etc/network/interfaces  ||   true   )  if   [[  !  $LINE   ==  *169.254.169.254/32*   ]] ;   then \n    sed -i  /iface lo inet loopback/a up ip addr add 169.254.169.254/32 dev lo scope host  /etc/network/interfaces fi  LINE = $(  /sbin/ip addr  | grep  169 .254.169.254/32  ||   true   )  if   [[  !  $LINE   ==  *169.254.169.254/32*   ]] ;   then \n    /sbin/ip addr add  169 .254.169.254/32 dev lo scope host fi  LINE = $(  grep iptables-restore /etc/network/interfaces  ||   true   )  if   [[  !  $LINE   ==  *iptables-restore*   ]] ;   then \n    sed -i  /up ip addr add/a pre-up iptables-restore   /etc/network/iptables.rules  /etc/network/interfaces fi  LINE = $(  iptables -t nat -L  | grep  8000   ||   true   )  if   [[  !  $LINE   ==  *8000*   ]] ;   then \n    iptables -t nat -A OUTPUT -p tcp -d  169 .254.169.254/32 --dport  80  -j REDIRECT --to-ports  8000  fi \n\n/usr/sbin/update-rc.d fake-metadata defaults", 
            "title": "PostInstall Script"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#postuninstall-script", 
            "text": "This script is the inverse of the post install script; it returns the system to its previous state.  LINE = $(  grep  169 .254.169.254/32 /etc/network/interfaces  ||   true   )  if   [[   $LINE   ==  *169.254.169.254/32*   ]] ;   then \n    sed -i  /up ip addr add 169.254.169.254\\/32 dev lo scope host/d  /etc/network/interfaces fi  LINE = $(  /sbin/ip addr  | grep  169 .254.169.254/32  ||   true   )  if   [[   $LINE   ==  *169.254.169.254/32*   ]] ;   then \n    /sbin/ip addr delete  169 .254.169.254/32 dev lo scope host fi  LINE = $(  grep iptables-restore /etc/network/interfaces  ||   true   )  if   [[   $LINE   ==  *iptables-restore*   ]] ;   then \n    sed -i  /pre-up iptables-restore   \\/etc\\/network\\/iptables.rules/d  /etc/network/interfaces fi  LINE = $(  iptables -t nat -L  | grep  8000   ||   true   )  if   [[   $LINE   ==  *8000*   ]] ;   then \n    iptables -t nat -D OUTPUT -p tcp -d  169 .254.169.254/32 --dport  80  -j REDIRECT --to-ports  8000  fi \n\n/usr/sbin/update-rc.d -f fake-metadata remove\nrm -rf /apps/fake-metadata\npkill -f  supervise fake-metadata", 
            "title": "PostUninstall Script"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#log-rotation", 
            "text": "To keep the local Vagrant instance clean, it is useful to configure log rotation.  Sending a HUP\nsignal to the service allows it to continue writing to the new logfile.  /mnt/logs/fake-metadata/server.log  { \n    daily\n    rotate  7 \n    compress\n    missingok\n    notifempty\n    create  644  root root\n    postrotate\n        svc -h /service/fake-metadata\n    endscript }", 
            "title": "Log Rotation"
        }, 
        {
            "location": "/aws/checking-and-changing-content-type-of-s3-object-with-awscli/", 
            "text": "Checking and Changing Content-Type of S3 Object with AWSCLI\n\n\n\n  \n2017-03-04\n\n  \nDiscuss\n\n\n\n\n\n\n\naws s3api get-object \\\n    --bucket my.bucket \\\n    --key foo/bar/2017-01-26/usage.json \\\n    usage.json\n\naws s3api copy-object \\\n    --bucket archive \\\n    --content-type \napplication/rss+xml\n \\\n    --copy-source archive/test/test.html \\\n    --key test/test.html \\\n    --metadata-directive \nREPLACE", 
            "title": "Checking and Changing Content-Type of S3 Object with AWSCLI"
        }, 
        {
            "location": "/aws/checking-and-changing-content-type-of-s3-object-with-awscli/#checking-and-changing-content-type-of-s3-object-with-awscli", 
            "text": "2017-03-04 \n   Discuss    aws s3api get-object \\\n    --bucket my.bucket \\\n    --key foo/bar/2017-01-26/usage.json \\\n    usage.json\n\naws s3api copy-object \\\n    --bucket archive \\\n    --content-type  application/rss+xml  \\\n    --copy-source archive/test/test.html \\\n    --key test/test.html \\\n    --metadata-directive  REPLACE", 
            "title": "Checking and Changing Content-Type of S3 Object with AWSCLI"
        }, 
        {
            "location": "/aws/configuring-multiple-interfaces-on-the-same-network-in-ec2/", 
            "text": "Configuring Multiple Interfaces on the Same Network in EC2\n\n\n\n  \n2014-07-09\n\n  \nDiscuss\n\n\n\n\n\n\n\nI've been reading a bit lately on Linux policy and source-based routing for the purpose of\nconfiguring multiple NICs on the same network in EC2.  I found the following links helpful:\n\n\n\n\nMultiple IPs and ENIs on EC2 in a VPC\n\n\nRouting for Multiple Uplinks/Providers\n\n\nA Quick Introduction to Linux Policy Routing", 
            "title": "Configuring Multiple Interfaces on the Same Network in EC2"
        }, 
        {
            "location": "/aws/configuring-multiple-interfaces-on-the-same-network-in-ec2/#configuring-multiple-interfaces-on-the-same-network-in-ec2", 
            "text": "2014-07-09 \n   Discuss    I've been reading a bit lately on Linux policy and source-based routing for the purpose of\nconfiguring multiple NICs on the same network in EC2.  I found the following links helpful:   Multiple IPs and ENIs on EC2 in a VPC  Routing for Multiple Uplinks/Providers  A Quick Introduction to Linux Policy Routing", 
            "title": "Configuring Multiple Interfaces on the Same Network in EC2"
        }, 
        {
            "location": "/github/how-i-built-my-site/", 
            "text": "How I Built My Site\n\n\n\n  \n2017-10-12\n\n  \nDiscuss\n\n\n\n\n\n\n\nI recently rebuilt my GitHub Pages site, switching my tech stack to \nPython\n, \nMkDocs\n and\n\nMaterial for MkDocs\n. The previous workflow had too many moving parts and the theme was hard to\nread.  It turns out that MkDocs has some nice automation hooks for GitHub Pages and it looks great\non mobile browsers with the \nMaterial\n theme. Since I use MkDocs for site generation at work, I am\nalready pretty familiar with it.\n\n\nWriting documents in \nMarkdown\n and turning them into web pages with a static site generator is the\nfastest and easiest way to post articles on your GitHub Pages site.  This format allows you to\ndrop down into HTML, when necessary, to enhance your page formatting, but you shouldn't need to do\nthis for most pages.\n\n\nInstall Python and MkDocs Packages\n\n\nIf you are relying on a system Python, consider following these instructions:\n\nInstall the Latest Python Versions on Mac OSX\n.\n\n\nInstall \nMkDocs\n and \nMaterial for MkDocs\n. These two packages will bring along all of the necessary\ndependencies for a fully functioning site.\n\n\npip install mkdocs mkdocs-material\n\n\n\n\n\nSign Up for Google Analytics\n\n\n\n\nNote\n\n\nYou must disable ad-blocking software in order to be able to see the Google Analytics page.\n\n\n\n\n\n\nNavigate to \nGoogle Analytics\n \n Admin\n\n\nProperty \n Create New Property\n\n\nAccount Name:\n $YOUR_ACCOUNT_NAME\n\n\nWebsite Name:\n username.github.io\n\n\nWebsite URL:\n https://username.github.io\n\n\nGo with the default configuration options - you can change these later.\n\n\nGet Tracking ID\n\n\n\n\n\n\nNote the Tracking ID (looks like: \nUA-00000000-0\n) assigned to this property.\n\n\n\n\nBuild the Site\n\n\n\n\n\n\nCreate a new repository\n named \nusername.github.io\n, where \nusername\n is\nyour username (or organization name) on GitHub.  It must match exactly, or it will not work.\n\n\n\n\n\n\nClone the repository locally.  Requires \nSSH keys\n.\n\n\ngit clone git@github.com:username/username.github.io.git\ncd username.github.com\n\n\n\n\n\n\n\n\n\nCreate a new branch to store the source files for the site.  In this configuration, GitHub Pages\nwill be served from the \nmaster\n branch, which will contain the the static site generated by MkDocs.\nTo keep the \nmaster\n branch clean and avoid file conflicts, the source for the site will be stored\nin the \nsource\n branch.\n\n\ngit checkout -b source\n\n\n\n\n\n\n\n\n\nYou will create a directory structure that looks like the following:\n\n\nusername.github.io/\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 docs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 css\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 custom.css\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500 requirements.txt\n\n\n\n\n\n\n\n\n\nCreate a \n./mkdocs.yml\n site configuration file. Setting the \nremote_branch\n to \nmaster\n is\nimportant for the GitHub deployment hooks in MkDocs. Choose a\n\nCreative Commons license\n for your site - I chose\n\nCC BY-NC-SA 4.0\n for my site.  Add your Google\nAnalytics property Tracking ID to \ngoogle_analytics\n.  If you do not have a Tracking ID, then delete this\nline in the configuration file.\n\n\nsite_name: My Site\nsite_url: \nhttp://username.github.io/\n\nrepo_url: \nhttps://github.com/username/username.github.io\n\nedit_uri: \n\nsite_description: My Site\nsite_author: My Name\ncopyright: \na\n \nrel=\nlicense\n \nhref=\nhttp://creativecommons.org/licenses/by-nc-sa/4.0/\nimg\n \nalt=\nCreative Commons License\n \nstyle=\nborder-width:0\n \nsrc=\nhttps://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\n \n/\n/a\n\ngoogle_analytics: [\nUA-00000000-0\n, \nusername.github.io\n]\nremote_branch: master\n\nstrict: True\n\npages:\n- Home: index.md\n\ntheme: material\n\nextra:\n  font:\n    code: \nMenlo\n\n\nextra_css:\n- css/custom.css\n\nmarkdown_extensions:\n- admonition\n- codehilite\n- pymdownx.tilde\n- toc:\n    permalink: True\n\n\n\n\n\n\n\n\n\nCreate a \n./docs/index.md\n file.\n\n\nHello World!\n\n\n\n\n\n\n\n\n\nCreate a \n./docs/css/custom.css\n file.\n\n\n/* Avoid showing first header, i.e., the page title in the sidebar.\n\n\n *\n\n\n * https://github.com/mkdocs/mkdocs/issues/318#issuecomment-98520139\n\n\n */\n\n\nli\n.\ntoctree-l3\n:\nfirst-child\n \n{\n\n  \ndisplay\n:\n \nnone\n;\n\n\n}\n\n\n\ncode\n \n{\n\n  \nfont-family\n:\n \nMenlo\n,\n \nmonospace\n;\n\n\n}\n\n\n\n\n\n\n\n\n\n\nCreate a \n./requirements.txt\n file, so that you can easily reinstall the necessary Python packages\nwith \npip install -r requirements.txt\n.\n\n\nmkdocs\nmkdocs-material\n\n\n\n\n\n\n\n\n\nCreate a \n./.gitignore\n file.\n\n\nsite\n\n\n\n\n\n\n\n\n\nBuild and serve the site locally, to verify that your changes look good.  When MkDocs is up and\nrunning, you will see \nServing on http://127.0.0.1:8000\n. Leave this process running. When you are\ndone developing and testing your site, you can stop this process with \nctrl+c\n. Open a new Terminal\nto run the second command.\n\n\nmkdocs serve\nopen http://localhost:8000\n\n\n\n\n\n\n\n\n\nPush the source branch to GitHub to save your changes. You only need to set the upstream for the\nfirst push; subsequent pushes will be \ngit push origin\n as long as you remain on the \nsource\n branch.\n\n\ngit add --all\ngit commit -m \nfirst commit\n\ngit push --set-upstream origin source\n\n\n\n\n\n\n\n\n\nBuild and deploy the site. The static HTML site generated by this process will be pushed to the\n\nmaster\n branch origin for this repository. Changes will be live within one minute. Navigate to\nthe site url to see your changes.\n\n\nmkdocs gh-deploy\n\n\n\n\n\n\n\n\n\nRepo Configuration\n\n\nNavigate to the repository on GitHub and set some useful configuration options.\n\n\n\n\nCode \n Description\n\n\nCode \n Website\n\n\nCode \n Manage topics\n\n\nSettings \n Branches \n Default branch: \nsource\n\n\nSettings \n Branches \n Protected branches: \nsource\n\n\nCheck: Protect this branch\n\n\nCheck: Include administrators\n\n\n\n\n\n\n\n\nNew Post Workflow\n\n\n\n\n\n\nStart serving the site locally, with file change detection.\n\n\nmkdocs serve\nopen http://localhost:8000\n\n\n\n\n\n\n\n\n\nStart a new post by creating a markdown file in the \n./docs\n directory hierarchy.\n\n\n\n\n\n\nImages can be served from a location such as \n./docs/images\n, with references as follows:\n\n\n!\n[\nLink\n \nName\n](\n/\nimages\n/\nmy\n-\nfile\n.\npng\n \nAlt Text\n)\n\n\n\n\n\n\n\n\n\n\nAdd the new markdown file to the \n./mkdocs.yml\n site configuration and continue editing. See\n\nWriting Your Docs\n for tips on arranging\nyour Markdown files.\n\n\n\n\n\n\nWhen editing is complete, commit and push the file to save your work.\n\n\ngit add --all\ngit commit -m \nmy new post\n\ngit push origin\n\n\n\n\n\n\n\n\n\nBuild and deploy the site.\n\n\nmkdocs gh-deploy", 
            "title": "How I Built My Site"
        }, 
        {
            "location": "/github/how-i-built-my-site/#how-i-built-my-site", 
            "text": "2017-10-12 \n   Discuss    I recently rebuilt my GitHub Pages site, switching my tech stack to  Python ,  MkDocs  and Material for MkDocs . The previous workflow had too many moving parts and the theme was hard to\nread.  It turns out that MkDocs has some nice automation hooks for GitHub Pages and it looks great\non mobile browsers with the  Material  theme. Since I use MkDocs for site generation at work, I am\nalready pretty familiar with it.  Writing documents in  Markdown  and turning them into web pages with a static site generator is the\nfastest and easiest way to post articles on your GitHub Pages site.  This format allows you to\ndrop down into HTML, when necessary, to enhance your page formatting, but you shouldn't need to do\nthis for most pages.", 
            "title": "How I Built My Site"
        }, 
        {
            "location": "/github/how-i-built-my-site/#install-python-and-mkdocs-packages", 
            "text": "If you are relying on a system Python, consider following these instructions: Install the Latest Python Versions on Mac OSX .  Install  MkDocs  and  Material for MkDocs . These two packages will bring along all of the necessary\ndependencies for a fully functioning site.  pip install mkdocs mkdocs-material", 
            "title": "Install Python and MkDocs Packages"
        }, 
        {
            "location": "/github/how-i-built-my-site/#sign-up-for-google-analytics", 
            "text": "Note  You must disable ad-blocking software in order to be able to see the Google Analytics page.    Navigate to  Google Analytics    Admin  Property   Create New Property  Account Name:  $YOUR_ACCOUNT_NAME  Website Name:  username.github.io  Website URL:  https://username.github.io  Go with the default configuration options - you can change these later.  Get Tracking ID    Note the Tracking ID (looks like:  UA-00000000-0 ) assigned to this property.", 
            "title": "Sign Up for Google Analytics"
        }, 
        {
            "location": "/github/how-i-built-my-site/#build-the-site", 
            "text": "Create a new repository  named  username.github.io , where  username  is\nyour username (or organization name) on GitHub.  It must match exactly, or it will not work.    Clone the repository locally.  Requires  SSH keys .  git clone git@github.com:username/username.github.io.git\ncd username.github.com    Create a new branch to store the source files for the site.  In this configuration, GitHub Pages\nwill be served from the  master  branch, which will contain the the static site generated by MkDocs.\nTo keep the  master  branch clean and avoid file conflicts, the source for the site will be stored\nin the  source  branch.  git checkout -b source    You will create a directory structure that looks like the following:  username.github.io/\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 docs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 css\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 custom.css\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500 requirements.txt    Create a  ./mkdocs.yml  site configuration file. Setting the  remote_branch  to  master  is\nimportant for the GitHub deployment hooks in MkDocs. Choose a Creative Commons license  for your site - I chose CC BY-NC-SA 4.0  for my site.  Add your Google\nAnalytics property Tracking ID to  google_analytics .  If you do not have a Tracking ID, then delete this\nline in the configuration file.  site_name: My Site\nsite_url:  http://username.github.io/ \nrepo_url:  https://github.com/username/username.github.io \nedit_uri:  \nsite_description: My Site\nsite_author: My Name\ncopyright:  a   rel= license   href= http://creativecommons.org/licenses/by-nc-sa/4.0/ img   alt= Creative Commons License   style= border-width:0   src= https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png   / /a \ngoogle_analytics: [ UA-00000000-0 ,  username.github.io ]\nremote_branch: master\n\nstrict: True\n\npages:\n- Home: index.md\n\ntheme: material\n\nextra:\n  font:\n    code:  Menlo \n\nextra_css:\n- css/custom.css\n\nmarkdown_extensions:\n- admonition\n- codehilite\n- pymdownx.tilde\n- toc:\n    permalink: True    Create a  ./docs/index.md  file.  Hello World!    Create a  ./docs/css/custom.css  file.  /* Avoid showing first header, i.e., the page title in the sidebar.   *   * https://github.com/mkdocs/mkdocs/issues/318#issuecomment-98520139   */  li . toctree-l3 : first-child   { \n   display :   none ;  }  code   { \n   font-family :   Menlo ,   monospace ;  }     Create a  ./requirements.txt  file, so that you can easily reinstall the necessary Python packages\nwith  pip install -r requirements.txt .  mkdocs\nmkdocs-material    Create a  ./.gitignore  file.  site    Build and serve the site locally, to verify that your changes look good.  When MkDocs is up and\nrunning, you will see  Serving on http://127.0.0.1:8000 . Leave this process running. When you are\ndone developing and testing your site, you can stop this process with  ctrl+c . Open a new Terminal\nto run the second command.  mkdocs serve\nopen http://localhost:8000    Push the source branch to GitHub to save your changes. You only need to set the upstream for the\nfirst push; subsequent pushes will be  git push origin  as long as you remain on the  source  branch.  git add --all\ngit commit -m  first commit \ngit push --set-upstream origin source    Build and deploy the site. The static HTML site generated by this process will be pushed to the master  branch origin for this repository. Changes will be live within one minute. Navigate to\nthe site url to see your changes.  mkdocs gh-deploy", 
            "title": "Build the Site"
        }, 
        {
            "location": "/github/how-i-built-my-site/#repo-configuration", 
            "text": "Navigate to the repository on GitHub and set some useful configuration options.   Code   Description  Code   Website  Code   Manage topics  Settings   Branches   Default branch:  source  Settings   Branches   Protected branches:  source  Check: Protect this branch  Check: Include administrators", 
            "title": "Repo Configuration"
        }, 
        {
            "location": "/github/how-i-built-my-site/#new-post-workflow", 
            "text": "Start serving the site locally, with file change detection.  mkdocs serve\nopen http://localhost:8000    Start a new post by creating a markdown file in the  ./docs  directory hierarchy.    Images can be served from a location such as  ./docs/images , with references as follows:  ! [ Link   Name ]( / images / my - file . png   Alt Text )     Add the new markdown file to the  ./mkdocs.yml  site configuration and continue editing. See Writing Your Docs  for tips on arranging\nyour Markdown files.    When editing is complete, commit and push the file to save your work.  git add --all\ngit commit -m  my new post \ngit push origin    Build and deploy the site.  mkdocs gh-deploy", 
            "title": "New Post Workflow"
        }, 
        {
            "location": "/linux/linux-ftrace-delivers-dtrace-like-functionality/", 
            "text": "Linux ftrace Delivers dtrace-like Functionality\n\n\n\n  \n2014-07-22\n\n  \nDiscuss\n\n\n\n\n\n\n\nBrendan Gregg\n recently released\n\nperf-tools\n, which is a collection of low-level\nperformance observability scripts akin to the \nDTraceToolkit\n.\n\n\nHe has a \nnew article\n posted on the tools at LWN.", 
            "title": "Linux ftrace Delivers dtrace-like Functionality"
        }, 
        {
            "location": "/linux/linux-ftrace-delivers-dtrace-like-functionality/#linux-ftrace-delivers-dtrace-like-functionality", 
            "text": "2014-07-22 \n   Discuss    Brendan Gregg  recently released perf-tools , which is a collection of low-level\nperformance observability scripts akin to the  DTraceToolkit .  He has a  new article  posted on the tools at LWN.", 
            "title": "Linux ftrace Delivers dtrace-like Functionality"
        }, 
        {
            "location": "/misc/clone-stash-pull-requests/", 
            "text": "Clone Stash Pull Requests\n\n\n\n  \n2015-11-18\n\n  \nDiscuss\n\n\n\n\n\n\n\nhttps://answers.atlassian.com/questions/179848/local-checkout-of-a-pull-request-in-stash\n\n\n[alias]\n\n  \nprstash\n \n=\n \n!f() { git fetch $1 refs/pull-requests/$2/from:$3; } ; f\n\n\n\n\n\n\n# where 3 is the 3rd pull request\n\ngit prstash origin \n3\n dest-branch\ngit checkout dest-branch\n\n\n# to reclone\n\ngit checkout master\ngit prstash origin \n3\n dest-branch\ngit checkout dest-branch", 
            "title": "Clone Stash Pull Requests"
        }, 
        {
            "location": "/misc/clone-stash-pull-requests/#clone-stash-pull-requests", 
            "text": "2015-11-18 \n   Discuss    https://answers.atlassian.com/questions/179848/local-checkout-of-a-pull-request-in-stash  [alias] \n   prstash   =   !f() { git fetch $1 refs/pull-requests/$2/from:$3; } ; f   # where 3 is the 3rd pull request \ngit prstash origin  3  dest-branch\ngit checkout dest-branch # to reclone \ngit checkout master\ngit prstash origin  3  dest-branch\ngit checkout dest-branch", 
            "title": "Clone Stash Pull Requests"
        }, 
        {
            "location": "/misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/", 
            "text": "Dual Boot OSX 10.10 and Ubuntu 14.04 on a 2013 Macbook Pro Retina\n\n\n\n  \n2015-06-24\n\n  \nDiscuss\n\n\n\n\n\n\n\nIntroduction\n\n\nWhy?  Some recent benchmarks have shown that Ubuntu can out-perform OSX on Macbook hardware.\n\n\n\n\nhttp://www.phoronix.com/scan.php?page=article\nitem=osx10_ubuntu1410\nnum=1\n\n\nhttp://www.phoronix.com/scan.php?page=article\nitem=ubuntu_1404_mba2013gl\nnum=1\n\n\n\n\nThis process was tested on a late 2013 Macbook11,2 A1398 model:\n\n\n\n\n2GHz Intel Core i7\n\n\n16GB 1600 MHz DDR3\n\n\nIntel Iris Pro 1536MB\n\n\n\n\nYou can check your Macbook model from Ubuntu like so:\n\n\nsudo dmidecode --type 1\n\n\n\n\n\nIssues\n\n\nThunderbolt Monitor support is not great.  It will work if the monitor is connected to the system at\nboot-up, but it does not support hot-plug.\n\n\n\n\nNote\n\n\nSee \nhttps://blog.jessfraz.com/post/linux-on-mac/\n for some new information on this.\n\n\nApparently, kernel 3.17 has support for hotplugging Thunderbolt connections.\n\n\nSee \nhttp://ubuntuhandbook.org/index.php/2014/11/how-to-upgrade-to-linux-kernel-3-17-4-in-ubuntu-14-10/\n for adding 3.17 to Ubuntu 14.\n\n\n\n\nInstalling\n\n\n\n\n\n\nPrepare Macbook for Ubuntu installation.\n\n\n\n\n\n\nPartition hard disk.\n\n\n\n\nFinder \n Applications \n Utilities \n Disk Utility\n\n\nMacintosh HD \n Partition\n\n\nAdd a \"Free Space\" partition equal to half the drive.\n\n\nApply and wait for the file system to shrink (30-60 minutes).\n\n\n\n\n\n\n\n\nDownload\n\na binary zip and install \nrEFInd Boot Manager\n.  This will\nallow you to switch between booting OSX 10.10 and Ubuntu 14.04.\n\n\nunzip refind-bin-0.8.4.zip\ncd refind-bin-0.8.4\n./install.sh --alldrivers\n\n\n\n\n\n\n\n\n\nDownload Ubuntu\n and create a\n\nbootable USB stick\n.\n\n\nhdiutil convert -format UDRW -o ~/path/to/target.img ~/path/to/ubuntu.iso\ndiskutil list\n# find usb disk\ndiskutil unmountDisk /dev/diskN\nsudo dd if=/path/to/downloaded.img of=/dev/rdiskN bs=1m\ndiskutil eject /dev/diskN\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Ubuntu.\n\n\n\n\nReboot, hold down the Option key and choose Ubuntu USB stick.\n\n\nGrub Boot Menu: Install Ubuntu\n\n\nContinue with the installation, without networking.\n\n\nInstallation Type \n Something Else\n\n\nThis will allow you to create a custom partition configuration and preserve your Mac OSX install.\n\n\n\n\nYou should see a partition list like the following:\n\n\n\n\n\n\n\n\nPartition\n\n\nType\n\n\nSize\n\n\n\n\n\n\n\n\n\n\n/dev/sda1\n\n\nefi\n\n\n209 MB\n\n\n\n\n\n\n/dev/sda2\n\n\nhfs+\n\n\n125441 MB\n\n\n\n\n\n\n/dev/sda3\n\n\nhfs+\n\n\n650 MB\n\n\n\n\n\n\nfree space\n\n\n\n\n124699 MB\n\n\n\n\n\n\n\n\n\n\n\n\nWithin the free space, create two new partitions:\n\n\n\n\nSwap space, equal to the same size as system RAM.\n\n\nAn Ext4 partition for the root mount point, consuming the remaining space.\n\n\n\n\n\n\nSelect the new root partition and continue the installation.\n\n\nTime Zone: Los Angeles\n\n\nKeyboard Layout: English (US) - English (Macintosh)\n\n\nWho Are You?  Re-use your existing username and computer name.\n\n\nReboot and choose Ubuntu installation from rEFInd menu.\n\n\n\n\n\n\n\n\nConfigure Ubuntu Environment.\n\n\n\n\n\n\nStart a Terminal\n\n\n\n\nUbuntu Search \n Terminal\n\n\nRight-click Launcher Icon \n Lock to Launcher\n\n\n\n\n\n\n\n\nConfigure \nwireless networking\n.\n\n\n\n\nInsert USB stick with Ubuntu installation media.\n\n\n\n\nInstall dkms and bcmwl-kernel-source packages.\n\n\ncd /media/`whoami`/Ubuntu\\ 14.04.1\\ LTS\\ amd64/\nsudo dpgk -i pool/main/d/dkms/dkms_2.2.0.3-1.1ubuntu5_all.deb\nsudo dpgk -i pool/restricted/b/bcmwl/bcmwl-kernel-source_6.30.223.141+bdcom-0ubuntu2_amd64.deb\n\n\n\n\n\n\n\n\n\nCreate a network manager wakeup script \n/etc/pm/sleep.d/99_wakeup\n and make it executable.\nThis will restore wireless networking when resuming from suspend.  Bugs\n\n1299282\n and\n\n1289884\n are tracking this issue.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n#!/bin/bash\n\n\n\ncase\n \n$1\n in\n  thaw\n|\nresume\n)\n\n    nmcli nm sleep \nfalse\n\n    \n;;\n\n\nesac\n\n\n\nexit\n \n$?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall binary drivers for Intel Iris Pro Graphics 5200.\n\n\n\n\nDownload the \n64-bit Ubuntu installer\n\nfrom \nhttps://01.org/linuxgraphics/\nsudo dpkg -i intel-linux-graphics-installer_1.0.7-0intel1_amd64.deb\nsudo apt-get install -f\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisable suspend when closing the lid.\n\n\nsudo vi /etc/systemd/logind.conf\n\nHandleLidSwitch=ignore\n\nsudo restart systemd-logind\n\n\n\n\n\n\n\n\n\nConfigure mouse behavior.\n\n\n\n\nSystem Settings \n Mouse and Touchpad\n\n\nCheck: Natural Scrolling\n\n\nUncheck: Disable while typing\n\n\nUncheck: Tap to click\n\n\n\n\n\n\n\n\nDisable turning the screen off.\n\n\n\n\nSystem Settings \n Brightness \n Lock\n\n\nTurn Screen Off When Inactive For: Never\n\n\nLock Screen After: 10 minutes\n\n\n\n\n\n\n\n\nAdd a screensaver.  The default gnome-screensaver is just a black screen.\n\n\nsudo apt-get remove gnome-screensaver\nsudo apt-get install xscreensaver xscreensaver-data-extra xscreensaver-gl-extra\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Applications\n\n\n\n\n\n\nOracle Java 7 and Java 8\n\n\nsudo add-apt-repository ppa:webupd8team/java\nsudo apt-get update\necho oracle-java7-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections\nsudo apt-get install oracle-java7-installer\n\necho oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections\nsudo apt-get install oracle-java8-installer\n\n# switching versions\nsudo update-java-alternatives -s java-7-oracle\nsudo update-java-alternatives -s java-8-oracle\nsudo apt-get install oracle-java8-set-default\n\n\n\n\n\n\n\n\n\nGoogle Chrome\n\n\n\n\nAs of version 35,\n\nChrome no longer supports the NPAPI plugin\n,\nwhich is required by the Oracle and OpenJDK Java plugins.  If you need to run a Java plugin\nfrom a web browser, use Firefox instead.\n\n\n\n\n\n\n\n\n\n\n\n\nTest Java version.\n\n\n\n\nhttps://www.java.com/en/download/testjava.jsp\n\n\n\n\n\n\n\n\nTest WiFi speed.\n\n\n\n\nDownload large files.\ncurl -O http://releases.ubuntu.com/14.10/ubuntu-14.10-desktop-amd64.iso\ncurl -O http://www.wswd.net/testdownloadfiles/1GB.zip\n\n\n\n\n\n\n\n\n\n\n\n\n\nUninstalling\n\n\n\n\n\n\nReboot into OSX.\n\n\n\n\n\n\nUninstall rEFInd.\n\n\ndiskutil list\ndiskutil mount disk0s1\nsudo su -\ncd /Volumes/EFI\nrm -rf BOOT ubuntu\n\n\n\n\n\n\n\n\n\nDownload \nGParted Live\n and install it on a USB stick.\n\n\nhdiutil convert -format UDRW -o ~/path/to/target.img ~/path/to/gparted.iso\ndiskutil list\n# find usb disk\ndiskutil unmountDisk /dev/diskN\nsudo dd if=/path/to/downloaded.img of=/dev/rdiskN bs=1m\ndiskutil eject /dev/diskN\n\n\n\n\n\n\n\n\n\nReboot and hold down the option key.\n\n\n\n\nRemove the Linux partitions.\n\n\n\n\nReboot into Mac OSX and extend the partition.\n\n\n\n\n\n\nCovert the CoreStorage volume back into HFS+\n.\n\n\ndiskutil cs list\ndiskutil coreStorage revert $VOLUME_UUID\n\n\n\n\n\n\n\n\n\nReboot.\n\n\n\n\nDelete the recovery partition.\n\n\nExtend the primary partition.\n\n\nRecreate the recovery partition\n.\n\n\n\n\nConvert the HFS+ volume back to CoreStorage\n.\n\n\ndiskutil cs convert \nMacintosh HD\n\n\n\n\n\n\n\n\n\n\nReboot.", 
            "title": "Dual Boot OSX 10.10 and Ubuntu 14.04 on a 2013 Macbook Pro Retina"
        }, 
        {
            "location": "/misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#dual-boot-osx-1010-and-ubuntu-1404-on-a-2013-macbook-pro-retina", 
            "text": "2015-06-24 \n   Discuss", 
            "title": "Dual Boot OSX 10.10 and Ubuntu 14.04 on a 2013 Macbook Pro Retina"
        }, 
        {
            "location": "/misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#introduction", 
            "text": "Why?  Some recent benchmarks have shown that Ubuntu can out-perform OSX on Macbook hardware.   http://www.phoronix.com/scan.php?page=article item=osx10_ubuntu1410 num=1  http://www.phoronix.com/scan.php?page=article item=ubuntu_1404_mba2013gl num=1   This process was tested on a late 2013 Macbook11,2 A1398 model:   2GHz Intel Core i7  16GB 1600 MHz DDR3  Intel Iris Pro 1536MB   You can check your Macbook model from Ubuntu like so:  sudo dmidecode --type 1", 
            "title": "Introduction"
        }, 
        {
            "location": "/misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#issues", 
            "text": "Thunderbolt Monitor support is not great.  It will work if the monitor is connected to the system at\nboot-up, but it does not support hot-plug.   Note  See  https://blog.jessfraz.com/post/linux-on-mac/  for some new information on this.  Apparently, kernel 3.17 has support for hotplugging Thunderbolt connections.  See  http://ubuntuhandbook.org/index.php/2014/11/how-to-upgrade-to-linux-kernel-3-17-4-in-ubuntu-14-10/  for adding 3.17 to Ubuntu 14.", 
            "title": "Issues"
        }, 
        {
            "location": "/misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#installing", 
            "text": "Prepare Macbook for Ubuntu installation.    Partition hard disk.   Finder   Applications   Utilities   Disk Utility  Macintosh HD   Partition  Add a \"Free Space\" partition equal to half the drive.  Apply and wait for the file system to shrink (30-60 minutes).     Download \na binary zip and install  rEFInd Boot Manager .  This will\nallow you to switch between booting OSX 10.10 and Ubuntu 14.04.  unzip refind-bin-0.8.4.zip\ncd refind-bin-0.8.4\n./install.sh --alldrivers    Download Ubuntu  and create a bootable USB stick .  hdiutil convert -format UDRW -o ~/path/to/target.img ~/path/to/ubuntu.iso\ndiskutil list\n# find usb disk\ndiskutil unmountDisk /dev/diskN\nsudo dd if=/path/to/downloaded.img of=/dev/rdiskN bs=1m\ndiskutil eject /dev/diskN      Install Ubuntu.   Reboot, hold down the Option key and choose Ubuntu USB stick.  Grub Boot Menu: Install Ubuntu  Continue with the installation, without networking.  Installation Type   Something Else  This will allow you to create a custom partition configuration and preserve your Mac OSX install.   You should see a partition list like the following:     Partition  Type  Size      /dev/sda1  efi  209 MB    /dev/sda2  hfs+  125441 MB    /dev/sda3  hfs+  650 MB    free space   124699 MB       Within the free space, create two new partitions:   Swap space, equal to the same size as system RAM.  An Ext4 partition for the root mount point, consuming the remaining space.    Select the new root partition and continue the installation.  Time Zone: Los Angeles  Keyboard Layout: English (US) - English (Macintosh)  Who Are You?  Re-use your existing username and computer name.  Reboot and choose Ubuntu installation from rEFInd menu.     Configure Ubuntu Environment.    Start a Terminal   Ubuntu Search   Terminal  Right-click Launcher Icon   Lock to Launcher     Configure  wireless networking .   Insert USB stick with Ubuntu installation media.   Install dkms and bcmwl-kernel-source packages.  cd /media/`whoami`/Ubuntu\\ 14.04.1\\ LTS\\ amd64/\nsudo dpgk -i pool/main/d/dkms/dkms_2.2.0.3-1.1ubuntu5_all.deb\nsudo dpgk -i pool/restricted/b/bcmwl/bcmwl-kernel-source_6.30.223.141+bdcom-0ubuntu2_amd64.deb    Create a network manager wakeup script  /etc/pm/sleep.d/99_wakeup  and make it executable.\nThis will restore wireless networking when resuming from suspend.  Bugs 1299282  and 1289884  are tracking this issue.  1\n2\n3\n4\n5\n6\n7\n8\n9 #!/bin/bash  case   $1  in\n  thaw | resume ) \n    nmcli nm sleep  false \n     ;;  esac  exit   $?        Install binary drivers for Intel Iris Pro Graphics 5200.   Download the  64-bit Ubuntu installer \nfrom  https://01.org/linuxgraphics/ sudo dpkg -i intel-linux-graphics-installer_1.0.7-0intel1_amd64.deb\nsudo apt-get install -f      Disable suspend when closing the lid.  sudo vi /etc/systemd/logind.conf\n\nHandleLidSwitch=ignore\n\nsudo restart systemd-logind    Configure mouse behavior.   System Settings   Mouse and Touchpad  Check: Natural Scrolling  Uncheck: Disable while typing  Uncheck: Tap to click     Disable turning the screen off.   System Settings   Brightness   Lock  Turn Screen Off When Inactive For: Never  Lock Screen After: 10 minutes     Add a screensaver.  The default gnome-screensaver is just a black screen.  sudo apt-get remove gnome-screensaver\nsudo apt-get install xscreensaver xscreensaver-data-extra xscreensaver-gl-extra      Install Applications    Oracle Java 7 and Java 8  sudo add-apt-repository ppa:webupd8team/java\nsudo apt-get update\necho oracle-java7-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections\nsudo apt-get install oracle-java7-installer\n\necho oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections\nsudo apt-get install oracle-java8-installer\n\n# switching versions\nsudo update-java-alternatives -s java-7-oracle\nsudo update-java-alternatives -s java-8-oracle\nsudo apt-get install oracle-java8-set-default    Google Chrome   As of version 35, Chrome no longer supports the NPAPI plugin ,\nwhich is required by the Oracle and OpenJDK Java plugins.  If you need to run a Java plugin\nfrom a web browser, use Firefox instead.       Test Java version.   https://www.java.com/en/download/testjava.jsp     Test WiFi speed.   Download large files. curl -O http://releases.ubuntu.com/14.10/ubuntu-14.10-desktop-amd64.iso\ncurl -O http://www.wswd.net/testdownloadfiles/1GB.zip", 
            "title": "Installing"
        }, 
        {
            "location": "/misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#uninstalling", 
            "text": "Reboot into OSX.    Uninstall rEFInd.  diskutil list\ndiskutil mount disk0s1\nsudo su -\ncd /Volumes/EFI\nrm -rf BOOT ubuntu    Download  GParted Live  and install it on a USB stick.  hdiutil convert -format UDRW -o ~/path/to/target.img ~/path/to/gparted.iso\ndiskutil list\n# find usb disk\ndiskutil unmountDisk /dev/diskN\nsudo dd if=/path/to/downloaded.img of=/dev/rdiskN bs=1m\ndiskutil eject /dev/diskN    Reboot and hold down the option key.   Remove the Linux partitions.   Reboot into Mac OSX and extend the partition.    Covert the CoreStorage volume back into HFS+ .  diskutil cs list\ndiskutil coreStorage revert $VOLUME_UUID    Reboot.   Delete the recovery partition.  Extend the primary partition.  Recreate the recovery partition .   Convert the HFS+ volume back to CoreStorage .  diskutil cs convert  Macintosh HD     Reboot.", 
            "title": "Uninstalling"
        }, 
        {
            "location": "/misc/reset-jenkins-build-number/", 
            "text": "Reset Jenkins Build Number\n\n\n\n  \n2017-03-14\n\n  \nDiscuss\n\n\n\n\n\n\n\nhttp://stackoverflow.com/questions/20901791/how-to-reset-build-number-in-jenkins\n\n\nGo to the Jenkins Script Console and enter:\n\n\nitem\n \n=\n \nJenkins\n.\ninstance\n.\ngetItemByFullName\n(\nyour-job-name-here\n)\n\n\n\n// remove all build history\n\n\nitem\n.\nbuilds\n.\neach\n()\n \n{\n \nbuild\n \n-\n\n  \nbuild\n.\ndelete\n()\n\n\n}\n\n\n\nitem\n.\nupdateNextBuildNumber\n(\n1\n)", 
            "title": "Reset Jenkins Build Number"
        }, 
        {
            "location": "/misc/reset-jenkins-build-number/#reset-jenkins-build-number", 
            "text": "2017-03-14 \n   Discuss    http://stackoverflow.com/questions/20901791/how-to-reset-build-number-in-jenkins  Go to the Jenkins Script Console and enter:  item   =   Jenkins . instance . getItemByFullName ( your-job-name-here )  // remove all build history  item . builds . each ()   {   build   - \n   build . delete ()  }  item . updateNextBuildNumber ( 1 )", 
            "title": "Reset Jenkins Build Number"
        }, 
        {
            "location": "/misc/switching-jdks-in-osx/", 
            "text": "Switching JDKs in OSX\n\n\n\n  \n2015-02-03\n\n  \nDiscuss\n\n\n\n\n\n\n\nAdd the following to your \n$HOME/.bash_profile\n:\n\n\nexport\n \nJAVA7_HOME\n=\n$(\n/usr/libexec/java_home -v \n1\n.7\n)\n\n\nexport\n \nJAVA8_HOME\n=\n$(\n/usr/libexec/java_home -v \n1\n.8\n)\n\n\nexport\n \nJAVA_HOME\n=\n$JAVA8_HOME\n\n\nswitch_java\n()\n \n{\n\n    \nif\n \necho\n \n$JAVA_HOME\n \n|\ngrep -q \n1\n.8\n;\n \nthen\n\n        \nexport\n \nJAVA_HOME\n=\n$JAVA7_HOME\n\n    \nelse\n\n        \nexport\n \nJAVA_HOME\n=\n$JAVA8_HOME\n\n    \nfi\n\n    \necho\n \nJAVA_HOME=\n$JAVA_HOME\n\n\n}", 
            "title": "Switching JDKs in OSX"
        }, 
        {
            "location": "/misc/switching-jdks-in-osx/#switching-jdks-in-osx", 
            "text": "2015-02-03 \n   Discuss    Add the following to your  $HOME/.bash_profile :  export   JAVA7_HOME = $( /usr/libexec/java_home -v  1 .7 )  export   JAVA8_HOME = $( /usr/libexec/java_home -v  1 .8 )  export   JAVA_HOME = $JAVA8_HOME \n\nswitch_java ()   { \n     if   echo   $JAVA_HOME   | grep -q  1 .8 ;   then \n         export   JAVA_HOME = $JAVA7_HOME \n     else \n         export   JAVA_HOME = $JAVA8_HOME \n     fi \n     echo   JAVA_HOME= $JAVA_HOME  }", 
            "title": "Switching JDKs in OSX"
        }, 
        {
            "location": "/python/accessing-google-apis-with-python/", 
            "text": "Accessing Google APIs with Python\n\n\n\n  \n2016-06-03\n\n  \nDiscuss\n\n\n\n\n\n\n\nLinks\n\n\n\n\nhttps://developers.google.com/admin-sdk/directory/v1/quickstart/python#prerequisites\n\n\nhttps://developers.google.com/admin-sdk/directory/v1/reference/users/list#try-it\n\n\nhttp://oauth2client.readthedocs.io/en/latest/source/oauth2client.service_account.html\n\n\nhttps://github.com/google/oauth2client/issues/401\n\n\nhttps://github.com/google/oauth2client/issues/418\n\n\nhttps://github.com/google/oauth2client/pull/420\n\n\n\n\nMethod\n\n\nimport\n \nhttplib2\n\n\nimport\n \nsecure_storage\n\n\nfrom\n \napiclient.discovery\n \nimport\n \nbuild\n\n\nfrom\n \noauth2client.service_account\n \nimport\n \nServiceAccountCredentials\n\n\n\nclient_email\n \n=\n \n...@developer.gserviceaccount.com\n\n\nscopes\n \n=\n \n[\n\n    \nhttps://www.googleapis.com/auth/admin.directory.user.readonly\n,\n\n    \nhttps://www.googleapis.com/auth/admin.directory.group.readonly\n,\n\n    \nhttps://www.googleapis.com/auth/apps.groups.settings\n\n\n]\n\n\nuname\n \n=\n \n...@example.com\n\n\nfname\n \n=\n \n/.../private_key.p12\n\n\n\ncreds\n \n=\n \nServiceAccountCredentials\n.\nfrom_p12_keyfile\n(\nclient_email\n,\n \nfname\n,\n \nscopes\n=\nscopes\n)\n\n\ndelegated_creds\n \n=\n \ncreds\n.\ncreate_delegated\n(\nuname\n)\n\n\n\nhttp_auth\n \n=\n \ndelegated_creds\n.\nauthorize\n(\nhttplib2\n.\nHttp\n())\n\n\nservice\n \n=\n \nbuild\n(\nadmin\n,\n \ndirectory_v1\n,\n \nhttp\n=\nhttp_auth\n)\n\n\nservice\n.\nusers\n()\n.\nlist\n(\ndomain\n=\nexample.com\n,\n \nmaxResults\n=\n10\n,\n \norderBy\n=\nemail\n)\n.\nexecute\n()\n\n\nservice\n.\ngroups\n()\n.\nlist\n(\ndomain\n=\nexample.com\n,\n \nmaxResults\n=\n10\n)\n.\nexecute\n()", 
            "title": "Accessing Google APIs with Python"
        }, 
        {
            "location": "/python/accessing-google-apis-with-python/#accessing-google-apis-with-python", 
            "text": "2016-06-03 \n   Discuss", 
            "title": "Accessing Google APIs with Python"
        }, 
        {
            "location": "/python/accessing-google-apis-with-python/#links", 
            "text": "https://developers.google.com/admin-sdk/directory/v1/quickstart/python#prerequisites  https://developers.google.com/admin-sdk/directory/v1/reference/users/list#try-it  http://oauth2client.readthedocs.io/en/latest/source/oauth2client.service_account.html  https://github.com/google/oauth2client/issues/401  https://github.com/google/oauth2client/issues/418  https://github.com/google/oauth2client/pull/420", 
            "title": "Links"
        }, 
        {
            "location": "/python/accessing-google-apis-with-python/#method", 
            "text": "import   httplib2  import   secure_storage  from   apiclient.discovery   import   build  from   oauth2client.service_account   import   ServiceAccountCredentials  client_email   =   ...@developer.gserviceaccount.com  scopes   =   [ \n     https://www.googleapis.com/auth/admin.directory.user.readonly , \n     https://www.googleapis.com/auth/admin.directory.group.readonly , \n     https://www.googleapis.com/auth/apps.groups.settings  ]  uname   =   ...@example.com  fname   =   /.../private_key.p12  creds   =   ServiceAccountCredentials . from_p12_keyfile ( client_email ,   fname ,   scopes = scopes )  delegated_creds   =   creds . create_delegated ( uname )  http_auth   =   delegated_creds . authorize ( httplib2 . Http ())  service   =   build ( admin ,   directory_v1 ,   http = http_auth )  service . users () . list ( domain = example.com ,   maxResults = 10 ,   orderBy = email ) . execute ()  service . groups () . list ( domain = example.com ,   maxResults = 10 ) . execute ()", 
            "title": "Method"
        }, 
        {
            "location": "/python/building-rest-apis-with-python-flask/", 
            "text": "Building REST APIs with Python Flask\n\n\n\n  \n2014-07-15\n\n  \nDiscuss\n\n\n\n\n\n\n\nMiguel Grinberg\n, author of\n\nFlask Web Development\n put together an excellent blog post on\n\nDesigning a RESTful API with Python and Flask\n.", 
            "title": "Building REST APIs with Python Flask"
        }, 
        {
            "location": "/python/building-rest-apis-with-python-flask/#building-rest-apis-with-python-flask", 
            "text": "2014-07-15 \n   Discuss    Miguel Grinberg , author of Flask Web Development  put together an excellent blog post on Designing a RESTful API with Python and Flask .", 
            "title": "Building REST APIs with Python Flask"
        }, 
        {
            "location": "/python/install-the-latest-python-versions-on-macosx/", 
            "text": "Install the Latest Python Versions on Mac OSX\n\n\n\n  \n2017-10-12\n\n  \nDiscuss\n\n\n\n\n\n\n\nOnce you have decided that Python is an \nawesome language to learn\n\nand you have heard all about the the \ncool features\n\nawaiting you, you are determined to install the latest versions on your laptop so that you can start\ndeveloping.\n\n\n\n\nWhat\u2019s New In Python 3.6\n - December 23, 2016\n\n\nWhat\u2019s New In Python 3.5\n - September 13, 2015\n\n\nWhat\u2019s New In Python 3.4\n - March 16, 2014\n\n\nWhat\u2019s New In Python 3.3\n - September 29, 2012\n\n\nWhat\u2019s New In Python 3.2\n - February 20, 2011\n\n\nWhat\u2019s New In Python 3.1\n - June 27, 2009\n\n\nWhat\u2019s New In Python 3.0\n - December 3, 2008\n\n\nWhat\u2019s New In Python 2.7\n - July 3, 2010\n\n\nWhat\u2019s New In Python 2.6\n - October 1, 2008\n\n\n\n\nYou may be a little concerned about the differences between Python 2 and Python 3, so you want\nto make sure that you have the latest versions of each available. This will enable you to switch\nbetween them easily and work on porting Python 2 programs to Python 3.\n\n\n\n\nShould I use Python 2 or Python 3 for my development activity?\n\n\nKey Differences between Python 2.7 and Python 3 with Examples\n\n\nPorting Python 2 Code to Python 3\n\n\nWriting code that runs under both Python2 and 3\n\n\n\n\nOS vendor Python versions lag behind the latest available and can only be updated by installing a\nmajor patch or bolting-on an additional Python installation:\n\n\n\n    \n \nOS Version \nSystem Python Version\n    \n \nOSX 10.13 \n2.7.10\n    \n \nOSX 10.12 \n2.7.10\n    \n \nOSX 10.11 \n2.7.10\n    \n \nUbuntu 16.04 LTS Xenial Xerus \n2.7.11\n\n    \n \nUbuntu 14.04 LTS Trusty Tahr \n2.7.5\n\n    \n \nUbuntu 12.04 LTS Precise Pangolin \n2.7.3\n\n\n\n\n\nOne way to work around this issue is to use \npyenv\n to install the\nversions of Python you want to use.  This tool provides a simplified build environment for installing\nthe Pythons you want, while providing a set of shell script shims that makes it easy to switch between\nthem.  This tool was inspired by and forked from \nrbenv\n and\n\nruby-build\n.\n\n\nInstall Python with pyenv\n\n\n\n\n\n\nStart a Terminal session.\n\n\n\n\n\n\nInstall \nHomebrew\n and Xcode Command Line Tools.\n\n\nxcode-select --install\n\n/usr/bin/ruby -e \n$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\n\n\n\n\n\n\n\n\n\n\nInstall \npyenv\n.\n\n\nbrew install pyenv\n\n\n\n\n\n\n\n\n\nIf you already have \npyenv\n installed, you can upgrade it to gain access to the latest Python\nversions.  The \npyenv\n tool is updated periodically with new formulas for the latest releases.\n\n\nbrew upgrade pyenv\n\n\n\n\n\n\n\n\n\nAdd configuration to your \n.bash_profile\n to initialize \npyenv\n every time you start a new\nTerminal. The \nPYENV_ROOT\n, which is where Python versions and packages will be installed,\ndefaults to \n$HOME/.pyenv\n.\n\n\ncat \n$HOME/.bash_profile \nEOF\n\nif which pyenv \n /dev/null 2\n1; then eval \n$(pyenv init -)\n; fi\nEOF\n\n\n\n\n\n\n\n\n\nLoad your \n.bash_profile\n configuration changes.\n\n\nsource $HOME/.bash_profile\n\n\n\n\n\n\n\n\n\nList the available Python versions.  Notice that you have access to several different\ndistributions: python.org (plain version numbers), anaconda, ironpython, jython, miniconda,\npypy and stackless.  We will install the standard Python versions released by\n\npython.org\n, otherwise known as CPython, because the\ninterpreter is written in C.\n\n\npyenv install --list\n\n\n\n\n\n\n\n\n\nIf you are running OSX 10.13, you will need to set the following environment variables, when you\ninstall new Python versions. See \n#988\n for more details.\n\n\nexport CFLAGS=\n-I$(brew --prefix openssl)/include\n\nexport LDFLAGS=\n-L$(brew --prefix openssl)/lib\n\n\n\n\n\n\n\n\n\n\nInstall Python versions.\n\n\npyenv install 2.7.14\npyenv install 3.6.3\n\n\n\n\n\n\n\n\n\nList the available Python versions.\n\n\npyenv versions\n\n\n\n\n\n\n\n\n\nActivate a Python version and verify that it is available.\n\n\npyenv global 2.7.14\npyenv versions\npython -V\n\n\n\n\n\n\n\n\n\nActivate another Python version and verify that it is available.\n\n\npyenv global 3.6.3\npyenv versions\npython -V\n\n\n\n\n\n\n\n\n\nIf desired, activate multiple Python versions and verify that they are available.\n\nPEP 394 -- The \"python\" Command on Unix-Like Systems\n\nexplains conventions for naming \npython\n binaries.\n\n\npyenv global 2.7.14 3.6.3\npyenv versions\npython -V\npython2 -V\npython2.7 -V\npython3 -V\npython3.6 -V\n\n\n\n\n\n\n\n\n\nCreate new directories for Python projects, add \npyenv local\n version files and verify the Python\nversions.  Your \npython\n version will automatically switch when you change into these directories.\n\n\nmkdir python2-project\ncd python2-project\npyenv local 2.7.13\ncat .python-version\npyenv local\npython -V\ncd ..\n\nmkdir python3-project\ncd python3-project\npyenv local 3.6.3\ncat .python-version\npyenv local\npython -V\ncd ..\n\n\n\n\n\n\n\n\n\nUseful Python Packages\n\n\nThis is a small collecton of useful packages that will help get you started doing useful things\nwith Python.\n\n\nInstall \nflake8\n, which is a static analyzer that enforces\ngood Python coding style and alerts you to coding mistakes.  Now that you have Python installed,\nyou can use the \npip\n command to install any additional modules that you want to use.  You will\nneed to install modules separately for each version of Python you are actively using.\n\n\npip install flake8\n\n\n\n\n\nInstall advanced Python Read-Eval-Print-Loop (REPL) packages, which will make it easier to explore\nPython code, because they grant access to tab completion and fancy editing capabilities.  The \nipython\n\ntool is a command line REPL, which can be exited with \nctrl-d\n.  Jupyter Notebook is a browser-based\nREPL that operates on individual cells of code.  The \nIPython Interactive Computing\n\nwebsite has more information on these tools.\n\n\npip install ipython jupyter\nipython\njupyter notebook\n\n\n\n\n\nInstall \nRequests: HTTP for Humans\n, which makes it easy\nto consume HTTP services. See GitHub's \nREST API v3\n documentation\nfor more details on endpoints that are avaialble.\n\n\npip install requests\n\n\n\n\n\npython\n\n\n \nimport\n \nrequests\n\n\n \nr\n \n=\n \nrequests\n.\nget\n(\nhttps://api.github.com/users/copperlight\n)\n\n\n \nr\n.\nok\n\n\nTrue\n\n\n \nr\n.\nstatus_code\n\n\n200\n\n\n \nr\n.\nheaders\n[\ncontent-type\n]\n\n\napplication/json; charset=utf8\n\n\n \nr\n.\nencoding\n\n\nutf-8\n\n\n \nr\n.\ntext\n\n\nu\n{\nlogin\n:\ncopperlight\n, ...}\n\n\n \nr\n.\njson\n()\n\n\n{\nu\npublic_repos\n:\n \n27\n,\n \n...\n}\n\n\n \nr\n.\njson\n()[\nlogin\n]\n\n\nu\ncopperlight\n\n\n\n\n\n\nInstall \nFlask: A Python Microframework\n, which can be used to quickly build\nsmall websites that automate everyday tasks.\n\n\npip install Flask\n\n\n\n\n\nfrom\n \nflask\n \nimport\n \nFlask\n\n\napp\n \n=\n \nFlask\n(\n__name__\n)\n\n\n\n@app.route\n(\n/\n)\n\n\ndef\n \nhello\n():\n\n    \nreturn\n \nHello World!\n\n\n\n\n\n\nFLASK_APP=hello.py flask run\ncurl http://127.0.0.1:5000\n\n\n\n\n\nIf you need Python package isolation on a per-project basis, because you have conflicting sets\nof packages specified for different projects, you can use \nvirtualenv\n\nto set up separate environments.  The \nvirtualenv\n tool establishes a clean room Python installation,\neither based upon the current version of Python in use, or a version specified on the command line.\nIf you have multiple Python versions activated in \npyenv\n, you can use \nvirtualenv\n to switch between\nthem easily.\n\n\npip install virtualenv\npip list\n\nmkdir python2-project\ncd python2-project\nvirtualenv -p python2.7 venv\nsource venv/bin/activate\npython -V\nwhich python\npip list\nwhich pip\ndeactivate\ncd ..\n\nmkdir python3-project\ncd python3-project\nvirtualenv -p python3.6 venv\nsource venv/bin/activate\npython -V\nwhich python\npip list\nwhich pip\ndeactivate\ncd ..\n\n\n\n\n\nAutomated Testing with Python\n\n\nWhile working on Python programs, you may be interested in automating testing.  The most common\nchoice for implementing this is \ntox\n, which will\nallow you to run tests with multiple different versions of Python, if needed.\n\n\nPython Packaging\n\n\nAfter completing a Python program or two, you will be interested in learning how to distribute them\neffectively.  The \nPython Packaging User Guide\n has some good advice\non this topic.\n\n\nQuickstart Commands\n\n\n# if you need homebrew\n\nxcode-select --install\n/usr/bin/ruby -e \n$(\ncurl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install\n)\n\n\n\n# install pyenv and latest major python versions\n\nbrew install pyenv\n\ncat \n$HOME\n/.bash_profile \nEOF\n\n\nif\n which pyenv \n /dev/null\n;\n \nthen\n \neval\n \n$(\npyenv init -\n)\n;\n \nfi\n\nEOF\n\n\nsource\n \n$HOME\n/.bash_profile\n\npyenv install \n2\n.7.14\npyenv install \n3\n.6.3\npyenv versions", 
            "title": "Install the Latest Python Versions on Mac OSX"
        }, 
        {
            "location": "/python/install-the-latest-python-versions-on-macosx/#install-the-latest-python-versions-on-mac-osx", 
            "text": "2017-10-12 \n   Discuss    Once you have decided that Python is an  awesome language to learn \nand you have heard all about the the  cool features \nawaiting you, you are determined to install the latest versions on your laptop so that you can start\ndeveloping.   What\u2019s New In Python 3.6  - December 23, 2016  What\u2019s New In Python 3.5  - September 13, 2015  What\u2019s New In Python 3.4  - March 16, 2014  What\u2019s New In Python 3.3  - September 29, 2012  What\u2019s New In Python 3.2  - February 20, 2011  What\u2019s New In Python 3.1  - June 27, 2009  What\u2019s New In Python 3.0  - December 3, 2008  What\u2019s New In Python 2.7  - July 3, 2010  What\u2019s New In Python 2.6  - October 1, 2008   You may be a little concerned about the differences between Python 2 and Python 3, so you want\nto make sure that you have the latest versions of each available. This will enable you to switch\nbetween them easily and work on porting Python 2 programs to Python 3.   Should I use Python 2 or Python 3 for my development activity?  Key Differences between Python 2.7 and Python 3 with Examples  Porting Python 2 Code to Python 3  Writing code that runs under both Python2 and 3   OS vendor Python versions lag behind the latest available and can only be updated by installing a\nmajor patch or bolting-on an additional Python installation:  \n       OS Version  System Python Version\n       OSX 10.13  2.7.10\n       OSX 10.12  2.7.10\n       OSX 10.11  2.7.10\n       Ubuntu 16.04 LTS Xenial Xerus  2.7.11 \n       Ubuntu 14.04 LTS Trusty Tahr  2.7.5 \n       Ubuntu 12.04 LTS Precise Pangolin  2.7.3   One way to work around this issue is to use  pyenv  to install the\nversions of Python you want to use.  This tool provides a simplified build environment for installing\nthe Pythons you want, while providing a set of shell script shims that makes it easy to switch between\nthem.  This tool was inspired by and forked from  rbenv  and ruby-build .", 
            "title": "Install the Latest Python Versions on Mac OSX"
        }, 
        {
            "location": "/python/install-the-latest-python-versions-on-macosx/#install-python-with-pyenv", 
            "text": "Start a Terminal session.    Install  Homebrew  and Xcode Command Line Tools.  xcode-select --install\n\n/usr/bin/ruby -e  $(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)     Install  pyenv .  brew install pyenv    If you already have  pyenv  installed, you can upgrade it to gain access to the latest Python\nversions.  The  pyenv  tool is updated periodically with new formulas for the latest releases.  brew upgrade pyenv    Add configuration to your  .bash_profile  to initialize  pyenv  every time you start a new\nTerminal. The  PYENV_ROOT , which is where Python versions and packages will be installed,\ndefaults to  $HOME/.pyenv .  cat  $HOME/.bash_profile  EOF \nif which pyenv   /dev/null 2 1; then eval  $(pyenv init -) ; fi\nEOF    Load your  .bash_profile  configuration changes.  source $HOME/.bash_profile    List the available Python versions.  Notice that you have access to several different\ndistributions: python.org (plain version numbers), anaconda, ironpython, jython, miniconda,\npypy and stackless.  We will install the standard Python versions released by python.org , otherwise known as CPython, because the\ninterpreter is written in C.  pyenv install --list    If you are running OSX 10.13, you will need to set the following environment variables, when you\ninstall new Python versions. See  #988  for more details.  export CFLAGS= -I$(brew --prefix openssl)/include \nexport LDFLAGS= -L$(brew --prefix openssl)/lib     Install Python versions.  pyenv install 2.7.14\npyenv install 3.6.3    List the available Python versions.  pyenv versions    Activate a Python version and verify that it is available.  pyenv global 2.7.14\npyenv versions\npython -V    Activate another Python version and verify that it is available.  pyenv global 3.6.3\npyenv versions\npython -V    If desired, activate multiple Python versions and verify that they are available. PEP 394 -- The \"python\" Command on Unix-Like Systems \nexplains conventions for naming  python  binaries.  pyenv global 2.7.14 3.6.3\npyenv versions\npython -V\npython2 -V\npython2.7 -V\npython3 -V\npython3.6 -V    Create new directories for Python projects, add  pyenv local  version files and verify the Python\nversions.  Your  python  version will automatically switch when you change into these directories.  mkdir python2-project\ncd python2-project\npyenv local 2.7.13\ncat .python-version\npyenv local\npython -V\ncd ..\n\nmkdir python3-project\ncd python3-project\npyenv local 3.6.3\ncat .python-version\npyenv local\npython -V\ncd ..", 
            "title": "Install Python with pyenv"
        }, 
        {
            "location": "/python/install-the-latest-python-versions-on-macosx/#useful-python-packages", 
            "text": "This is a small collecton of useful packages that will help get you started doing useful things\nwith Python.  Install  flake8 , which is a static analyzer that enforces\ngood Python coding style and alerts you to coding mistakes.  Now that you have Python installed,\nyou can use the  pip  command to install any additional modules that you want to use.  You will\nneed to install modules separately for each version of Python you are actively using.  pip install flake8  Install advanced Python Read-Eval-Print-Loop (REPL) packages, which will make it easier to explore\nPython code, because they grant access to tab completion and fancy editing capabilities.  The  ipython \ntool is a command line REPL, which can be exited with  ctrl-d .  Jupyter Notebook is a browser-based\nREPL that operates on individual cells of code.  The  IPython Interactive Computing \nwebsite has more information on these tools.  pip install ipython jupyter\nipython\njupyter notebook  Install  Requests: HTTP for Humans , which makes it easy\nto consume HTTP services. See GitHub's  REST API v3  documentation\nfor more details on endpoints that are avaialble.  pip install requests  python    import   requests    r   =   requests . get ( https://api.github.com/users/copperlight )    r . ok  True    r . status_code  200    r . headers [ content-type ]  application/json; charset=utf8    r . encoding  utf-8    r . text  u { login : copperlight , ...}    r . json ()  { u public_repos :   27 ,   ... }    r . json ()[ login ]  u copperlight   Install  Flask: A Python Microframework , which can be used to quickly build\nsmall websites that automate everyday tasks.  pip install Flask  from   flask   import   Flask  app   =   Flask ( __name__ )  @app.route ( / )  def   hello (): \n     return   Hello World!   FLASK_APP=hello.py flask run\ncurl http://127.0.0.1:5000  If you need Python package isolation on a per-project basis, because you have conflicting sets\nof packages specified for different projects, you can use  virtualenv \nto set up separate environments.  The  virtualenv  tool establishes a clean room Python installation,\neither based upon the current version of Python in use, or a version specified on the command line.\nIf you have multiple Python versions activated in  pyenv , you can use  virtualenv  to switch between\nthem easily.  pip install virtualenv\npip list\n\nmkdir python2-project\ncd python2-project\nvirtualenv -p python2.7 venv\nsource venv/bin/activate\npython -V\nwhich python\npip list\nwhich pip\ndeactivate\ncd ..\n\nmkdir python3-project\ncd python3-project\nvirtualenv -p python3.6 venv\nsource venv/bin/activate\npython -V\nwhich python\npip list\nwhich pip\ndeactivate\ncd ..", 
            "title": "Useful Python Packages"
        }, 
        {
            "location": "/python/install-the-latest-python-versions-on-macosx/#automated-testing-with-python", 
            "text": "While working on Python programs, you may be interested in automating testing.  The most common\nchoice for implementing this is  tox , which will\nallow you to run tests with multiple different versions of Python, if needed.", 
            "title": "Automated Testing with Python"
        }, 
        {
            "location": "/python/install-the-latest-python-versions-on-macosx/#python-packaging", 
            "text": "After completing a Python program or two, you will be interested in learning how to distribute them\neffectively.  The  Python Packaging User Guide  has some good advice\non this topic.", 
            "title": "Python Packaging"
        }, 
        {
            "location": "/python/install-the-latest-python-versions-on-macosx/#quickstart-commands", 
            "text": "# if you need homebrew \nxcode-select --install\n/usr/bin/ruby -e  $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install )  # install pyenv and latest major python versions \nbrew install pyenv\n\ncat  $HOME /.bash_profile  EOF  if  which pyenv   /dev/null ;   then   eval   $( pyenv init - ) ;   fi \nEOF source   $HOME /.bash_profile\n\npyenv install  2 .7.14\npyenv install  3 .6.3\npyenv versions", 
            "title": "Quickstart Commands"
        }, 
        {
            "location": "/python/memory-profiling-with-pyrasite-and-heapy/", 
            "text": "Memory Profiling with Pyrasite and Heapy\n\n\n\n  \n2017-09-17\n\n  \nDiscuss\n\n\n\n\n\n\n\n\n\n\n\nBuild or install Pyrasite from the develop branch in the GitHub repo.  The PyPi package does not\nhave the latest code, which allows you to control the IPC timeout.\n\n\n\n\nhttps://github.com/lmacken/pyrasite\n\n\n\n\n\n\n\n\nInstall profiling tools.\n\n\nsudo apt-get update\nsudo apt-get install pyrasite guppy\n\n\n\n\n\n\n\n\n\nEnable ptrace, so that you can inject into the process.\n\n\necho 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n\n\n\n\n\n\n\n\n\nConnect to the running process.\n\n\nexport PYRASITE_IPC_TIMEOUT=10  # default is 5 seconds\n\n/apps/python/bin/pyrasite-shell 3640\nPyrasite Shell 2.0\nConnected to \n...\n\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\n[GCC 5.4.0 20160609] on linux2\nType \nhelp\n, \ncopyright\n, \ncredits\n or \nlicense\n for more information.\n(DistantInteractiveConsole)\n\n\n\n\n\n\n\n\n\n\n\n\nEnumerate the threads in the current process.\n\n\nimport\n \nsys\n,\n \ntraceback\n\n\n\nfor\n \nthread\n,\n \nframe\n \nin\n \nsys\n.\n_current_frames\n()\n.\nitems\n():\n\n    \nprint\n(\nThread 0x\n%x\n \n%\n \nthread\n)\n\n    \ntraceback\n.\nprint_stack\n(\nframe\n)\n\n    \nprint\n()\n\n\n\n\n\n\n\n\n\n\nProfile process memory with heapy.\n\n\nfrom\n \nguppy\n \nimport\n \nhpy\n\n\nhp\n \n=\n \nhpy\n()\n\n\nhp\n.\nheap\n()\n\n\n\n\n\n\n\n\n\n\nDebug the pyrasite process, if needed.\n\n\n/apps/python/bin/pyrasite --verbose \npid\n helloworld.py", 
            "title": "Memory Profiling with Pyrasite and Heapy"
        }, 
        {
            "location": "/python/memory-profiling-with-pyrasite-and-heapy/#memory-profiling-with-pyrasite-and-heapy", 
            "text": "2017-09-17 \n   Discuss      Build or install Pyrasite from the develop branch in the GitHub repo.  The PyPi package does not\nhave the latest code, which allows you to control the IPC timeout.   https://github.com/lmacken/pyrasite     Install profiling tools.  sudo apt-get update\nsudo apt-get install pyrasite guppy    Enable ptrace, so that you can inject into the process.  echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope    Connect to the running process.  export PYRASITE_IPC_TIMEOUT=10  # default is 5 seconds\n\n/apps/python/bin/pyrasite-shell 3640\nPyrasite Shell 2.0\nConnected to  ... \nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\n[GCC 5.4.0 20160609] on linux2\nType  help ,  copyright ,  credits  or  license  for more information.\n(DistantInteractiveConsole)     Enumerate the threads in the current process.  import   sys ,   traceback  for   thread ,   frame   in   sys . _current_frames () . items (): \n     print ( Thread 0x %x   %   thread ) \n     traceback . print_stack ( frame ) \n     print ()     Profile process memory with heapy.  from   guppy   import   hpy  hp   =   hpy ()  hp . heap ()     Debug the pyrasite process, if needed.  /apps/python/bin/pyrasite --verbose  pid  helloworld.py", 
            "title": "Memory Profiling with Pyrasite and Heapy"
        }, 
        {
            "location": "/python/stop-using-print-for-debugging/", 
            "text": "Stop Using \"print\" for Debugging\n\n\n\n  \n2017-10-12\n\n  \nDiscuss\n\n\n\n\n\n\n\nMy favorite quickstart guide to the Python logging module, by \nAl Sweigart\n:\n\n\nStop Using \"print\" for Debugging: A 5 Minute Quickstart Guide to Python\u2019s logging Module\n\n\nThis is a slight tweak to that pattern, which adds the name of the logging source and an example of disabling selected log sources.\n\n\nimport\n \nlogging\n\n\nlogging\n.\nbasicConfig\n(\nlevel\n=\nlogging\n.\nDEBUG\n,\n \nformat\n=\n%(asctime)s\n - \n%(name)s\n - \n%(levelname)s\n - \n%(message)s\n)\n\n\nlogging\n.\ngetLogger\n(\nurllib3\n)\n.\nsetLevel\n(\nlogging\n.\nCRITICAL\n)\n\n\nlogging\n.\ndebug\n(\nThis is a log message.\n)", 
            "title": "Stop Using \"print\" for Debugging"
        }, 
        {
            "location": "/python/stop-using-print-for-debugging/#stop-using-print-for-debugging", 
            "text": "2017-10-12 \n   Discuss    My favorite quickstart guide to the Python logging module, by  Al Sweigart :  Stop Using \"print\" for Debugging: A 5 Minute Quickstart Guide to Python\u2019s logging Module  This is a slight tweak to that pattern, which adds the name of the logging source and an example of disabling selected log sources.  import   logging  logging . basicConfig ( level = logging . DEBUG ,   format = %(asctime)s  -  %(name)s  -  %(levelname)s  -  %(message)s )  logging . getLogger ( urllib3 ) . setLevel ( logging . CRITICAL )  logging . debug ( This is a log message. )", 
            "title": "Stop Using \"print\" for Debugging"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/", 
            "text": "IntelliJ Configuration Tips\n\n\n\n  \n2017-10-05\n\n  \nDiscuss\n\n\n\n\n\n\n\nThis page provides a few tips on configuring IntelliJ for working with Scala code.\n\n\nOptimize Imports\n\n\nThe preference for organizing imports is to keep them on a single line per import, rather than\nbunching them together with curly braces.  You can enforce this behavior with IntelliJ by modifying\nthe following configuration:\n\n\n\n\nIntelliJ IDEA \n Preferences\n\n\nCode Style \n Scala \n Imports\n\n\nUncheck: \nCollect imports with the same prefix into one import\n\n\n\n\nWhen you run Code \n Optimize Imports on a source file, it will unbundle imports\nand leave them with a single one per line.\n\n\nCode Style\n\n\nNavigate to these configuration options as follows:\n\n\n\n\nIntelliJ IDEA \n Preferences\n\n\nEditor \n Code Style \n Scala\n\n\n\n\nMethod declaration parameters\n\n\n\n\nUncheck: Align when multiline\n\n\n\n\nThis prevents Intellij from indenting parameter lists when cutting and pasting code within a file.\n\n\nMethod Signature Warnings\n\n\nNavigate to these configuration options as follows:\n\n\n\n\nIntelliJ IDEA \n Preferences\n\n\nEditor \n Inspections \n Scala \n Method Signature\n\n\n\n\nMethod with accessor-like name has Unit result type\n\n\nThe intent of this warning is to cover the following case:\n\n\n\n\nMethods that follow JavaBean naming contract for accessors are expected to have no side effects.\nHowever, methods with a result type of Unit are only executed for their side effects.\n\n\nRefer to Programming in Scala, 2.3 Define some functions\n\n\n\n\nThis indicates poor naming of the method involved, but depending on the code you are working with,\nthere may be multiple examples of this present.  You may want to set this to be a weak warning until\nyou can update method names to be more descriptive.\n\n\nMethod with Unit result type is defined like procedure\n\n\nThis inspection is disabled by default.  It should be enabled.\n\n\n\n\nIt is not recommended to use procedure-like syntax for methods with Unit return type. It is\ninconsistent, may lead to errors and will be deprecated in future versions of Scala.\n\n\nReference: The talk \"Scala with Style\" by Martin Odersky at ScalaDays 2013\n\n\nHint: You can use Analyze / Run Inspection by Name (Ctrl+Alt+Shift+I) to apply this inspection\nto the whole project\n\n\n\n\nScala Worksheet\n\n\nThis is a good way to gain access to an interactive repl-style experience for\nevaluating Scala code.  As long as the worksheet file is established within\nyour \nmain\n directory, you will have access to import all the dependencies of\nyour project.\n\n\nHowever, the default configuration of the Scala Worksheet will not be able to find\nAkka actor resources (\nSCL-9229\n).\nYou will receive the following error:\n\n\ncom.typesafe.config.ConfigException$Missing:\n  No configuration setting found for key \nakka\n\n\n\n\n\n\nThe work-around for this issue is as follows:\n\n\n\n\nIntelliJ IDEA \n Preferences\n\n\nLanguages \n Frameworks \n Scala \n Worksheet\n\n\nUncheck: \nRun worksheet in the compiler process\n\n\n\n\nZero-Latency Typing\n\n\nSee \nExperimental Zero-latency Typing in IntelliJ IDEA 15 EAP\n for an explanation of this feature.\nTo implement it, follow these steps:\n\n\n\n\nHelp \n Edit Custom Properties...\n\n\nAdd \neditor.zero.latency.typing=true\n\n\nRestart IntelliJ\n\n\n\n\nAt some point, this will become the default.", 
            "title": "IntelliJ Configuration Tips"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/#intellij-configuration-tips", 
            "text": "2017-10-05 \n   Discuss    This page provides a few tips on configuring IntelliJ for working with Scala code.", 
            "title": "IntelliJ Configuration Tips"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/#optimize-imports", 
            "text": "The preference for organizing imports is to keep them on a single line per import, rather than\nbunching them together with curly braces.  You can enforce this behavior with IntelliJ by modifying\nthe following configuration:   IntelliJ IDEA   Preferences  Code Style   Scala   Imports  Uncheck:  Collect imports with the same prefix into one import   When you run Code   Optimize Imports on a source file, it will unbundle imports\nand leave them with a single one per line.", 
            "title": "Optimize Imports"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/#code-style", 
            "text": "Navigate to these configuration options as follows:   IntelliJ IDEA   Preferences  Editor   Code Style   Scala", 
            "title": "Code Style"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/#method-declaration-parameters", 
            "text": "Uncheck: Align when multiline   This prevents Intellij from indenting parameter lists when cutting and pasting code within a file.", 
            "title": "Method declaration parameters"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/#method-signature-warnings", 
            "text": "Navigate to these configuration options as follows:   IntelliJ IDEA   Preferences  Editor   Inspections   Scala   Method Signature", 
            "title": "Method Signature Warnings"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/#method-with-accessor-like-name-has-unit-result-type", 
            "text": "The intent of this warning is to cover the following case:   Methods that follow JavaBean naming contract for accessors are expected to have no side effects.\nHowever, methods with a result type of Unit are only executed for their side effects.  Refer to Programming in Scala, 2.3 Define some functions   This indicates poor naming of the method involved, but depending on the code you are working with,\nthere may be multiple examples of this present.  You may want to set this to be a weak warning until\nyou can update method names to be more descriptive.", 
            "title": "Method with accessor-like name has Unit result type"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/#method-with-unit-result-type-is-defined-like-procedure", 
            "text": "This inspection is disabled by default.  It should be enabled.   It is not recommended to use procedure-like syntax for methods with Unit return type. It is\ninconsistent, may lead to errors and will be deprecated in future versions of Scala.  Reference: The talk \"Scala with Style\" by Martin Odersky at ScalaDays 2013  Hint: You can use Analyze / Run Inspection by Name (Ctrl+Alt+Shift+I) to apply this inspection\nto the whole project", 
            "title": "Method with Unit result type is defined like procedure"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/#scala-worksheet", 
            "text": "This is a good way to gain access to an interactive repl-style experience for\nevaluating Scala code.  As long as the worksheet file is established within\nyour  main  directory, you will have access to import all the dependencies of\nyour project.  However, the default configuration of the Scala Worksheet will not be able to find\nAkka actor resources ( SCL-9229 ).\nYou will receive the following error:  com.typesafe.config.ConfigException$Missing:\n  No configuration setting found for key  akka   The work-around for this issue is as follows:   IntelliJ IDEA   Preferences  Languages   Frameworks   Scala   Worksheet  Uncheck:  Run worksheet in the compiler process", 
            "title": "Scala Worksheet"
        }, 
        {
            "location": "/scala/intellij-configuration-tips/#zero-latency-typing", 
            "text": "See  Experimental Zero-latency Typing in IntelliJ IDEA 15 EAP  for an explanation of this feature.\nTo implement it, follow these steps:   Help   Edit Custom Properties...  Add  editor.zero.latency.typing=true  Restart IntelliJ   At some point, this will become the default.", 
            "title": "Zero-Latency Typing"
        }, 
        {
            "location": "/scala/overview-of-debugging-tools/", 
            "text": "Overview of Debugging Tools\n\n\n\n  \n2016-10-13\n\n  \nDiscuss\n\n\n\n\n\n\n\n\n\n\n    \n \nTool \nUse Cases\n    \n\n        \nJava Flight Recorder\n\n        \n\n            \n\n            \nfirst choice for GC allocation pauses and heap pressure\n            \nfirst choice for thread contention\n    \n\n        \nJava Flame Graphs\n\n        \n\n            \n\n            \nfirst choice for CPU contention\n    \n\n        \nVisualVM\n\n        \n\n            \n\n            \nsecond choice for CPU contention\n    \n\n        \nYourKit Profiler\n\n        \n\n            \n\n            \nthird choice for CPU contention, other options are more accessible\n            \nnot as good as flight recorder for heap pressure\n    \n\n        \nJava Micro Harness (JMH)\n\n        \n\n            \n\n            \nfirst choice for writing tiny benchmarks to evaluate code samples", 
            "title": "Overview of Debugging Tools"
        }, 
        {
            "location": "/scala/overview-of-debugging-tools/#overview-of-debugging-tools", 
            "text": "2016-10-13 \n   Discuss     \n       Tool  Use Cases\n     \n         Java Flight Recorder \n         \n             \n             first choice for GC allocation pauses and heap pressure\n             first choice for thread contention\n     \n         Java Flame Graphs \n         \n             \n             first choice for CPU contention\n     \n         VisualVM \n         \n             \n             second choice for CPU contention\n     \n         YourKit Profiler \n         \n             \n             third choice for CPU contention, other options are more accessible\n             not as good as flight recorder for heap pressure\n     \n         Java Micro Harness (JMH) \n         \n             \n             first choice for writing tiny benchmarks to evaluate code samples", 
            "title": "Overview of Debugging Tools"
        }, 
        {
            "location": "/scala/scalafmt-configuration-tips/", 
            "text": "Scalafmt Configuration Tips\n\n\n\n  \n2017-08-03\n\n  \nDiscuss\n\n\n\n\n\n\n\nScalafmt\n is a code formatter for Scala.  It is available as\nan \nIntelliJ plugin\n and a CLI version can\nbe installed using \nHomebrew\n.\n\n\nThe following \n.scalafmt.conf\n configuration is recommended for use with Insight projects:\n\n\nstyle = defaultWithAlign\n\nalign.openParenCallSite = false\nalign.openParenDefnSite = false\nalign.tokens = [{code = \n-\n}, {code = \n-\n}, {code = \n=\n, owner = \nCase\n}]\ncontinuationIndent.callSite = 2\ncontinuationIndent.defnSite = 2\ndanglingParentheses = true\nindentOperator = spray\nmaxColumn = 100\nnewlines.alwaysBeforeTopLevelStatements = true\nproject.excludeFilters = [\n.*\\\\.sbt\n]\nrewrite.rules = [RedundantParens, SortImports]\nspaces.inImportCurlyBraces = false\nunindentTopLevelOperators = true\n\n\n\n\n\nThis config file should be created in the project's root directory or the user's home directory. To\nenable formatting on file save in IntelliJ: IntelliJ IDEA \n Preferences \n Tools \n Scalafmt \n check:\nFormat on file save.\n\n\nBlank lines separate alignment blocks.\n\n\nFor sections of code that require formatting which does not follow the scalafmt conventions,\nuse \nformat:off\n blocks to disable the formatter.\n\n\nWhen formatter changes include altering whitespace to align operators, it can make diffs more\ndifficult to read.  You can use \ngit diff -w\n to ignore whitespace changes.\n\n\nThe \nmaxColumn\n value is chosen so that it is easy to do side-by-side views of code and tests,\nwhile also avoiding wrapping in diffs on GitGub and Stash.\n\n\nAlignment is allowed for case statements \n=\n and map assignments \n-\n and it is disallowed for\nassignment \n=\n.  This option taken together with the \nopenParen.*Site\n configuration minimizes\nthe amount of whitespace fiddling that will occur in code changes, which should lead to more\nreadable diffs.", 
            "title": "Scalafmt Configuration Tips"
        }, 
        {
            "location": "/scala/scalafmt-configuration-tips/#scalafmt-configuration-tips", 
            "text": "2017-08-03 \n   Discuss    Scalafmt  is a code formatter for Scala.  It is available as\nan  IntelliJ plugin  and a CLI version can\nbe installed using  Homebrew .  The following  .scalafmt.conf  configuration is recommended for use with Insight projects:  style = defaultWithAlign\n\nalign.openParenCallSite = false\nalign.openParenDefnSite = false\nalign.tokens = [{code =  - }, {code =  - }, {code =  = , owner =  Case }]\ncontinuationIndent.callSite = 2\ncontinuationIndent.defnSite = 2\ndanglingParentheses = true\nindentOperator = spray\nmaxColumn = 100\nnewlines.alwaysBeforeTopLevelStatements = true\nproject.excludeFilters = [ .*\\\\.sbt ]\nrewrite.rules = [RedundantParens, SortImports]\nspaces.inImportCurlyBraces = false\nunindentTopLevelOperators = true  This config file should be created in the project's root directory or the user's home directory. To\nenable formatting on file save in IntelliJ: IntelliJ IDEA   Preferences   Tools   Scalafmt   check:\nFormat on file save.  Blank lines separate alignment blocks.  For sections of code that require formatting which does not follow the scalafmt conventions,\nuse  format:off  blocks to disable the formatter.  When formatter changes include altering whitespace to align operators, it can make diffs more\ndifficult to read.  You can use  git diff -w  to ignore whitespace changes.  The  maxColumn  value is chosen so that it is easy to do side-by-side views of code and tests,\nwhile also avoiding wrapping in diffs on GitGub and Stash.  Alignment is allowed for case statements  =  and map assignments  -  and it is disallowed for\nassignment  = .  This option taken together with the  openParen.*Site  configuration minimizes\nthe amount of whitespace fiddling that will occur in code changes, which should lead to more\nreadable diffs.", 
            "title": "Scalafmt Configuration Tips"
        }, 
        {
            "location": "/scala/starting-scala-repls-with-gradle-and-sbt/", 
            "text": "Starting Scala REPLs with Gradle and SBT\n\n\n\n  \n2015-07-04\n\n  \nDiscuss\n\n\n\n\n\n\n\nThese examples are tailored for usage with the Atomic Scala exercises, but they demonstrate the\nprinciples.\n\n\nbuild.gradle\n\n\napply\n \nplugin:\n \nscala\n\n\n\nrepositories\n \n{\n\n    \nmavenCentral\n()\n\n\n}\n\n\n\next\n \n{\n\n    \nversions\n \n=\n \n[\n\n        \ncommons_math3:\n \n3.5\n,\n\n        \njline:\n \n2.12.1\n,\n\n        \nscala:\n \n2.11.6\n\n    \n]\n\n\n}\n\n\n\ndependencies\n \n{\n\n    \ncompile\n \norg.apache.commons:commons-math3:${versions.commons_math3}\n\n    \ncompile\n \norg.scala-lang:scala-library:${versions.scala}\n\n\n    \nruntime\n \njline:jline:${versions.jline}\n\n    \nruntime\n \norg.scala-lang:scala-compiler:${versions.scala}\n\n\n}\n\n\n\nsourceSets\n \n{\n\n    \nmain\n \n{\n\n        \nscala\n \n{\n\n            \nsrcDirs\n \n=\n \n[\n.\n]\n\n            \ninclude\n \nAtomicTest.scala\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\nscalaConsole\n.\ndependsOn\n(\nbuild\n)\n\n\nscalaConsole\n.\nclasspath\n \n+=\n \nsourceSets\n.\nmain\n.\nruntimeClasspath\n\n\n\n// usage: gradlew scalaConsole -q\n\n\n\n\n\n\nbuild.sbt\n\n\nscalaVersion\n \n:=\n \n2.11.6\n\n\n\nsources\n \nin\n \nCompile\n \n=\n \n(\nsources\n \nin\n \nCompile\n).\nmap\n(\n_\n \nfilter\n(\n_\n.\nname\n \n==\n \nAtomicTest.scala\n))\n\n\n\nlibraryDependencies\n \n+=\n \norg.apache.commons\n \n%\n \ncommons-math3\n \n%\n \n3.5\n\n\n\ninitialCommands\n \nin\n \nconsole\n \n:=\n \n\n\n    |import org.apache.commons.math3._\n\n\n    |import com.atomicscala.AtomicTest._\n\n\n    |\n.\nstripMargin\n\n\n\n// usage: sbt console", 
            "title": "Starting Scala REPLs with Gradle and SBT"
        }, 
        {
            "location": "/scala/starting-scala-repls-with-gradle-and-sbt/#starting-scala-repls-with-gradle-and-sbt", 
            "text": "2015-07-04 \n   Discuss    These examples are tailored for usage with the Atomic Scala exercises, but they demonstrate the\nprinciples.  build.gradle  apply   plugin:   scala  repositories   { \n     mavenCentral ()  }  ext   { \n     versions   =   [ \n         commons_math3:   3.5 , \n         jline:   2.12.1 , \n         scala:   2.11.6 \n     ]  }  dependencies   { \n     compile   org.apache.commons:commons-math3:${versions.commons_math3} \n     compile   org.scala-lang:scala-library:${versions.scala} \n\n     runtime   jline:jline:${versions.jline} \n     runtime   org.scala-lang:scala-compiler:${versions.scala}  }  sourceSets   { \n     main   { \n         scala   { \n             srcDirs   =   [ . ] \n             include   AtomicTest.scala \n         } \n     }  }  scalaConsole . dependsOn ( build )  scalaConsole . classpath   +=   sourceSets . main . runtimeClasspath  // usage: gradlew scalaConsole -q   build.sbt  scalaVersion   :=   2.11.6  sources   in   Compile   =   ( sources   in   Compile ). map ( _   filter ( _ . name   ==   AtomicTest.scala ))  libraryDependencies   +=   org.apache.commons   %   commons-math3   %   3.5  initialCommands   in   console   :=        |import org.apache.commons.math3._      |import com.atomicscala.AtomicTest._      | . stripMargin  // usage: sbt console", 
            "title": "Starting Scala REPLs with Gradle and SBT"
        }, 
        {
            "location": "/scala/testing-aws-clients-with-iam-assumerole-credentials-in-scala/", 
            "text": "Testing AWS Clients with IAM AssumeRole Credentials in Scala\n\n\n\n  \n2017-09-28\n\n  \nDiscuss\n\n\n\n\n\n\n\nThis technique is meant to be used with IntelliJ Scala worksheets or similar scratch code, so you\ncan test clients and validate their behavior. Don't check-in secrets \u2013 use on-instance credentials\nwith the \nDefaultAWSCredentialsProviderChain\n in production code.\n\n\nimport\n \ncom.amazonaws.auth.AWSStaticCredentialsProvider\n\n\nimport\n \ncom.amazonaws.auth.BasicSessionCredentials\n\n\nimport\n \ncom.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder\n\n\nimport\n \ncom.amazonaws.services.securitytoken.model.AssumeRoleRequest\n\n\nimport\n \ncom.amazonaws.ClientConfiguration\n\n\nimport\n \ncom.amazonaws.regions.Regions\n\n\nimport\n \ncom.amazonaws.services.sqs.AmazonSQSClientBuilder\n\n\nimport\n \nscala.collection.JavaConverters._\n\n\n\n// curl http://169.254.169.254/latest/meta-data/iam/security-credentials/MyProfile\n\n\nval\n \ninstanceCreds\n \n=\n \nnew\n \nBasicSessionCredentials\n(\n\n  \nAccessKeyId\n,\n\n  \nSecretAccessKey\n,\n\n  \nToken\n\n\n)\n\n\n\nval\n \nstsClient\n \n=\n \nAWSSecurityTokenServiceClientBuilder\n\n  \n.\nstandard\n()\n\n  \n.\nwithCredentials\n(\nnew\n \nAWSStaticCredentialsProvider\n(\ninstanceCreds\n))\n\n  \n.\nwithRegion\n(\nRegions\n.\nUS_EAST_1\n)\n\n  \n.\nbuild\n()\n\n\n\nval\n \naccountId\n \n=\n \n\n\nval\n \nrole\n \n=\n \n\n\n\nval\n \nassumeRoleResult\n \n=\n \nstsClient\n.\nassumeRole\n(\n\n  \nnew\n \nAssumeRoleRequest\n()\n\n    \n.\nwithRoleSessionName\n(\nTestAssumeRole\n)\n\n    \n.\nwithRoleArn\n(\ns\narn:aws:iam::\n$accountId\n:role/\n$role\n)\n\n\n)\n\n\n\nval\n \nstsCredentials\n \n=\n \nassumeRoleResult\n.\ngetCredentials\n\n\n\nval\n \nassumeCreds\n \n=\n \nnew\n \nBasicSessionCredentials\n(\n\n  \nstsCredentials\n.\ngetAccessKeyId\n,\n\n  \nstsCredentials\n.\ngetSecretAccessKey\n,\n\n  \nstsCredentials\n.\ngetSessionToken\n\n\n)\n\n\n\nval\n \nsqsClient\n \n=\n \nAmazonSQSClientBuilder\n\n  \n.\nstandard\n()\n\n  \n.\nwithCredentials\n(\nnew\n \nAWSStaticCredentialsProvider\n(\nassumeCreds\n))\n\n  \n.\nwithClientConfiguration\n(\nnew\n \nClientConfiguration\n())\n\n  \n.\nwithRegion\n(\nRegions\n.\nUS_EAST_1\n)\n\n  \n.\nbuild\n()\n\n\n\nsqsClient\n.\nlistQueues\n().\ngetQueueUrls\n.\nasScala\n.\nforeach\n(\nprintln\n(\n_\n))", 
            "title": "Testing AWS Clients with IAM AssumeRole Credentials in Scala"
        }, 
        {
            "location": "/scala/testing-aws-clients-with-iam-assumerole-credentials-in-scala/#testing-aws-clients-with-iam-assumerole-credentials-in-scala", 
            "text": "2017-09-28 \n   Discuss    This technique is meant to be used with IntelliJ Scala worksheets or similar scratch code, so you\ncan test clients and validate their behavior. Don't check-in secrets \u2013 use on-instance credentials\nwith the  DefaultAWSCredentialsProviderChain  in production code.  import   com.amazonaws.auth.AWSStaticCredentialsProvider  import   com.amazonaws.auth.BasicSessionCredentials  import   com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder  import   com.amazonaws.services.securitytoken.model.AssumeRoleRequest  import   com.amazonaws.ClientConfiguration  import   com.amazonaws.regions.Regions  import   com.amazonaws.services.sqs.AmazonSQSClientBuilder  import   scala.collection.JavaConverters._  // curl http://169.254.169.254/latest/meta-data/iam/security-credentials/MyProfile  val   instanceCreds   =   new   BasicSessionCredentials ( \n   AccessKeyId , \n   SecretAccessKey , \n   Token  )  val   stsClient   =   AWSSecurityTokenServiceClientBuilder \n   . standard () \n   . withCredentials ( new   AWSStaticCredentialsProvider ( instanceCreds )) \n   . withRegion ( Regions . US_EAST_1 ) \n   . build ()  val   accountId   =    val   role   =    val   assumeRoleResult   =   stsClient . assumeRole ( \n   new   AssumeRoleRequest () \n     . withRoleSessionName ( TestAssumeRole ) \n     . withRoleArn ( s arn:aws:iam:: $accountId :role/ $role )  )  val   stsCredentials   =   assumeRoleResult . getCredentials  val   assumeCreds   =   new   BasicSessionCredentials ( \n   stsCredentials . getAccessKeyId , \n   stsCredentials . getSecretAccessKey , \n   stsCredentials . getSessionToken  )  val   sqsClient   =   AmazonSQSClientBuilder \n   . standard () \n   . withCredentials ( new   AWSStaticCredentialsProvider ( assumeCreds )) \n   . withClientConfiguration ( new   ClientConfiguration ()) \n   . withRegion ( Regions . US_EAST_1 ) \n   . build ()  sqsClient . listQueues (). getQueueUrls . asScala . foreach ( println ( _ ))", 
            "title": "Testing AWS Clients with IAM AssumeRole Credentials in Scala"
        }, 
        {
            "location": "/scala/using-gradle-to-run-scala-projects-locally/", 
            "text": "Using Gradle to Run Scala Projects Locally\n\n\n\n  \n2017-10-12\n\n  \nDiscuss\n\n\n\n\n\n\n\nRunning projects locally in a minimal fashion is often useful for understanding the code, using a\ndebugger and performing interactive integration testing.\n\n\nYou can run REPLs with \n./gradlew\n, but due to the input fiddling that is going on, it's very\ndistracting and not effective, especially if you need to build up anything more than simple state.\nYou will be better served by using the \nIntelliJ Scala Worksheet\n, which makes it easy to gain\naccess to the libraries you have included in your build.\n\n\nbuildscript\n \n{\n\n    \ndependencies\n \n{\n\n        \nclasspath\n \ngradle.plugin.com.github.maiflai:gradle-scalatest:0.14\n\n        \nclasspath\n \norg.akhikhl.gretty:gretty:2.0.0\n\n    \n}\n\n\n}\n\n\n\napply\n \nplugin:\n \ncom.github.maiflai.scalatest\n\n\napply\n \nplugin:\n \norg.akhikhl.gretty\n\n\n\ntasks\n.\nwithType\n(\nScalaCompile\n)\n \n{\n\n    \nscalaCompileOptions\n.\nsetAdditionalParameters\n([\n\n        \n-deprecation\n,\n\n        \n-unchecked\n,\n\n        \n-Xexperimental\n,\n\n        \n-Xlint:_,-infer-any\n,\n\n        \n-feature\n,\n\n        \n-Ydelambdafy:method\n\n    \n])\n\n\n}\n\n\n\n// for servlet applications\n\n\ngretty\n \n{\n\n    \nhttpPort\n \n=\n \n7101\n\n    \ncontextPath\n \n=\n \n/\n\n    \nservletContainer\n \n=\n \njetty7\n\n    \njvmArgs\n \n=\n \n[\n\n        \n-Dlog4j.configurationFile=\n \n+\n \nSystem\n.\ngetProperty\n(\nuser.dir\n)\n \n+\n \n/src/main/resources/log4j_dev.xml\n\n    \n]\n\n\n}\n\n\n\n// for main class applications\n\n\ntask\n \nrunService\n(\ndependsOn:\n \nclasses\n,\n \ntype:\n \nJavaExec\n)\n \n{\n\n    \nmain\n \n=\n \ncom.example.app.Main\n\n    \nclasspath\n \nsourceSets\n.\nmain\n.\nruntimeClasspath\n\n\n}\n\n\n\n\n\n\n./gradlew appRun\n./gradlew runService", 
            "title": "Using Gradle to Run Scala Projects Locally"
        }, 
        {
            "location": "/scala/using-gradle-to-run-scala-projects-locally/#using-gradle-to-run-scala-projects-locally", 
            "text": "2017-10-12 \n   Discuss    Running projects locally in a minimal fashion is often useful for understanding the code, using a\ndebugger and performing interactive integration testing.  You can run REPLs with  ./gradlew , but due to the input fiddling that is going on, it's very\ndistracting and not effective, especially if you need to build up anything more than simple state.\nYou will be better served by using the  IntelliJ Scala Worksheet , which makes it easy to gain\naccess to the libraries you have included in your build.  buildscript   { \n     dependencies   { \n         classpath   gradle.plugin.com.github.maiflai:gradle-scalatest:0.14 \n         classpath   org.akhikhl.gretty:gretty:2.0.0 \n     }  }  apply   plugin:   com.github.maiflai.scalatest  apply   plugin:   org.akhikhl.gretty  tasks . withType ( ScalaCompile )   { \n     scalaCompileOptions . setAdditionalParameters ([ \n         -deprecation , \n         -unchecked , \n         -Xexperimental , \n         -Xlint:_,-infer-any , \n         -feature , \n         -Ydelambdafy:method \n     ])  }  // for servlet applications  gretty   { \n     httpPort   =   7101 \n     contextPath   =   / \n     servletContainer   =   jetty7 \n     jvmArgs   =   [ \n         -Dlog4j.configurationFile=   +   System . getProperty ( user.dir )   +   /src/main/resources/log4j_dev.xml \n     ]  }  // for main class applications  task   runService ( dependsOn:   classes ,   type:   JavaExec )   { \n     main   =   com.example.app.Main \n     classpath   sourceSets . main . runtimeClasspath  }   ./gradlew appRun\n./gradlew runService", 
            "title": "Using Gradle to Run Scala Projects Locally"
        }, 
        {
            "location": "/scala/using-the-scala-repl-to-configure-amazon-ses-notifications/", 
            "text": "Using the Scala REPL to Configure Amazon SES Notifications\n\n\n\n  \n2014-06-30\n\n  \nDiscuss\n\n\n\n\n\n\n\nLet's say you want to configure bounce notifications for SES emails using the Scala REPL.  How do\nyou go about accessing the AWS API to make it happen?  Make sure to have the\n\nAWS SDK for Java API Reference\n available\nfor additional details.\n\n\nInstall Scala and SBT:\n\n\nbrew install scala sbt\n\n\n\n\n\nCreate an AWS credentials file at \n~/.aws/credentials\n that is formatted like so:\n\n\n[default]\n\n\naws_access_key_id\n=\n...\n\n\naws_secret_access_key\n=\n...\n\n\naws_session_token\n=\n...\n\n\n\n\n\n\nCreate a \nbuild.sbt\n file that looks like:\n\n\nname\n \n:=\n \naws-sdk-client\n\n\n\nversion\n \n:=\n \n1.0\n\n\n\nscalaVersion\n \n:=\n \n2.11.0\n\n\n\nlibraryDependencies\n \n++=\n \nSeq\n(\n\n    \ncommons-logging\n \n%\n \ncommons-logging\n \n%\n \n1.1.3\n,\n\n    \ncom.amazonaws\n \n%\n \naws-java-sdk\n \n%\n \n1.8.2\n\n\n)\n\n\n\n\n\n\nYou can then engage the Scala REPL to talk to the AWS API like so (Scala will download the AWS SDK\nfrom Maven Central automatically):\n\n\nsbt\nconsole\n\n\n\n\n\nimport\n \ncom.amazonaws.auth._\n\n\nimport\n \ncom.amazonaws.auth.profile._\n\n\nimport\n \ncom.amazonaws.services.simpleemail._\n\n\nimport\n \ncom.amazonaws.services.simpleemail.model._\n\n\nimport\n \ncom.amazonaws.services.sns._\n\n\nimport\n \ncom.amazonaws.services.sns.model._\n\n\n\nval\n \nc\n \n=\n \nnew\n \nAmazonSNSClient\n(\n\n    \nnew\n \nAWSCredentialsProviderChain\n(\n\n        \n// attempt on-instance credentials first\n\n        \nnew\n \nInstanceProfileCredentialsProvider\n(),\n\n        \n// fallback to aws credentials file\n\n        \nnew\n \nProfileCredentialsProvider\n()\n\n    \n)\n\n\n)\n\n\n\nc\n.\nsetEndpoint\n(\nsns.us-east-1.amazonaws.com\n)\n\n\n\nc\n.\ncreateTopic\n(\nses-email-bounce\n)\n\n\n\nres0\n:\n \ncom.amazonaws.services.sns.model.CreateTopicResult\n \n=\n\n\n{\nTopicArn\n:\n \narn:aws:sns:us-east-\n1\n:\n000000000000\n:ses-email-bounce\n}\n\n\n\nval\n \nc\n \n=\n \nnew\n \nAmazonSimpleEmailServiceClient\n(\n\n    \nnew\n \nAWSCredentialsProviderChain\n(\n\n        \n// attempt on-instance credentials first\n\n        \nnew\n \nInstanceProfileCredentialsProvider\n(),\n\n        \n// fallback to aws credentials file\n\n        \nnew\n \nProfileCredentialsProvider\n()\n\n    \n)\n\n\n)\n\n\n\nc\n.\nsetEndpoint\n(\nemail.us-east-1.amazonaws.com\n)\n\n\n\nc\n.\ngetSendQuota\n()\n\n\n\nres1\n:\n \ncom.amazonaws.services.simpleemail.model.GetSendQuotaResult\n \n=\n\n\n{\nMax24HourSend\n:\n \n100\n.\n0\n,\nMaxSendRate\n:\n \n10\n.\n0\n,\nSentLast24Hours\n:\n \n1\n.\n0\n}\n\n\n\nc\n.\ngetSendStatistics\n()\n\n\n\nres2\n:\n \ncom.amazonaws.services.simpleemail.model.GetSendStatisticsResult\n \n=\n\n\n{\nSendDataPoints\n:\n \n[\n{\nTimestamp:\n \n...\n,\nDeliveryAttempts:\n \n0\n,\nBounces\n:\n \n0\n,\nComplaints\n:\n \n0\n,\nRejects\n:\n \n0\n},\n \n...\n\n\n\nc\n.\ngetIdentityVerificationAttributes\n(\n\n    \nnew\n \nGetIdentityVerificationAttributesRequest\n()\n\n        \n.\nwithIdentities\n(\nsome.email@example.com\n)\n\n\n)\n\n\n\nres3\n:\n \ncom.amazonaws.services.simpleemail.model.GetIdentityVerificationAttributesResult\n \n=\n\n\n{\nVerificationAttributes\n:\n \n{\nsome.email@mydomain.com\n={\nVerificationStatus:\n \nSuccess\n,}}}\n\n\n\nc\n.\nsetIdentityNotificationTopic\n(\n\n    \nnew\n \nSetIdentityNotificationTopicRequest\n()\n\n        \n.\nwithIdentity\n(\nsome.email@example.com\n)\n\n        \n.\nwithNotificationType\n(\nBounce\n)\n\n        \n.\nwithSnsTopic\n(\ns\narn:aws:sns:us-east-1:\n$accountId\n:ses-email-bounce\n)\n\n\n)\n\n\n\nc\n.\ngetIdentityNotificationAttributes\n(\n\n    \nnew\n \nGetIdentityNotificationAttributesRequest\n()\n\n        \n.\nwithIdentities\n(\nsome.email@example.com\n)\n\n\n)\n\n\n\n\n\n\nOne of the nice things about using the Scala REPL is that you get some useful tab completions (SES\nobject here):\n\n\nscala\n c.\naddRequestHandler                   sendEmail\nasInstanceOf                        sendRawEmail\ndeleteIdentity                      setConfiguration\ndeleteVerifiedEmailAddress          setEndpoint\ngetCachedResponseMetadata           setIdentityDkimEnabled\ngetIdentityDkimAttributes           setIdentityFeedbackForwardingEnabled\ngetIdentityNotificationAttributes   setIdentityNotificationTopic\ngetIdentityVerificationAttributes   setRegion\ngetRequestMetricsCollector          setServiceNameIntern\ngetSendQuota                        setSignerRegionOverride\ngetSendStatistics                   setTimeOffset\ngetServiceName                      shutdown\ngetSignerByURI                      toString\ngetSignerRegionOverride             verifyDomainDkim\ngetTimeOffset                       verifyDomainIdentity\nisInstanceOf                        verifyEmailAddress\nlistIdentities                      verifyEmailIdentity\nlistVerifiedEmailAddresses          withTimeOffset\nremoveRequestHandler\n\n\n\n\n\nOnce you have a feel for how the Scala REPL works with a library, you can create alternate entry\npoint for sbt called \nscalas\n which allows you to write full Scala scripts with library\ndependencies.  See \nScripts, REPL, and Dependencies\n\nfor additional details.  In brief, create a \nscalas\n script and make it available on your \nPATH\n:\n\n\n#!/bin/sh\n\n\ntest\n -f ~/.sbtconfig \n . ~/.sbtconfig\n\nexec\n java \n\\\n\n    -Xms512M \n\\\n\n    -Xmx1536M \n\\\n\n    -Xss1M \n\\\n\n    -XX:+CMSClassUnloadingEnabled \n\\\n\n    -XX:MaxPermSize\n=\n256M \n\\\n\n    \n${\nSBT_OPTS\n}\n \n\\\n\n    -jar /usr/local/Cellar/sbt/0.13.2/libexec/sbt-launch.jar \n\\\n\n    -Dsbt.main.class\n=\nsbt.ScriptMain \n\\\n\n    \n$@\n\n\n\n\n\n\nYou can then write Scala scripts like so:\n\n\n#!/\nusr\n/\nbin\n/\nenv\n \nscalas\n\n\n\n/***\n\n\n    scalaVersion := \n2.11.0\n\n\n\n    libraryDependencies ++= Seq(\n\n\n        \ncommons-logging\n % \ncommons-logging\n % \n1.1.3\n,\n\n\n        \ncom.amazonaws\n % \naws-java-sdk\n % \n1.8.2\n\n\n    )\n\n\n*/\n\n\n\nimport\n \ncom.amazonaws.auth._\n\n\nimport\n \ncom.amazonaws.auth.profile._\n\n\nimport\n \ncom.amazonaws.services.simpleemail._\n\n\nimport\n \ncom.amazonaws.services.simpleemail.model._\n\n\n\nval\n \nc\n \n=\n \nnew\n \nAmazonSimpleEmailServiceClient\n(\n\n    \nnew\n \nAWSCredentialsProviderChain\n(\n\n        \n// attempt on-instance credentials first\n\n        \nnew\n \nInstanceProfileCredentialsProvider\n(),\n\n        \n// fallback to aws credentials file\n\n        \nnew\n \nProfileCredentialsProvider\n()\n\n    \n)\n\n\n)\n\n\n\nc\n.\nsetEndpoint\n(\nemail.us-east-1.amazonaws.com\n)\n\n\n\nprintln\n(\nc\n.\ngetSendQuota\n())\n\n\n\nprintln\n(\nc\n.\ngetSendStatistics\n().\ngetSendDataPoints\n.\nget\n(\n0\n))\n\n\n\nprintln\n(\nc\n.\ngetSendStatistics\n().\ngetSendDataPoints\n.\nsize\n)\n\n\n\n\n\n\nExecute the script as follows:\n\n\n$ ./demo.scala\n.\n.\n.\n\n{\nTimestamp: Sat Jul \n05\n \n04\n:01:00 PDT \n2014\n,DeliveryAttempts: \n3\n,Bounces: \n0\n,Complaints: \n0\n,Rejects: \n0\n}\n\n\n1301", 
            "title": "Using the Scala REPL to Configure Amazon SES Notifications"
        }, 
        {
            "location": "/scala/using-the-scala-repl-to-configure-amazon-ses-notifications/#using-the-scala-repl-to-configure-amazon-ses-notifications", 
            "text": "2014-06-30 \n   Discuss    Let's say you want to configure bounce notifications for SES emails using the Scala REPL.  How do\nyou go about accessing the AWS API to make it happen?  Make sure to have the AWS SDK for Java API Reference  available\nfor additional details.  Install Scala and SBT:  brew install scala sbt  Create an AWS credentials file at  ~/.aws/credentials  that is formatted like so:  [default]  aws_access_key_id = ...  aws_secret_access_key = ...  aws_session_token = ...   Create a  build.sbt  file that looks like:  name   :=   aws-sdk-client  version   :=   1.0  scalaVersion   :=   2.11.0  libraryDependencies   ++=   Seq ( \n     commons-logging   %   commons-logging   %   1.1.3 , \n     com.amazonaws   %   aws-java-sdk   %   1.8.2  )   You can then engage the Scala REPL to talk to the AWS API like so (Scala will download the AWS SDK\nfrom Maven Central automatically):  sbt\nconsole  import   com.amazonaws.auth._  import   com.amazonaws.auth.profile._  import   com.amazonaws.services.simpleemail._  import   com.amazonaws.services.simpleemail.model._  import   com.amazonaws.services.sns._  import   com.amazonaws.services.sns.model._  val   c   =   new   AmazonSNSClient ( \n     new   AWSCredentialsProviderChain ( \n         // attempt on-instance credentials first \n         new   InstanceProfileCredentialsProvider (), \n         // fallback to aws credentials file \n         new   ProfileCredentialsProvider () \n     )  )  c . setEndpoint ( sns.us-east-1.amazonaws.com )  c . createTopic ( ses-email-bounce )  res0 :   com.amazonaws.services.sns.model.CreateTopicResult   =  { TopicArn :   arn:aws:sns:us-east- 1 : 000000000000 :ses-email-bounce }  val   c   =   new   AmazonSimpleEmailServiceClient ( \n     new   AWSCredentialsProviderChain ( \n         // attempt on-instance credentials first \n         new   InstanceProfileCredentialsProvider (), \n         // fallback to aws credentials file \n         new   ProfileCredentialsProvider () \n     )  )  c . setEndpoint ( email.us-east-1.amazonaws.com )  c . getSendQuota ()  res1 :   com.amazonaws.services.simpleemail.model.GetSendQuotaResult   =  { Max24HourSend :   100 . 0 , MaxSendRate :   10 . 0 , SentLast24Hours :   1 . 0 }  c . getSendStatistics ()  res2 :   com.amazonaws.services.simpleemail.model.GetSendStatisticsResult   =  { SendDataPoints :   [ { Timestamp:   ... , DeliveryAttempts:   0 , Bounces :   0 , Complaints :   0 , Rejects :   0 },   ...  c . getIdentityVerificationAttributes ( \n     new   GetIdentityVerificationAttributesRequest () \n         . withIdentities ( some.email@example.com )  )  res3 :   com.amazonaws.services.simpleemail.model.GetIdentityVerificationAttributesResult   =  { VerificationAttributes :   { some.email@mydomain.com ={ VerificationStatus:   Success ,}}}  c . setIdentityNotificationTopic ( \n     new   SetIdentityNotificationTopicRequest () \n         . withIdentity ( some.email@example.com ) \n         . withNotificationType ( Bounce ) \n         . withSnsTopic ( s arn:aws:sns:us-east-1: $accountId :ses-email-bounce )  )  c . getIdentityNotificationAttributes ( \n     new   GetIdentityNotificationAttributesRequest () \n         . withIdentities ( some.email@example.com )  )   One of the nice things about using the Scala REPL is that you get some useful tab completions (SES\nobject here):  scala  c.\naddRequestHandler                   sendEmail\nasInstanceOf                        sendRawEmail\ndeleteIdentity                      setConfiguration\ndeleteVerifiedEmailAddress          setEndpoint\ngetCachedResponseMetadata           setIdentityDkimEnabled\ngetIdentityDkimAttributes           setIdentityFeedbackForwardingEnabled\ngetIdentityNotificationAttributes   setIdentityNotificationTopic\ngetIdentityVerificationAttributes   setRegion\ngetRequestMetricsCollector          setServiceNameIntern\ngetSendQuota                        setSignerRegionOverride\ngetSendStatistics                   setTimeOffset\ngetServiceName                      shutdown\ngetSignerByURI                      toString\ngetSignerRegionOverride             verifyDomainDkim\ngetTimeOffset                       verifyDomainIdentity\nisInstanceOf                        verifyEmailAddress\nlistIdentities                      verifyEmailIdentity\nlistVerifiedEmailAddresses          withTimeOffset\nremoveRequestHandler  Once you have a feel for how the Scala REPL works with a library, you can create alternate entry\npoint for sbt called  scalas  which allows you to write full Scala scripts with library\ndependencies.  See  Scripts, REPL, and Dependencies \nfor additional details.  In brief, create a  scalas  script and make it available on your  PATH :  #!/bin/sh  test  -f ~/.sbtconfig   . ~/.sbtconfig exec  java  \\ \n    -Xms512M  \\ \n    -Xmx1536M  \\ \n    -Xss1M  \\ \n    -XX:+CMSClassUnloadingEnabled  \\ \n    -XX:MaxPermSize = 256M  \\ \n     ${ SBT_OPTS }   \\ \n    -jar /usr/local/Cellar/sbt/0.13.2/libexec/sbt-launch.jar  \\ \n    -Dsbt.main.class = sbt.ScriptMain  \\ \n     $@   You can then write Scala scripts like so:  #!/ usr / bin / env   scalas  /***      scalaVersion :=  2.11.0      libraryDependencies ++= Seq(           commons-logging  %  commons-logging  %  1.1.3 ,           com.amazonaws  %  aws-java-sdk  %  1.8.2      )  */  import   com.amazonaws.auth._  import   com.amazonaws.auth.profile._  import   com.amazonaws.services.simpleemail._  import   com.amazonaws.services.simpleemail.model._  val   c   =   new   AmazonSimpleEmailServiceClient ( \n     new   AWSCredentialsProviderChain ( \n         // attempt on-instance credentials first \n         new   InstanceProfileCredentialsProvider (), \n         // fallback to aws credentials file \n         new   ProfileCredentialsProvider () \n     )  )  c . setEndpoint ( email.us-east-1.amazonaws.com )  println ( c . getSendQuota ())  println ( c . getSendStatistics (). getSendDataPoints . get ( 0 ))  println ( c . getSendStatistics (). getSendDataPoints . size )   Execute the script as follows:  $ ./demo.scala\n.\n.\n. { Timestamp: Sat Jul  05   04 :01:00 PDT  2014 ,DeliveryAttempts:  3 ,Bounces:  0 ,Complaints:  0 ,Rejects:  0 }  1301", 
            "title": "Using the Scala REPL to Configure Amazon SES Notifications"
        }, 
        {
            "location": "/shell/measuring-transfer-speed-over-time-with-curl/", 
            "text": "Measuring Transfer Speed Over Time with cURL\n\n\n\n  \n2014-07-02\n\n  \nDiscuss\n\n\n\n\n\n\n\nOrdinarily when you run the \ncURL\n command to download a file, you see a\nprogress meter that updates every second.\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   346  100   346    0     0    422      0 --:--:-- --:--:-- --:--:--   422\n  4  635M    4 29.8M    0     0  1793k      0  0:06:02  0:00:17  0:05:45 2394k\n\n\n\n\n\nThis progress meter is written to stderr and if you were to redirect both stderr and stdout to a\nfile and then run \ntail -f\n on that file, you would see the exact same progress meter being updated\nonce per second, with no running log of download speed.  The reason that this output updates in\nplace is because the program is writing a carriage return \n\\r\n at the end of the progress line\ninstead of a newline \n\\n\n.  This causes the cursor to return to the beginning of the line without\nadvancing.\n\n\nWith the knowledge of how this operates, it is possible to alter the output of the cURL command to\nsave the per-second speed of a download.  If you further send the results of a large file download\nto \n/dev/null\n, then you have a reasonable approximation of of a speedtest tool and you can graph\nthe download speed over time.  The command below uses \ntr\n to rewrite carriage returns as newlines\nin an unbuffered manner, so that data is instantly available in the output file.\n\n\nAs an aside on the power of the \ntr command\n, the\n\nMore Shell, Less Egg\n blog post by\n\nDr. Drang\n discusses a programming\nchallenge proposed to \nDonald Knuth\n, who solved it with\n~10 pages of literate Pascal, and \nDoug McIlroy\n who\ncritiqued the solution and provided an alternative solution in six shell commands.\n\n\nURL\n=\nhttp://cdimage.debian.org/debian-cd/7.5.0/amd64/iso-cd/debian-7.5.0-amd64-CD-1.iso\n\n\ncurl -L -o /dev/null \n$URL\n \n2\n1\n \n\\\n\n  \n|\ntr -u \n\\r\n \n\\n\n \n curl.out\n\n\n\n\n\nThis results in an output file that looks like this:\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   346  100   346    0     0    295      0  0:00:01  0:00:01 --:--:--   295\n\n  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n  0  635M    0 70871    0     0  12988      0 14:14:26  0:00:05 14:14:21 17260\n  0  635M    0  608k    0     0  97534      0  1:53:46  0:00:06  1:53:40  120k\n  0  635M    0 1489k    0     0   201k      0  0:53:41  0:00:07  0:53:34  296k\n  0  635M    0 2742k    0     0   328k      0  0:33:00  0:00:08  0:32:52  548k\n  0  635M    0 4297k    0     0   456k      0  0:23:43  0:00:09  0:23:34  849k\n  0  635M    0 6015k    0     0   580k      0  0:18:40  0:00:10  0:18:30 1210k\n  1  635M    1 8014k    0     0   701k      0  0:15:27  0:00:11  0:15:16 1471k\n  1  635M    1 10.0M    0     0   827k      0  0:13:05  0:00:12  0:12:53 1749k\n  1  635M    1 11.0M    0     0   841k      0  0:12:52  0:00:13  0:12:39 1682k\n  .\n  .\n  .\n\n\n\n\n\nWrite a Python script \nplot_curl_data.py\n to process the data to convert it into a format useful for\n\ngnuplot\n and render a plot:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n#!/usr/bin/env python\n\n\n\nimport\n \nos\n,\n \nsys\n\n\n\n\ndef\n \nreadCurlData\n(\nfname\n):\n\n    \nlines\n \n=\n \n[]\n\n    \nwith\n \nopen\n(\nfname\n)\n \nas\n \nf\n:\n\n        \nfor\n \nline\n \nin\n \nf\n:\n\n            \nlines\n.\nappend\n(\nline\n.\nsplit\n())\n\n    \nreturn\n \nlines\n[\n3\n:]\n\n\n\n\ndef\n \nconvertUnits\n(\nlines\n):\n\n    \nconverted\n \n=\n \n[]\n\n    \nfor\n \nline\n \nin\n \nlines\n:\n\n        \nif\n \nlen\n(\nline\n)\n \n==\n \n12\n \nand\n \nnot\n \n--\n \nin\n \nline\n[\n9\n]:\n\n            \n# curl reports speed in bytes per second\n\n            \nif\n \nk\n \nin\n \nline\n[\n11\n]:\n\n                \nline\n[\n11\n]\n \n=\n \nstr\n(\nint\n(\nline\n[\n11\n]\n.\nreplace\n(\nk\n,\n))\n \n*\n \n8\n \n*\n \n1024\n)\n\n            \nelif\n \nM\n \nin\n \nline\n[\n11\n]:\n\n                \nline\n[\n11\n]\n \n=\n \nstr\n(\nint\n(\nline\n[\n11\n]\n.\nreplace\n(\nM\n,\n))\n \n*\n \n8\n \n*\n \n1048576\n)\n\n            \nelif\n \nG\n \nin\n \nline\n[\n11\n]:\n\n                \nline\n[\n11\n]\n \n=\n \nstr\n(\nint\n(\nline\n[\n11\n]\n.\nreplace\n(\nG\n,\n))\n \n*\n \n8\n \n*\n \n1073741824\n)\n\n            \nconverted\n.\nappend\n([\nline\n[\n9\n],\n \nline\n[\n11\n]])\n\n    \nreturn\n \nconverted\n\n\n\n\ndef\n \nwriteGnuplotData\n(\nfname\n,\n \nlines\n):\n\n    \nfname\n \n=\n \nfname\n \n+\n \n.gnuplot.data\n\n    \nwith\n \nopen\n(\nfname\n,\n \nw\n)\n \nas\n \nf\n:\n\n        \nfor\n \nline\n \nin\n \nlines\n:\n\n            \nf\n.\nwrite\n(\n,\n.\njoin\n(\nline\n)\n \n+\n \n\\n\n)\n\n\n\n\ndef\n \nplot\n(\nfname\n):\n\n    \ngp_fname\n \n=\n \nfname\n \n+\n \n.gp\n\n    \ngpdata_fname\n \n=\n \nfname\n \n+\n \n.gnuplot.data\n\n    \npng_fname\n \n=\n \nfname\n \n+\n \n.png\n\n\n    \nf\n \n=\n \nopen\n(\ngp_fname\n,\n \nw\n)\n\n    \nf\n.\nwrite\n(\nset output \n%s\n\\n\n \n%\n \npng_fname\n)\n\n    \nf\n.\nwrite\n(\nset datafile separator \n,\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset terminal png size 1400,800\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset title \nDownload Speed\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset ylabel \nSpeed (Mbits/s)\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset xlabel \nTime (seconds)\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset xdata time\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset timefmt \n%H:%M:%S\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset key outside\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset grid\n\\n\n)\n\n    \nf\n.\nwrite\n(\nplot \n\\\\\\n\n)\n\n    \nf\n.\nwrite\n(\n%s\n using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \nspeed\n\\n\n \n%\n \ngpdata_fname\n)\n\n    \nf\n.\nclose\n()\n\n\n    \nos\n.\nsystem\n(\ngnuplot \n%s\n \n%\n \ngp_fname\n)\n\n\n\n\nif\n \nlen\n(\nsys\n.\nargv\n)\n \n \n2\n:\n\n    \nprint\n \nUsage: \n%s\n [curl_data_filename]\n \n%\n \nsys\n.\nargv\n[\n0\n]\n\n    \nexit\n(\n1\n)\n\n\nelse\n:\n\n    \nlines\n \n=\n \nreadCurlData\n(\nsys\n.\nargv\n[\n1\n])\n\n    \nlines\n \n=\n \nconvertUnits\n(\nlines\n)\n\n    \nwriteGnuplotData\n(\nsys\n.\nargv\n[\n1\n],\n \nlines\n)\n\n    \nplot\n(\nsys\n.\nargv\n[\n1\n])\n\n\n\n\n\n\n\nRun this script like so:\n\n\n./plot_curl_data.py curl.out\n\n\n\n\n\nYou will end up with data (\ncurl.out.gp.data\n) and configuration files (\ncurl.out.gp\n) like so:\n\n\n0:00:01,295\n0:00:01,0\n0:00:02,0\n0:00:03,0\n0:00:04,0\n0:00:05,17260\n0:00:06,983040\n0:00:07,2424832\n0:00:08,4489216\n0:00:09,6955008\n0:00:10,9912320\n0:00:11,12050432\n0:00:12,14327808\n0:00:13,13778944\n0:00:14,12173312\n.\n.\n.\n\n\n\n\n\nset output \ncurl.out.png\n\nset datafile separator \n,\n\nset terminal png size 1400,800\nset title \nDownload Speed\n\nset ylabel \nSpeed (Mbits/s)\n\nset xlabel \nTime (seconds)\n\nset xdata time\nset timefmt \n%H:%M:%S\n\nset key outside\nset grid\nplot \\\n\ncurl.out.gp.data\n using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \nspeed\n\n\n\n\n\n\nThe graph will be rendered in PNG format:", 
            "title": "Measuring Transfer Speed Over Time with cURL"
        }, 
        {
            "location": "/shell/measuring-transfer-speed-over-time-with-curl/#measuring-transfer-speed-over-time-with-curl", 
            "text": "2014-07-02 \n   Discuss    Ordinarily when you run the  cURL  command to download a file, you see a\nprogress meter that updates every second.    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   346  100   346    0     0    422      0 --:--:-- --:--:-- --:--:--   422\n  4  635M    4 29.8M    0     0  1793k      0  0:06:02  0:00:17  0:05:45 2394k  This progress meter is written to stderr and if you were to redirect both stderr and stdout to a\nfile and then run  tail -f  on that file, you would see the exact same progress meter being updated\nonce per second, with no running log of download speed.  The reason that this output updates in\nplace is because the program is writing a carriage return  \\r  at the end of the progress line\ninstead of a newline  \\n .  This causes the cursor to return to the beginning of the line without\nadvancing.  With the knowledge of how this operates, it is possible to alter the output of the cURL command to\nsave the per-second speed of a download.  If you further send the results of a large file download\nto  /dev/null , then you have a reasonable approximation of of a speedtest tool and you can graph\nthe download speed over time.  The command below uses  tr  to rewrite carriage returns as newlines\nin an unbuffered manner, so that data is instantly available in the output file.  As an aside on the power of the  tr command , the More Shell, Less Egg  blog post by Dr. Drang  discusses a programming\nchallenge proposed to  Donald Knuth , who solved it with\n~10 pages of literate Pascal, and  Doug McIlroy  who\ncritiqued the solution and provided an alternative solution in six shell commands.  URL = http://cdimage.debian.org/debian-cd/7.5.0/amd64/iso-cd/debian-7.5.0-amd64-CD-1.iso \n\ncurl -L -o /dev/null  $URL   2 1   \\ \n   | tr -u  \\r   \\n    curl.out  This results in an output file that looks like this:    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   346  100   346    0     0    295      0  0:00:01  0:00:01 --:--:--   295\n\n  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n  0  635M    0 70871    0     0  12988      0 14:14:26  0:00:05 14:14:21 17260\n  0  635M    0  608k    0     0  97534      0  1:53:46  0:00:06  1:53:40  120k\n  0  635M    0 1489k    0     0   201k      0  0:53:41  0:00:07  0:53:34  296k\n  0  635M    0 2742k    0     0   328k      0  0:33:00  0:00:08  0:32:52  548k\n  0  635M    0 4297k    0     0   456k      0  0:23:43  0:00:09  0:23:34  849k\n  0  635M    0 6015k    0     0   580k      0  0:18:40  0:00:10  0:18:30 1210k\n  1  635M    1 8014k    0     0   701k      0  0:15:27  0:00:11  0:15:16 1471k\n  1  635M    1 10.0M    0     0   827k      0  0:13:05  0:00:12  0:12:53 1749k\n  1  635M    1 11.0M    0     0   841k      0  0:12:52  0:00:13  0:12:39 1682k\n  .\n  .\n  .  Write a Python script  plot_curl_data.py  to process the data to convert it into a format useful for gnuplot  and render a plot:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66 #!/usr/bin/env python  import   os ,   sys  def   readCurlData ( fname ): \n     lines   =   [] \n     with   open ( fname )   as   f : \n         for   line   in   f : \n             lines . append ( line . split ()) \n     return   lines [ 3 :]  def   convertUnits ( lines ): \n     converted   =   [] \n     for   line   in   lines : \n         if   len ( line )   ==   12   and   not   --   in   line [ 9 ]: \n             # curl reports speed in bytes per second \n             if   k   in   line [ 11 ]: \n                 line [ 11 ]   =   str ( int ( line [ 11 ] . replace ( k , ))   *   8   *   1024 ) \n             elif   M   in   line [ 11 ]: \n                 line [ 11 ]   =   str ( int ( line [ 11 ] . replace ( M , ))   *   8   *   1048576 ) \n             elif   G   in   line [ 11 ]: \n                 line [ 11 ]   =   str ( int ( line [ 11 ] . replace ( G , ))   *   8   *   1073741824 ) \n             converted . append ([ line [ 9 ],   line [ 11 ]]) \n     return   converted  def   writeGnuplotData ( fname ,   lines ): \n     fname   =   fname   +   .gnuplot.data \n     with   open ( fname ,   w )   as   f : \n         for   line   in   lines : \n             f . write ( , . join ( line )   +   \\n )  def   plot ( fname ): \n     gp_fname   =   fname   +   .gp \n     gpdata_fname   =   fname   +   .gnuplot.data \n     png_fname   =   fname   +   .png \n\n     f   =   open ( gp_fname ,   w ) \n     f . write ( set output  %s \\n   %   png_fname ) \n     f . write ( set datafile separator  , \\n ) \n     f . write ( set terminal png size 1400,800 \\n ) \n     f . write ( set title  Download Speed \\n ) \n     f . write ( set ylabel  Speed (Mbits/s) \\n ) \n     f . write ( set xlabel  Time (seconds) \\n ) \n     f . write ( set xdata time \\n ) \n     f . write ( set timefmt  %H:%M:%S \\n ) \n     f . write ( set key outside \\n ) \n     f . write ( set grid \\n ) \n     f . write ( plot  \\\\\\n ) \n     f . write ( %s  using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title  speed \\n   %   gpdata_fname ) \n     f . close () \n\n     os . system ( gnuplot  %s   %   gp_fname )  if   len ( sys . argv )     2 : \n     print   Usage:  %s  [curl_data_filename]   %   sys . argv [ 0 ] \n     exit ( 1 )  else : \n     lines   =   readCurlData ( sys . argv [ 1 ]) \n     lines   =   convertUnits ( lines ) \n     writeGnuplotData ( sys . argv [ 1 ],   lines ) \n     plot ( sys . argv [ 1 ])    Run this script like so:  ./plot_curl_data.py curl.out  You will end up with data ( curl.out.gp.data ) and configuration files ( curl.out.gp ) like so:  0:00:01,295\n0:00:01,0\n0:00:02,0\n0:00:03,0\n0:00:04,0\n0:00:05,17260\n0:00:06,983040\n0:00:07,2424832\n0:00:08,4489216\n0:00:09,6955008\n0:00:10,9912320\n0:00:11,12050432\n0:00:12,14327808\n0:00:13,13778944\n0:00:14,12173312\n.\n.\n.  set output  curl.out.png \nset datafile separator  , \nset terminal png size 1400,800\nset title  Download Speed \nset ylabel  Speed (Mbits/s) \nset xlabel  Time (seconds) \nset xdata time\nset timefmt  %H:%M:%S \nset key outside\nset grid\nplot \\ curl.out.gp.data  using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title  speed   The graph will be rendered in PNG format:", 
            "title": "Measuring Transfer Speed Over Time with cURL"
        }, 
        {
            "location": "/shell/parsing-json-on-the-command-line/", 
            "text": "Parsing JSON on the Command Line\n\n\n\n  \n2014-07-16\n\n  \nDiscuss\n\n\n\n\n\n\n\nWith more APIs moving to JSON, being able to parse it at the command line allows you to write more\nsophisticated shell scripts that can interact with your favorite services.  Most first attempts at\nJSON parsing using some variation of the following to get the job done, which initially seems\nreasonable:\n\n\ncurl -s http://endpoint.info \n\\\n\n    \n|\npython -mjson.tool \n\\\n\n    \n|\ngrep foo \n\\\n\n    \n|\ncut -d: -f2 \n\\\n\n    \n|\nsed -e \ns/\n//g\n\n\n\n\n\n\nHowever, this can rapidly get out of hand, if you have key duplication, complex nested structures or\nyou need to pull in all of the elements of a list.  For awhile,\n\nunderscore-cli\n was my favorite fully-featured JSON\nparser, but I found its documentation somewhat lacking and it hasn't seen serious development since\nNovember 2012.  Since then, I found \njq\n which has a beautiful,\nwell-written \nmanual\n with many usage examples and it is under\nactive development.  It also has the benefit of being written in C, which helps speed and it has a\nfairly concise descriptor language.  To install:\n\n\nbrew install jq\n\n\n\n\n\nThen you can do things like flattening a complex JSON structure into a simple CSV:\n\n\nPAYLOADS\n=\n$(\n curl -s \n$URL\n \n|\njq \n.payloads\n \n)\n\n\nif\n \n[\n \n$PAYLOADS\n !\n=\n \n[]\n \n]\n;\n \nthen\n\n    \necho\n \n$PAYLOADS\n \n\\\n\n        \n|\n jq -r \n.[] | .minutes[].payload.items[] | [.actions.actionTime, (.actions | {actions} | .actions.email.state), .sourceInstance, (.actions | {actions} | .actions.email.info )] | @csv\n\n\nfi\n\n\n\n\n\n\nAs a bonus feature, if you have to deal in XML rather than JSON, then\n\nxmlstarlet\n is a good choice for handling it.  Naturally,\ninstallation:\n\n\nbrew install xmlstarlet\n\n\n\n\n\nOnce you have xmlstarlet, you can do things like pull the build number out of an Atlassian Bamboo\nHTML page, which was necessary when they did not have an API call available to report the version\nnumber of the last known good build for a project:\n\n\ncurl -s --insecure https://bamboo.local/browse/\n${\nBUILDKEY\n}${\nBUILD\n}\n \n\\\n\n    \n|\ntidy -asxhtml -numeric --force-output \ntrue\n \n2\n/dev/null \n\\\n\n    \n|\nxmlstarlet sel -N \nx\n=\nhttp://www.w3.org/1999/xhtml\n -t -m \n//x:div[@id=\nsr-build\n]/x:h2/x:a\n -v \n.\n \n\\\n\n    \n|\nsed -e \ns/#//g", 
            "title": "Parsing JSON on the Command Line"
        }, 
        {
            "location": "/shell/parsing-json-on-the-command-line/#parsing-json-on-the-command-line", 
            "text": "2014-07-16 \n   Discuss    With more APIs moving to JSON, being able to parse it at the command line allows you to write more\nsophisticated shell scripts that can interact with your favorite services.  Most first attempts at\nJSON parsing using some variation of the following to get the job done, which initially seems\nreasonable:  curl -s http://endpoint.info  \\ \n     | python -mjson.tool  \\ \n     | grep foo  \\ \n     | cut -d: -f2  \\ \n     | sed -e  s/ //g   However, this can rapidly get out of hand, if you have key duplication, complex nested structures or\nyou need to pull in all of the elements of a list.  For awhile, underscore-cli  was my favorite fully-featured JSON\nparser, but I found its documentation somewhat lacking and it hasn't seen serious development since\nNovember 2012.  Since then, I found  jq  which has a beautiful,\nwell-written  manual  with many usage examples and it is under\nactive development.  It also has the benefit of being written in C, which helps speed and it has a\nfairly concise descriptor language.  To install:  brew install jq  Then you can do things like flattening a complex JSON structure into a simple CSV:  PAYLOADS = $(  curl -s  $URL   | jq  .payloads   )  if   [   $PAYLOADS  ! =   []   ] ;   then \n     echo   $PAYLOADS   \\ \n         |  jq -r  .[] | .minutes[].payload.items[] | [.actions.actionTime, (.actions | {actions} | .actions.email.state), .sourceInstance, (.actions | {actions} | .actions.email.info )] | @csv  fi   As a bonus feature, if you have to deal in XML rather than JSON, then xmlstarlet  is a good choice for handling it.  Naturally,\ninstallation:  brew install xmlstarlet  Once you have xmlstarlet, you can do things like pull the build number out of an Atlassian Bamboo\nHTML page, which was necessary when they did not have an API call available to report the version\nnumber of the last known good build for a project:  curl -s --insecure https://bamboo.local/browse/ ${ BUILDKEY }${ BUILD }   \\ \n     | tidy -asxhtml -numeric --force-output  true   2 /dev/null  \\ \n     | xmlstarlet sel -N  x = http://www.w3.org/1999/xhtml  -t -m  //x:div[@id= sr-build ]/x:h2/x:a  -v  .   \\ \n     | sed -e  s/#//g", 
            "title": "Parsing JSON on the Command Line"
        }, 
        {
            "location": "/shell/pet-cli-snippet-manager/", 
            "text": "pet - CLI Snippet Manager\n\n\n\n  \n2017-10-13\n\n  \nDiscuss\n\n\n\n\n\n\n\nhttps://github.com/knqyf263/pet\n\n\nThis shell utility that makes it easy to save, search and recall shell commands.  It uses\n\nGitHub Gists\n as the backend storage, so your work is backed\nup and accessible from multiple locations.  Of course, since you are storing data publicly, make\nsure that you are sanitizing the data, so no secrets are shared.\n\n\nInstall and Configure\n\n\nInstall with Homebrew:\n\n\nbrew install pet\n\n\n\n\n\nCreate a new access token\n on GitHub that allows only the\n\ngist\n scope.  Create a pet configuration file and set the \naccess_token\n.\n\n\npet configure\n\n\n\n\n\nUsage\n\n\nCreate new snippets:\n\n\npet new\n\n\n\n\n\nList existing snippets:\n\n\npet list\n\n\n\n\n\nSearch for snippets:\n\n\npet search\n\n\n\n\n\n\n\nWarning\n\n\nThe \npet\n command does not have version validation associated with snippets uploaded to Gist,\nso it is possible to wipe out your snippets in two separate ways:\n\n\n\n\n\n\nDownloading empty snippets from your Gist, which overwrite your local snippets.\n\n\n\n\n\n\nUploading empty local snippets to your Gist.  Since GitHub stores revisions, you can recover\nfrom this.\n\n\n\n\n\n\nMake sure that you have the latest snippets locally before updating your Gist.\n\n\n\n\nUpload snippets to Gist:\n\n\npet sync -u\n\n\n\n\n\nDownload snippets from Gist:\n\n\npet sync", 
            "title": "pet - CLI Snippet Manager"
        }, 
        {
            "location": "/shell/pet-cli-snippet-manager/#pet-cli-snippet-manager", 
            "text": "2017-10-13 \n   Discuss    https://github.com/knqyf263/pet  This shell utility that makes it easy to save, search and recall shell commands.  It uses GitHub Gists  as the backend storage, so your work is backed\nup and accessible from multiple locations.  Of course, since you are storing data publicly, make\nsure that you are sanitizing the data, so no secrets are shared.", 
            "title": "pet - CLI Snippet Manager"
        }, 
        {
            "location": "/shell/pet-cli-snippet-manager/#install-and-configure", 
            "text": "Install with Homebrew:  brew install pet  Create a new access token  on GitHub that allows only the gist  scope.  Create a pet configuration file and set the  access_token .  pet configure", 
            "title": "Install and Configure"
        }, 
        {
            "location": "/shell/pet-cli-snippet-manager/#usage", 
            "text": "Create new snippets:  pet new  List existing snippets:  pet list  Search for snippets:  pet search   Warning  The  pet  command does not have version validation associated with snippets uploaded to Gist,\nso it is possible to wipe out your snippets in two separate ways:    Downloading empty snippets from your Gist, which overwrite your local snippets.    Uploading empty local snippets to your Gist.  Since GitHub stores revisions, you can recover\nfrom this.    Make sure that you have the latest snippets locally before updating your Gist.   Upload snippets to Gist:  pet sync -u  Download snippets from Gist:  pet sync", 
            "title": "Usage"
        }, 
        {
            "location": "/shell/schedule-ad-hoc-jobs-for-later-on-an-instance-with-atd/", 
            "text": "Schedule Ad-Hoc Jobs for Later on an Instance with atd\n\n\n\n  \n2017-08-15\n\n  \nDiscuss\n\n\n\n\n\n\n\nhttps://www.computerhope.com/unix/uat.htm\n\n\nThe purpose of this technique is to allow you to disconnect from the instance, while leaving the job\nrunning (i.e. not dependent on a shell fork).\n\n\n(root) # at now + 1 minute\nwarning: commands will be executed using /bin/sh\nat\n /apps/something/bin/upload_to_s3.sh\nat\n ^d\njob 2 at Wed Aug  9 20:37:00 2017\n\n(root) # atq\n2   Wed Aug  9 20:37:00 2017 a root\n\n\n\n\n\nIf you need to remove and reschedule a job, you can do the following:\n\n\n(root) # atrm 2\n\n\n\n\n\nThe following are examples of casual times that can be used with at:\n\n\n\n\n\n\n\n\nexpression\n\n\ntime\n\n\n\n\n\n\n\n\n\n\nnoon\n\n\n12:00 PM October 18 2014\n\n\n\n\n\n\nmidnight\n\n\n12:00 AM October 19 2014\n\n\n\n\n\n\nteatime\n\n\n4:00 PM October 18 2014\n\n\n\n\n\n\ntomorrow\n\n\n10:00 AM October 19 2014\n\n\n\n\n\n\nnoon tomorrow\n\n\n12:00 PM October 19 2014\n\n\n\n\n\n\nnext week\n\n\n10:00 AM October 25 2014\n\n\n\n\n\n\nnext monday\n\n\n10:00 AM October 24 2014\n\n\n\n\n\n\nfri\n\n\n10:00 AM October 21 2014\n\n\n\n\n\n\nNOV\n\n\n10:00 AM November 18 2014\n\n\n\n\n\n\n9:00 AM\n\n\n9:00 AM October 19 2014\n\n\n\n\n\n\n2:30 PM\n\n\n2:30 PM October 18 2014\n\n\n\n\n\n\n1430\n\n\n2:30 PM October 18 2014\n\n\n\n\n\n\n2:30 PM tomorrow\n\n\n2:30 PM October 19 2014\n\n\n\n\n\n\n2:30 PM next month\n\n\n2:30 PM November 18 2014\n\n\n\n\n\n\n2:30 PM Fri\n\n\n2:30 PM October 21 2014\n\n\n\n\n\n\n2:30 PM 10/21\n\n\n2:30 PM October 21 2014\n\n\n\n\n\n\n2:30 PM Oct 21\n\n\n2:30 PM October 21 2014\n\n\n\n\n\n\n2:30 PM 10/21/2014\n\n\n2:30 PM October 21 2014\n\n\n\n\n\n\n2:30 PM 21.10.14\n\n\n2:30 PM October 21 2014\n\n\n\n\n\n\nnow + 30 minutes\n\n\n10:30 AM October 18 2014\n\n\n\n\n\n\nnow + 1 hour\n\n\n11:00 AM October 18 2014\n\n\n\n\n\n\nnow + 2 days\n\n\n10:00 AM October 20 2014\n\n\n\n\n\n\n4 PM + 2 days\n\n\n4:00 PM October 20 2014\n\n\n\n\n\n\nnow + 3 weeks\n\n\n10:00 AM November 8 2014\n\n\n\n\n\n\nnow + 4 months\n\n\n10:00 AM February 18 2015\n\n\n\n\n\n\nnow + 5 years\n\n\n10:00 AM October 18 2019", 
            "title": "Schedule Ad-Hoc Jobs for Later on an Instance with atd"
        }, 
        {
            "location": "/shell/schedule-ad-hoc-jobs-for-later-on-an-instance-with-atd/#schedule-ad-hoc-jobs-for-later-on-an-instance-with-atd", 
            "text": "2017-08-15 \n   Discuss    https://www.computerhope.com/unix/uat.htm  The purpose of this technique is to allow you to disconnect from the instance, while leaving the job\nrunning (i.e. not dependent on a shell fork).  (root) # at now + 1 minute\nwarning: commands will be executed using /bin/sh\nat  /apps/something/bin/upload_to_s3.sh\nat  ^d\njob 2 at Wed Aug  9 20:37:00 2017\n\n(root) # atq\n2   Wed Aug  9 20:37:00 2017 a root  If you need to remove and reschedule a job, you can do the following:  (root) # atrm 2  The following are examples of casual times that can be used with at:     expression  time      noon  12:00 PM October 18 2014    midnight  12:00 AM October 19 2014    teatime  4:00 PM October 18 2014    tomorrow  10:00 AM October 19 2014    noon tomorrow  12:00 PM October 19 2014    next week  10:00 AM October 25 2014    next monday  10:00 AM October 24 2014    fri  10:00 AM October 21 2014    NOV  10:00 AM November 18 2014    9:00 AM  9:00 AM October 19 2014    2:30 PM  2:30 PM October 18 2014    1430  2:30 PM October 18 2014    2:30 PM tomorrow  2:30 PM October 19 2014    2:30 PM next month  2:30 PM November 18 2014    2:30 PM Fri  2:30 PM October 21 2014    2:30 PM 10/21  2:30 PM October 21 2014    2:30 PM Oct 21  2:30 PM October 21 2014    2:30 PM 10/21/2014  2:30 PM October 21 2014    2:30 PM 21.10.14  2:30 PM October 21 2014    now + 30 minutes  10:30 AM October 18 2014    now + 1 hour  11:00 AM October 18 2014    now + 2 days  10:00 AM October 20 2014    4 PM + 2 days  4:00 PM October 20 2014    now + 3 weeks  10:00 AM November 8 2014    now + 4 months  10:00 AM February 18 2015    now + 5 years  10:00 AM October 18 2019", 
            "title": "Schedule Ad-Hoc Jobs for Later on an Instance with atd"
        }, 
        {
            "location": "/talks/favorite/", 
            "text": "Favorite Talks\n\n\nC\n\n\n\n\nHandmade Hero Day 001: Setting Up the Windows Build - Casey Muratori | 2014\n\n\n\n\nCareer\n\n\n\n\nRethinking the Developer Career Path \u2013 Randall Koutnik | The Lead Developer UK 2017\n\n\n\n\nClojure\n\n\n\n\nAre We There Yet? - Rich Hickey | JVM Language Summit 2009\n\n\nBoot Can Built It - Alan Dipert, Micha Niskin | Clojure/west 2015\n\n\nBottom Up vs Top Down Design in Clojure - Mark Bastian | Clojure/conj 2015\n\n\nClojure Concurrency - Rich Hickey | Western Mass. Developers Group\n\n\nClojure core.async - Rich Hickey | Strange Loop 2013\n\n\nClojure Made Simple - Rich Hickey | Oracle Developers 2015\n\n\nClojure Parallelism Beyond Futures - Leon Barrett | Clojure/west 2015\n\n\nclojure.spec - Rich Hickey | LispNYC 2017\n\n\nComposing Interactive Apps With Zelkova - James MacAulay | Clojure/west 2015\n\n\nDagobah, a Data centric Meta scheduler - Matt Bossenbroek | Clojure/conj 2015\n\n\nDebugging Clojure Code with Cursive - Colin Fleming | Clojure/west 2015\n\n\nDesign and Prototype a Language In Clojure - Jeanine Adkisson | Clojure/west 2015\n\n\nHammock Driven Development - Rich Hickey | Clojure/conj 2010\n\n\nParallel Programming, Fork Join, and Reducers - Daniel Higginbotham | Clojure/west 2016\n\n\nParsing Text with a Virtual Machine - Ghadi Shayban | Clojure/west 2016\n\n\nREPL-Based Development Demo - Valentin Waeselynck | 2017\n (from \nWhat makes a good REPL?\n)\n\n\nSimple Made Easy - Rich Hickey | Strange Loop 2011\n\n\nSpec-ulation - Rich Hickey | Clojure/conj 2016\n\n\nThe Joys and Perils of Interactive Development - Stuart Sierra | Clojure/west 2016\n\n\nThe ReactJS Landscape - Luke VanderHart | Clojure/west 2015\n\n\nTypes are Like the Weather, Type Systems are Like Weathermen - Matthias Felleisen | Clojure/west 2016\n\n\n\n\nClojureScript\n\n\n\n\nClojureScript for Skeptics - Derek Slager | Clojure/conj 2015\n\n\nDeveloping ClojureScript With Figwheel - Bruce Hauman | Clojure/west 2015\n\n\nFrom 0 to Prototype Using ClojureScript, Re-Frame and Friends - Martin Clausen | Dutch Clojure Days 2017\n\n\nInteractive Programming Flappy Bird in ClojureScript - Bruce Hauman | 2014\n\n\n\n\nNetflix Atlas\n\n\n\n\nNetflix: Amazon S3 \n Amazon Elastic MapReduce to Monitor at Gigascale - Roy Rapoport | AWS re:Invent 2013\n\n\nMonitoring Monitoring Systems at Netflix - Roy Rapoport | Monitorama PDX 2017\n\n\nNetflix Atlas Telemetry - A Platform Begets an Ecosystem\n\n\n\n\nPython\n\n\n\n\nAutomating Your Browser and Desktop Apps - Al Sweigart | PyBay2016\n\n\nBeyond PEP 8: Best Practices for Beautiful Intelligible Code - Raymond Hettinger | PyCon 2015\n\n\nBuilt in Super Heroes - David Beazley | PyData Chicago 2016\n\n\nFear and Awaiting in Async: A Savage Journey to the Heart of the Coroutine Dream - David Beazley | PyOhio 2016\n\n\nLogging and Testing and Debugging, Oh My! - Albert Sweigart | PyBay2017\n\n\nMachete-Mode Debugging: Hacking Your Way Out of a Tight Spot - Ned Batchelder | PyCon 2016\n\n\nPython as a Configuration Language - Guillermo P\u00e9rez | PyCon 2016\n\n\nPython Concurrency From the Ground Up: LIVE! - David Beazley | PyCon 2015\n\n\nPython Language - Guido van Rossum | PyCon 2016\n\n\nPython vs Ruby: A Battle to The Death - Gary Bernhardt | NWPD 2010\n\n\nRefactoring Python: Why and How to Restructure Your Code - Brett Slatkin | PyCon 2016\n\n\nRemoving Python's GIL: The Gilectomy - Larry Hastings | PyCon 2016\n\n\nStop Writing Classes - Jack Diederich | PyCon 2012\n\n\nThe Packaging Gradient - Mahmoud Hashemi | PyBay 2017\n\n\nThink Like a Pythonista - Luciano Ramalho | PyBay 2017\n\n\nThinking In Coroutines - \u0141ukasz Langa | PyCon 2016\n\n\nTo Mock, or Not to Mock, That is the Question - Ana Balica | PyCon 2016\n\n\nWriting Awesome Command-Line Programs in Python - Mark Smith | EuroPython 2014\n\n\n\n\nScala\n\n\n\n\nA Better Scala REPL - Li Haoyi | SBTB 2015\n\n\n\n\nUnix\n\n\n\n\nThe Unix Chainsaw - Gary Bernhardt | Cascadia Ruby Conf 2011", 
            "title": "Favorite"
        }, 
        {
            "location": "/talks/favorite/#favorite-talks", 
            "text": "", 
            "title": "Favorite Talks"
        }, 
        {
            "location": "/talks/favorite/#c", 
            "text": "Handmade Hero Day 001: Setting Up the Windows Build - Casey Muratori | 2014", 
            "title": "C"
        }, 
        {
            "location": "/talks/favorite/#career", 
            "text": "Rethinking the Developer Career Path \u2013 Randall Koutnik | The Lead Developer UK 2017", 
            "title": "Career"
        }, 
        {
            "location": "/talks/favorite/#clojure", 
            "text": "Are We There Yet? - Rich Hickey | JVM Language Summit 2009  Boot Can Built It - Alan Dipert, Micha Niskin | Clojure/west 2015  Bottom Up vs Top Down Design in Clojure - Mark Bastian | Clojure/conj 2015  Clojure Concurrency - Rich Hickey | Western Mass. Developers Group  Clojure core.async - Rich Hickey | Strange Loop 2013  Clojure Made Simple - Rich Hickey | Oracle Developers 2015  Clojure Parallelism Beyond Futures - Leon Barrett | Clojure/west 2015  clojure.spec - Rich Hickey | LispNYC 2017  Composing Interactive Apps With Zelkova - James MacAulay | Clojure/west 2015  Dagobah, a Data centric Meta scheduler - Matt Bossenbroek | Clojure/conj 2015  Debugging Clojure Code with Cursive - Colin Fleming | Clojure/west 2015  Design and Prototype a Language In Clojure - Jeanine Adkisson | Clojure/west 2015  Hammock Driven Development - Rich Hickey | Clojure/conj 2010  Parallel Programming, Fork Join, and Reducers - Daniel Higginbotham | Clojure/west 2016  Parsing Text with a Virtual Machine - Ghadi Shayban | Clojure/west 2016  REPL-Based Development Demo - Valentin Waeselynck | 2017  (from  What makes a good REPL? )  Simple Made Easy - Rich Hickey | Strange Loop 2011  Spec-ulation - Rich Hickey | Clojure/conj 2016  The Joys and Perils of Interactive Development - Stuart Sierra | Clojure/west 2016  The ReactJS Landscape - Luke VanderHart | Clojure/west 2015  Types are Like the Weather, Type Systems are Like Weathermen - Matthias Felleisen | Clojure/west 2016", 
            "title": "Clojure"
        }, 
        {
            "location": "/talks/favorite/#clojurescript", 
            "text": "ClojureScript for Skeptics - Derek Slager | Clojure/conj 2015  Developing ClojureScript With Figwheel - Bruce Hauman | Clojure/west 2015  From 0 to Prototype Using ClojureScript, Re-Frame and Friends - Martin Clausen | Dutch Clojure Days 2017  Interactive Programming Flappy Bird in ClojureScript - Bruce Hauman | 2014", 
            "title": "ClojureScript"
        }, 
        {
            "location": "/talks/favorite/#netflix-atlas", 
            "text": "Netflix: Amazon S3   Amazon Elastic MapReduce to Monitor at Gigascale - Roy Rapoport | AWS re:Invent 2013  Monitoring Monitoring Systems at Netflix - Roy Rapoport | Monitorama PDX 2017  Netflix Atlas Telemetry - A Platform Begets an Ecosystem", 
            "title": "Netflix Atlas"
        }, 
        {
            "location": "/talks/favorite/#python", 
            "text": "Automating Your Browser and Desktop Apps - Al Sweigart | PyBay2016  Beyond PEP 8: Best Practices for Beautiful Intelligible Code - Raymond Hettinger | PyCon 2015  Built in Super Heroes - David Beazley | PyData Chicago 2016  Fear and Awaiting in Async: A Savage Journey to the Heart of the Coroutine Dream - David Beazley | PyOhio 2016  Logging and Testing and Debugging, Oh My! - Albert Sweigart | PyBay2017  Machete-Mode Debugging: Hacking Your Way Out of a Tight Spot - Ned Batchelder | PyCon 2016  Python as a Configuration Language - Guillermo P\u00e9rez | PyCon 2016  Python Concurrency From the Ground Up: LIVE! - David Beazley | PyCon 2015  Python Language - Guido van Rossum | PyCon 2016  Python vs Ruby: A Battle to The Death - Gary Bernhardt | NWPD 2010  Refactoring Python: Why and How to Restructure Your Code - Brett Slatkin | PyCon 2016  Removing Python's GIL: The Gilectomy - Larry Hastings | PyCon 2016  Stop Writing Classes - Jack Diederich | PyCon 2012  The Packaging Gradient - Mahmoud Hashemi | PyBay 2017  Think Like a Pythonista - Luciano Ramalho | PyBay 2017  Thinking In Coroutines - \u0141ukasz Langa | PyCon 2016  To Mock, or Not to Mock, That is the Question - Ana Balica | PyCon 2016  Writing Awesome Command-Line Programs in Python - Mark Smith | EuroPython 2014", 
            "title": "Python"
        }, 
        {
            "location": "/talks/favorite/#scala", 
            "text": "A Better Scala REPL - Li Haoyi | SBTB 2015", 
            "title": "Scala"
        }, 
        {
            "location": "/talks/favorite/#unix", 
            "text": "The Unix Chainsaw - Gary Bernhardt | Cascadia Ruby Conf 2011", 
            "title": "Unix"
        }, 
        {
            "location": "/talks/netflix-atlas-telemetry-a-platform-begets-an-ecosystem/", 
            "text": "Netflix Atlas Telemetry - A Platform Begets an Ecosystem", 
            "title": "Netflix Atlas Telemetry - A Platform Begets an Ecosystem"
        }, 
        {
            "location": "/talks/netflix-atlas-telemetry-a-platform-begets-an-ecosystem/#netflix-atlas-telemetry-a-platform-begets-an-ecosystem", 
            "text": "", 
            "title": "Netflix Atlas Telemetry - A Platform Begets an Ecosystem"
        }
    ]
}