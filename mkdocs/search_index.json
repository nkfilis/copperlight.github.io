{
    "docs": [
        {
            "location": "/", 
            "text": "Copperlight Writes\n\n\n2014-06-29\n\n\nMy name is \nMatthew Johnson\n and I am currently a Sr. DevOps Engineer at \nNetflix\n; I have been working in the DevOps space since I joined \nBlackboard\n in 2004.  The purpose of this site is to share notes and neat ways of solving problems.\n\n\nSite Discussion\n\n\nFollow @copperlight", 
            "title": "Home"
        }, 
        {
            "location": "/#copperlight-writes", 
            "text": "2014-06-29  My name is  Matthew Johnson  and I am currently a Sr. DevOps Engineer at  Netflix ; I have been working in the DevOps space since I joined  Blackboard  in 2004.  The purpose of this site is to share notes and neat ways of solving problems.  Site Discussion  Follow @copperlight", 
            "title": "Copperlight Writes"
        }, 
        {
            "location": "/ansible/ansible-vault-and-ssh-key-distribution/", 
            "text": "Ansible Vault and SSH Key Distribution\n\n\n2014-06-30\n\n\nThere are two types of SSH key distribution discussed in this post: private keys on local hosts and\npublic keys on remote hosts.  SSH private key distribution is best used for setting up your own\nworkstation or possibly an \nAnsible Tower\n server.  In general, you\nshould not be distributing private keys widely; with a good SSH tunneling configuration and SSH\npublic key distribution, there should be no need for the private keys to be installed in more than\nfew places.  This configuration will show off a technique for configuring an SSH jump host bastion\nthat allows you to keep your private key on your own workstation; there is no need to have the SSH\nprivate key on the bastion host.\n\n\nFor the purpose of this post, I have generated a new SSH key pair to demonstrate this technique;\nthis keypair is used nowhere.  Part of the trick to making this work is that the private key needs\nto be base64 encoded so that line breaks are preserved when it is stored as a yaml string in the\n\nvars_files\n.  Template files are created for the public and private keys to preserve file change\ndetection.\n\n\nGenerate a new key pair:\n\n\n$ ssh-keygen -b \n2048\n -f junk_key -C junk\nGenerating public/private rsa key pair.\nEnter passphrase \n(\nempty \nfor\n no passphrase\n)\n:\nEnter same passphrase again:\nYour identification has been saved in junk_key.\nYour public key has been saved in junk_key.pub.\nThe key fingerprint is:\n\n94\n:94:ae:ac:c3:5e:ee:7d:fa:2c:cb:0f:ae:19:c8:99 junk\nThe key\ns randomart image is:\n+--\n[\n RSA \n2048\n]\n----+\n\n|\n        ..       \n|\n\n\n|\n       ...       \n|\n\n\n|\n       .o        \n|\n\n\n|\n       ..        \n|\n\n\n|\n     . .S        \n|\n\n\n|\n   . +o          \n|\n\n\n|\n   .E.o .        \n|\n\n\n|\n    +o *.o.      \n|\n\n\n|\n   ..o\n=\n.*B+      \n|\n\n+-----------------+\n\n/pre\n\n\n\n\n\n\nBase64 encode the private key:\n\n\nbase64 -i junk_key \n junk_key.b64\n\n\n\n\n\nCreate an Ansible vars_files yaml data file named \nssh_keys/ssh_key_vault.yml\n.  The\n\nssh_private_key\n variable should contain the base64 encoded private key and the \nssh_public_key\n\nvariable should contain the public key. Encrypt the file with \nansible-vault\n:\n\n\nssh_private_key\n:\n \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\n\n\n\nssh_public_key\n:\n \nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk\n\n\n\n\n\n\nansible-vault encrypt ssh_keys/ssh_key_vault.yml\nVault password:\nConfirm Vault password:\nEncryption successful\n\n\n\n\n\nCreate an inventory file named \ninventory\n, showing off the SSH jump host connection capability:\n\n\n[localhost]\n\n\nlocalhost ansible_connection\n=\nlocal\n\n\n\n[group-all:children]\n\n\ngroup-01\n\n\ngroup-02\n\n\n\n[group-01]\n\n\ni-00000001 ansible_ssh_host\n=\nbastion+192.168.1.1 ansible_ssh_user=remoteuser\n\n\ni-00000002 ansible_ssh_host\n=\nbastion+192.168.1.2 ansible_ssh_user=remoteuser\n\n\n\n[group-02]\n\n\ni-00000003 ansible_ssh_host\n=\nbastion+192.168.1.3 ansible_ssh_user=remoteuser\n\n\ni-00000004 ansible_ssh_host\n=\nbastion+192.168.1.4 ansible_ssh_user=remoteuser\n\n\n\n\n\n\nCreate a template file for the private key named \ntemplates/HOME_.ssh_junk\n:\n\n\n{{\nssh_private_key_decoded.stdout\n}}\n\n\n\n\n\n\nCreate a template file for the public key named \ntemplates/HOME_.ssh_junk.pub\n:\n\n\n{{\nssh_public_key\n}}\n\n\n\n\n\n\nCreate a template file for the SSH jump host configuration named \ntemplates/HOME_.ssh_config\n:\n\n\nHost *\n\n\n    ServerAliveInterval 30\n\n\n    ServerAliveCountMax 5\n\n\n\nHost bastion\n\n\n    User \n{{\nremote_user\n}}\n\n\n    IdentityFile ~/.ssh/junk\n\n\n    Hostname bastion\n\n\n\nHost bastion+*\n\n\n    User \n{{\nremote_user\n}}\n\n\n    IdentityFile ~/.ssh/junk\n\n\n    ProxyCommand ssh -T -a bastion nc $(echo %h |cut -d+ -f2) %p 2\n/dev/null\n\n\n    StrictHostKeyChecking no\n\n\n\n\n\n\nThis configuration assumes that you have a consistent remote username defined on the bastion server\nand your protected hosts.\n\n\nWrite a playbook to install the SSH key and configuration on your local workstation named\n\nconfig_local-ssh.yml\n:\n\n\n-\n \nname\n:\n \nconfigure local ssh\n\n  \nhosts\n:\n\n  \n-\n \nlocalhost\n\n  \ngather_facts\n:\n \nfalse\n\n  \nsudo\n:\n \nfalse\n\n  \nvars\n:\n\n    \nlocal_home\n:\n \n{{\n \nlookup(\nenv\n,\nHOME\n)\n \n}}\n\n    \nlocal_user\n:\n \n{{\n \nlookup(\nenv\n,\nUSER\n)\n \n}}\n\n    \nremote_user\n:\n \nremoteuser\n\n  \nvars_files\n:\n\n  \n-\n \nssh_keys/ssh_key_vault.yml\n\n  \ntasks\n:\n\n  \n-\n \nfile\n:\n \npath={{local_home}}/.ssh state=directory mode=0700 owner={{local_user}}\n\n\n  \n-\n \ntemplate\n:\n \nsrc=templates/HOME_.ssh_config dest={{local_home}}/.ssh/config mode=0644 owner={{local_user}} backup=yes\n\n\n  \n-\n \nshell\n:\n \necho {{ssh_private_key}} |base64 --decode\n\n    \nregister\n:\n \nssh_private_key_decoded\n\n\n  \n-\n \ntemplate\n:\n \nsrc=templates/HOME_.ssh_junk dest={{local_home}}/.ssh/junk mode=0600 owner={{local_user}}\n\n\n  \n-\n \ntemplate\n:\n \nsrc=templates/HOME_.ssh_junk.pub dest={{local_home}}/.ssh/junk.pub mode=0644 owner={{local_user}}\n\n\n\n\n\n\nRun the playbook to setup your local workstation with SSH keys and configuration:\n\n\nansible-playbook -i inventory config_local-ssh.yml --ask-vault-pass\nVault password:\n\n\n\n\n\nTest your SSH tunneling access to a remote host behind the bastion server:\n\n\nssh bastion+192.168.1.1\nssh bastion+192.168.1.1 \ndate; date \n /tmp/date.out\n\nscp bastion+192.168.1.1:/tmp/date.out .\n\n\n\n\n\nNotice that the hosts behind the bastion server are referenced in the SSH command the same way that\nthey are referenced in the Ansible inventory file.  The \"+\" character used as a separator was\nselected explicitly for its ability to be used interchangeably at the command line and in the\nAnsible inventory.  IP addresses are being used on the right hand side of the expression since the\nsecondary connection to the protected host relies on the name resolution capabilities of the first\nhost in the tunnel.  If you had a reliable dynamic DNS service that was keeping up with changes to\nthe protected hosts and was accessible to the bastion host, then you could use host names instead,\nsuch as \nbastion+webserver01\n.  This host selection technique can be extended to an Ansible dynamic\ninventory script, if you were running instances at a cloud provider such as AWS.  When you write a\n\ndynamic inventory script\n, the data format\nshould look like this:\n\n\n{\n\n    \ngroup-01\n:\n \n{\n\n        \nhosts\n:\n \n[\n\n            \ni-00000001\n,\n\n            \ni-00000002\n\n        \n]\n\n    \n},\n\n    \ngroup-02\n:\n \n{\n\n        \nhosts\n:\n \n[\n\n            \ni-00000003\n,\n\n            \ni-00000004\n\n        \n]\n\n    \n},\n\n    \ngroup-all\n:\n \n{\n\n        \nchildren\n:\n \n[\n\n            \ngroup-01\n,\n\n            \ngroup-02\n\n        \n]\n\n    \n},\n\n    \nlocalhost\n:\n \n[\n\n        \nlocalhost\n\n    \n],\n\n    \n_meta\n:\n \n{\n\n        \nhostvars\n:\n \n{\n\n            \ni-00000001\n:\n \n{\n\n                \nansible_ssh_host\n:\n \nbastion+192.168.1.1\n,\n\n                \nansible_ssh_user\n:\n \nremoteuser\n\n            \n},\n\n            \ni-00000002\n:\n \n{\n\n                \nansible_ssh_host\n:\n \nbastion+192.168.1.2\n,\n\n                \nansible_ssh_user\n:\n \nremoteuser\n\n            \n},\n\n            \ni-00000003\n:\n \n{\n\n                \nansible_ssh_host\n:\n \nbastion+192.168.1.3\n,\n\n                \nansible_ssh_user\n:\n \nremoteuser\n\n            \n},\n\n            \ni-00000004\n:\n \n{\n\n                \nansible_ssh_host\n:\n \nbastion+192.168.1.4\n,\n\n                \nansible_ssh_user\n:\n \nremoteuser\n\n            \n},\n\n            \nlocalhost\n:\n \n{\n\n                \nansible_connection\n:\n \nlocal\n\n            \n},\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nThe nice thing about this style of SSH configuration is that you can have multiple bastion hosts in\ndifferent locations and target the hosts behind each of them, provided that you give your bastion\nhosts different names.  The method of accessing them is the same between direct SSH connections and\nAnsible execution.  Once you have this infrastructure in place, you can start distributing public\nSSH keys to your protected hosts.\n\n\nWrite a \ntemplates/etc_sudoers\n file that grants NOPASSWD access to the sudo group:\n\n\nDefaults\n    \nenv_reset\n\n\nDefaults\n    \nmail_badpass\n\n\nDefaults\n    \nsecure_path\n=\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n\n\n# Host alias specification\n\n\n\n# User alias specification\n\n\n\n# Cmnd alias specification\n\n\n\n# User privilege specification\n\n\nroot\n    \nALL\n=\n(\nALL\n:\nALL\n)\n \nALL\n\n\n\n# Allow members of group sudo to execute any command\n\n\n%sudo\n   \nALL\n=\nNOPASSWD\n:\n \nALL\n\n\n\n#include\ndir /etc/sudoers.d\n\n\n\n\n\n\nWrite a playbook \nupdate_remote-ssh.yml\n to configure NOPASSWD sudo access for your remote user and\ndistribute SSH public keys on your remote hosts.  This will allow subsequent playbook execution to\noperate more easily against your remote hosts.  In order for this to work, the paramiko connection\ntype must be used initially, so that the password can be requested once and re-used across all hosts.\n\n\n-\n \nname\n:\n \nupdate remote ssh\n\n  \nhosts\n:\n\n  \n-\n \ngroup-all\n\n  \ngather_facts\n:\n \nfalse\n\n  \nsudo\n:\n \ntrue\n\n  \nconnection\n:\n \nparamiko\n\n  \nvars_files\n:\n\n  \n-\n \nssh_keys/ssh_key_vault.yml\n\n  \ntasks\n:\n\n  \n-\n \ncopy\n:\n \nsrc=templates/etc_sudoers dest=/etc/sudoers mode=0440 owner=root group=root\n\n\n  \n-\n \nuser\n:\n \nname=remoteuser groups=sudo shell=/bin/bash state=present\n\n\n  \n-\n \nauthorized_key\n:\n \nuser=remoteuser state=present key={{ssh_public_key}}\n\n\n\n\n\n\nRun an SSH configuration playbook against remote hosts through the SSH tunnel, providing the SSH\npassword, sudo password and vault password:\n\n\nansible-playbook -i inventory update_remote-ssh.yml --ask-pass --ask-sudo-pass --ask-vault-pass\nSSH password:\nsudo password \n[\ndefaults to SSH password\n]\n:\nVault password:\n\nPLAY \n[\nupdate ssh\n]\n *************************************************************\n\nTASK: \n[\ncopy \nsrc\n=\ntemplates/etc_sudoers \ndest\n=\n/etc/sudoers \nmode\n=\n0440\n \nowner\n=\nroot \ngroup\n=\nroot\n]\n ***\nok: \n[\ni-00000001\n]\n\nok: \n[\ni-00000002\n]\n\nok: \n[\ni-00000003\n]\n\nok: \n[\ni-00000004\n]\n\n\nTASK: \n[\nuser \nname\n=\nremoteuser \ngroups\n=\nsudo \nshell\n=\n/bin/bash \nstate\n=\npresent\n]\n ***\nok: \n[\ni-00000001\n]\n\nok: \n[\ni-00000002\n]\n\nok: \n[\ni-00000003\n]\n\nok: \n[\ni-00000004\n]\n\n\nTASK: \n[\nauthorized_key \nuser\n=\nremoteuser \nstate\n=\npresent \nkey\n=\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk\n]\n ***\nok: \n[\ni-00000001\n]\n\nok: \n[\ni-00000002\n]\n\nok: \n[\ni-00000003\n]\n\nok: \n[\ni-00000004\n]\n\n\n\nPLAY RECAP ********************************************************************\ni-00000001               : \nok\n=\n3\n    \nchanged\n=\n0\n    \nunreachable\n=\n0\n    \nfailed\n=\n0\n\ni-00000002               : \nok\n=\n3\n    \nchanged\n=\n0\n    \nunreachable\n=\n0\n    \nfailed\n=\n0\n\ni-00000003               : \nok\n=\n3\n    \nchanged\n=\n0\n    \nunreachable\n=\n0\n    \nfailed\n=\n0\n\ni-00000004               : \nok\n=\n3\n    \nchanged\n=\n0\n    \nunreachable\n=\n0\n    \nfailed\n=\n0\n\n\n\n\n\n\nThe best way to keep Ansible output concise is to run without verbosity -- only crank this up if you\nneed it to diagnose a problem.", 
            "title": "Ansible Vault and SSH Key Distribution"
        }, 
        {
            "location": "/ansible/ansible-vault-and-ssh-key-distribution/#ansible-vault-and-ssh-key-distribution", 
            "text": "2014-06-30  There are two types of SSH key distribution discussed in this post: private keys on local hosts and\npublic keys on remote hosts.  SSH private key distribution is best used for setting up your own\nworkstation or possibly an  Ansible Tower  server.  In general, you\nshould not be distributing private keys widely; with a good SSH tunneling configuration and SSH\npublic key distribution, there should be no need for the private keys to be installed in more than\nfew places.  This configuration will show off a technique for configuring an SSH jump host bastion\nthat allows you to keep your private key on your own workstation; there is no need to have the SSH\nprivate key on the bastion host.  For the purpose of this post, I have generated a new SSH key pair to demonstrate this technique;\nthis keypair is used nowhere.  Part of the trick to making this work is that the private key needs\nto be base64 encoded so that line breaks are preserved when it is stored as a yaml string in the vars_files .  Template files are created for the public and private keys to preserve file change\ndetection.  Generate a new key pair:  $ ssh-keygen -b  2048  -f junk_key -C junk\nGenerating public/private rsa key pair.\nEnter passphrase  ( empty  for  no passphrase ) :\nEnter same passphrase again:\nYour identification has been saved in junk_key.\nYour public key has been saved in junk_key.pub.\nThe key fingerprint is: 94 :94:ae:ac:c3:5e:ee:7d:fa:2c:cb:0f:ae:19:c8:99 junk\nThe key s randomart image is:\n+-- [  RSA  2048 ] ----+ |         ..        |  |        ...        |  |        .o         |  |        ..         |  |      . .S         |  |    . +o           |  |    .E.o .         |  |     +o *.o.       |  |    ..o = .*B+       | \n+-----------------+ /pre   Base64 encode the private key:  base64 -i junk_key   junk_key.b64  Create an Ansible vars_files yaml data file named  ssh_keys/ssh_key_vault.yml .  The ssh_private_key  variable should contain the base64 encoded private key and the  ssh_public_key \nvariable should contain the public key. Encrypt the file with  ansible-vault :  ssh_private_key :   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  ssh_public_key :   ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk   ansible-vault encrypt ssh_keys/ssh_key_vault.yml\nVault password:\nConfirm Vault password:\nEncryption successful  Create an inventory file named  inventory , showing off the SSH jump host connection capability:  [localhost]  localhost ansible_connection = local  [group-all:children]  group-01  group-02  [group-01]  i-00000001 ansible_ssh_host = bastion+192.168.1.1 ansible_ssh_user=remoteuser  i-00000002 ansible_ssh_host = bastion+192.168.1.2 ansible_ssh_user=remoteuser  [group-02]  i-00000003 ansible_ssh_host = bastion+192.168.1.3 ansible_ssh_user=remoteuser  i-00000004 ansible_ssh_host = bastion+192.168.1.4 ansible_ssh_user=remoteuser   Create a template file for the private key named  templates/HOME_.ssh_junk :  {{ ssh_private_key_decoded.stdout }}   Create a template file for the public key named  templates/HOME_.ssh_junk.pub :  {{ ssh_public_key }}   Create a template file for the SSH jump host configuration named  templates/HOME_.ssh_config :  Host *      ServerAliveInterval 30      ServerAliveCountMax 5  Host bastion      User  {{ remote_user }}      IdentityFile ~/.ssh/junk      Hostname bastion  Host bastion+*      User  {{ remote_user }}      IdentityFile ~/.ssh/junk      ProxyCommand ssh -T -a bastion nc $(echo %h |cut -d+ -f2) %p 2 /dev/null      StrictHostKeyChecking no   This configuration assumes that you have a consistent remote username defined on the bastion server\nand your protected hosts.  Write a playbook to install the SSH key and configuration on your local workstation named config_local-ssh.yml :  -   name :   configure local ssh \n   hosts : \n   -   localhost \n   gather_facts :   false \n   sudo :   false \n   vars : \n     local_home :   {{   lookup( env , HOME )   }} \n     local_user :   {{   lookup( env , USER )   }} \n     remote_user :   remoteuser \n   vars_files : \n   -   ssh_keys/ssh_key_vault.yml \n   tasks : \n   -   file :   path={{local_home}}/.ssh state=directory mode=0700 owner={{local_user}} \n\n   -   template :   src=templates/HOME_.ssh_config dest={{local_home}}/.ssh/config mode=0644 owner={{local_user}} backup=yes \n\n   -   shell :   echo {{ssh_private_key}} |base64 --decode \n     register :   ssh_private_key_decoded \n\n   -   template :   src=templates/HOME_.ssh_junk dest={{local_home}}/.ssh/junk mode=0600 owner={{local_user}} \n\n   -   template :   src=templates/HOME_.ssh_junk.pub dest={{local_home}}/.ssh/junk.pub mode=0644 owner={{local_user}}   Run the playbook to setup your local workstation with SSH keys and configuration:  ansible-playbook -i inventory config_local-ssh.yml --ask-vault-pass\nVault password:  Test your SSH tunneling access to a remote host behind the bastion server:  ssh bastion+192.168.1.1\nssh bastion+192.168.1.1  date; date   /tmp/date.out \nscp bastion+192.168.1.1:/tmp/date.out .  Notice that the hosts behind the bastion server are referenced in the SSH command the same way that\nthey are referenced in the Ansible inventory file.  The \"+\" character used as a separator was\nselected explicitly for its ability to be used interchangeably at the command line and in the\nAnsible inventory.  IP addresses are being used on the right hand side of the expression since the\nsecondary connection to the protected host relies on the name resolution capabilities of the first\nhost in the tunnel.  If you had a reliable dynamic DNS service that was keeping up with changes to\nthe protected hosts and was accessible to the bastion host, then you could use host names instead,\nsuch as  bastion+webserver01 .  This host selection technique can be extended to an Ansible dynamic\ninventory script, if you were running instances at a cloud provider such as AWS.  When you write a dynamic inventory script , the data format\nshould look like this:  { \n     group-01 :   { \n         hosts :   [ \n             i-00000001 , \n             i-00000002 \n         ] \n     }, \n     group-02 :   { \n         hosts :   [ \n             i-00000003 , \n             i-00000004 \n         ] \n     }, \n     group-all :   { \n         children :   [ \n             group-01 , \n             group-02 \n         ] \n     }, \n     localhost :   [ \n         localhost \n     ], \n     _meta :   { \n         hostvars :   { \n             i-00000001 :   { \n                 ansible_ssh_host :   bastion+192.168.1.1 , \n                 ansible_ssh_user :   remoteuser \n             }, \n             i-00000002 :   { \n                 ansible_ssh_host :   bastion+192.168.1.2 , \n                 ansible_ssh_user :   remoteuser \n             }, \n             i-00000003 :   { \n                 ansible_ssh_host :   bastion+192.168.1.3 , \n                 ansible_ssh_user :   remoteuser \n             }, \n             i-00000004 :   { \n                 ansible_ssh_host :   bastion+192.168.1.4 , \n                 ansible_ssh_user :   remoteuser \n             }, \n             localhost :   { \n                 ansible_connection :   local \n             }, \n         } \n     }  }   The nice thing about this style of SSH configuration is that you can have multiple bastion hosts in\ndifferent locations and target the hosts behind each of them, provided that you give your bastion\nhosts different names.  The method of accessing them is the same between direct SSH connections and\nAnsible execution.  Once you have this infrastructure in place, you can start distributing public\nSSH keys to your protected hosts.  Write a  templates/etc_sudoers  file that grants NOPASSWD access to the sudo group:  Defaults      env_reset  Defaults      mail_badpass  Defaults      secure_path = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin  # Host alias specification  # User alias specification  # Cmnd alias specification  # User privilege specification  root      ALL = ( ALL : ALL )   ALL  # Allow members of group sudo to execute any command  %sudo     ALL = NOPASSWD :   ALL  #include dir /etc/sudoers.d   Write a playbook  update_remote-ssh.yml  to configure NOPASSWD sudo access for your remote user and\ndistribute SSH public keys on your remote hosts.  This will allow subsequent playbook execution to\noperate more easily against your remote hosts.  In order for this to work, the paramiko connection\ntype must be used initially, so that the password can be requested once and re-used across all hosts.  -   name :   update remote ssh \n   hosts : \n   -   group-all \n   gather_facts :   false \n   sudo :   true \n   connection :   paramiko \n   vars_files : \n   -   ssh_keys/ssh_key_vault.yml \n   tasks : \n   -   copy :   src=templates/etc_sudoers dest=/etc/sudoers mode=0440 owner=root group=root \n\n   -   user :   name=remoteuser groups=sudo shell=/bin/bash state=present \n\n   -   authorized_key :   user=remoteuser state=present key={{ssh_public_key}}   Run an SSH configuration playbook against remote hosts through the SSH tunnel, providing the SSH\npassword, sudo password and vault password:  ansible-playbook -i inventory update_remote-ssh.yml --ask-pass --ask-sudo-pass --ask-vault-pass\nSSH password:\nsudo password  [ defaults to SSH password ] :\nVault password:\n\nPLAY  [ update ssh ]  *************************************************************\n\nTASK:  [ copy  src = templates/etc_sudoers  dest = /etc/sudoers  mode = 0440   owner = root  group = root ]  ***\nok:  [ i-00000001 ] \nok:  [ i-00000002 ] \nok:  [ i-00000003 ] \nok:  [ i-00000004 ] \n\nTASK:  [ user  name = remoteuser  groups = sudo  shell = /bin/bash  state = present ]  ***\nok:  [ i-00000001 ] \nok:  [ i-00000002 ] \nok:  [ i-00000003 ] \nok:  [ i-00000004 ] \n\nTASK:  [ authorized_key  user = remoteuser  state = present  key = ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk ]  ***\nok:  [ i-00000001 ] \nok:  [ i-00000002 ] \nok:  [ i-00000003 ] \nok:  [ i-00000004 ] \n\n\nPLAY RECAP ********************************************************************\ni-00000001               :  ok = 3      changed = 0      unreachable = 0      failed = 0 \ni-00000002               :  ok = 3      changed = 0      unreachable = 0      failed = 0 \ni-00000003               :  ok = 3      changed = 0      unreachable = 0      failed = 0 \ni-00000004               :  ok = 3      changed = 0      unreachable = 0      failed = 0   The best way to keep Ansible output concise is to run without verbosity -- only crank this up if you\nneed it to diagnose a problem.", 
            "title": "Ansible Vault and SSH Key Distribution"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/", 
            "text": "Running Ansible Playbooks on Windows\n\n\n2014-06-29\n\n\nBut First, Some History\n\n\nIn early 2006, running almost a thousand servers for Blackboard Product Development that were evenly\ndistributed across Windows, Linux and Solaris, we needed an automation tool that would allow us to\nquickly deploy and configure the Blackboard Learning Management System (LMS).  The real sticking\npoint for us was managing the Windows ecosystem.\n\n\nThe \nfirst commit\n\nof PuppetLabs Puppet occurred in April 2005 and the \nfirst tagged release\n\nwas on Jan 3, 2006.  The \nfirst commit\n\nof OpsCode Chef occurred in March 2008 and the \nfirst tagged release\n\nwas on Jan 31, 2009.  Needless to say, the modern configuration management ecosystem was much sparser\nin 2006 then as compared to today.  Puppet was still very new and in the process of gaining mind share\nand adding functionality; it did not support Windows at first.  \nCFEngine\n was\navailable, but it also did not support Windows in the open source version.\n\n\nI worked with Dave Carter at the beginning of 2006 to develop our own in-house configuration\nmanagement system that we called Fusion.  It was based on \nAnt\n and\n\nAnt-Contrib\n.  Since Blackboard was a\nJava based application, we always had one or more versions of JDKs installed on our systems as a\npart of the imaging or virtual machine cloning process, so picking a tool that ran on the JDK made\nsense and offered us the platform independence we needed.  Dave developed a state machine with a\nsocket listener that would accept XML-formatted messages and then kick off various tasks.  I\ndeveloped a library and property inheritance hierarchy for the the system, along with a parallel job\nexecution client and added the set of scripts that deployed and configured the Blackboard LMS.  I\nfigured out that by cherry-picking a few key utilities out of the \nUnxUtils distribution\n,\nI could write Windows batch scripts in a manner similar to Linux bash scripts and this lent to a\nsomewhat manageable level of consistency between the disparate operating systems without completely\nabandoning the hooks we needed for Windows.\n\n\nMotivation\n\n\nAnyone who has spoken with me in the past year about my work knows that \nAnsible\n\nis hands-down my favorite piece of software tooling.  Using Ansible, I was able to effectively\nmanage 500-odd Ubuntu Linux systems at Blackboard, half of which were deployed in a VPC at AWS and\nhalf of which were deployed in the Blackboard Managed Hosting data centers.  Part of the challenge\nthat we had at Blackboard in the Product Development department is that 30-40% of the several\nthousand servers used for development and testing were Windows, which made it difficult to choose a\nsingle configuration management system to rule them all.  For the better part of a year, I kept\nsaying that it was a terrible shame that Ansible did not support Windows and that Opscode Chef would\nprobably be the best choice for configuration management of all systems, since it arguably had the\nbest support for that platform in 2013.  After meeting with \nMichael DeHaan\n,\ncreator and CTO of Ansible, at Blackboard headquarters, we talked through their rough plans for\nWindows support.  In short, it was something they wanted to be thoughtful about.  Some time later,\nwe had a hack day organized at Blackboard and I decided that I would attempt to develop an Ansible\nplaybook that could install and uninstall a JDK on Windows, using my previous experience with\nbuilding the Fusion configuration management system.\n\n\nIt turns out that this works. Quite well. Even though it wasn't intended to.\n\n\nAnsible Roadmap Update\n\n\nOn June 19, 2014, Michael DeHaan announced \nWindows Is Coming\n.\nPowerShell remoting is a far cleaner solution and I am looking forward to seeing it hit the release\nbranch, although I don't have to worry about Windows machines so much these days.  I learned this\nnifty fact from \nAnsible Weekly Issue 38\n;\nthis is not a bad way to keep up on the latest Ansible news.\n\n\nPre-Requisites\n\n\nWindows + Cygwin + SSHd + Python\n\n\nPlaybooks\n\n\nAnsible inventory file \nhosts\n:\n\n\n[w7x64-jf]\n\n\nw7x64-jf.pd.local\n\n\n\n\n\n\nJDK7 installation playbook \njdk7_install.yml\n:\n\n\n-\n \nname\n:\n \ninstall oracle jdk7\n\n  \nhosts\n:\n\n  \n-\n \nw7x64-jf\n\n  \nuser\n:\n \nadministrator\n\n  \ngather_facts\n:\n \nfalse\n\n  \nvars\n:\n\n    \nversion\n:\n \n7u45\n\n    \nbuild\n:\n \nb18\n\n    \nversion_padded\n:\n \n1.7.0_45\n\n    \ndodrootca2\n:\n \nc:\\\\jdk\\{{version_padded}}\\\\jre\\\\lib\\\\security\\\\dod.root-ca-2.pem\n\n    \ncacerts\n:\n \nc:\\\\jdk{{version_padded}}\\\\jre\\\\lib\\\\security\\\\cacerts\n\n  \ntasks\n:\n\n  \n-\n \ncommand\n:\n \nwget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/{{version}}-{{build}}/jdk-{{version}}-windows-x64.exe creates=/usr/local/src/jdk-{{version}}-windows-x64.exe\n\n\n  \n-\n \nfile\n:\n \npath=/usr/local/src/jdk-{{version}}-windows-x64.exe mode=0755\n\n\n  \n-\n \nshell\n:\n \n/usr/local/src/jdk-{{version}}-windows-x64.exe /s INSTALLDIR=c:\\\\jdk{{version_padded}} /INSTALLDIRPUBJRE=c:\\\\jre{{version_padded}} REBOOT=Suppress ADDLOCAL=ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE=0 SYSTRAY=0 SYSTRAY=0 /L c:\\\\cygwin\\\\usr\\\\local\\\\src\\\\jdk-install-log.txt creates=/cygdrive/c/jdk{{version_padded}}\n\n\n  \n-\n \ncopy\n:\n \nsrc=../certs/dod.root-ca-2.pem dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/dod.root-ca-2.pem\n\n\n  \n-\n \ncommand\n:\n \n/cygdrive/c/jdk{{version_padded}}/bin/keytool -import -trustcacerts -alias dodrootca2 -file {{dodrootca2}} -keystore $cacerts -storepass changeit -noprompt\n\n    \nignore_errors\n:\n \ntrue\n\n\n  \n-\n \ncopy\n:\n \nsrc=files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/US_export_policy.jar\n\n\n  \n-\n \ncopy\n:\n \nsrc=files/cygdrive_c_java_jre_lib_security_local_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/local_policy.jar\n\n\n  \n-\n \nshell\n:\n \n/cygdrive/c/jdk{{version_padded}}/bin/java -version 2\n1 |head -1 |awk \n{print $3}\n |sed -e \ns/\n//g\n\n    \nregister\n:\n \njava_version\n\n\n  \n-\n \nfail\n:\n \nmsg=\nThe Java version does not match the expected value {{ version_padded }}.\n\n    \nwhen\n:\n \n{{\n \njava_version.stdout\n \n}}\n \n!=\n \n{{\n \nversion_padded\n \n}}\n\n\n\n\n\n\nJDK7 uninstallation playbook \njdk7_uninstall.yml\n:\n\n\n-\n \nname\n:\n \nuninstall oracle jdk7\n\n  \nhosts\n:\n\n  \n-\n \nw7x64-jf\n\n  \nuser\n:\n \nadministrator\n\n  \ngather_facts\n:\n \nfalse\n\n  \nvars\n:\n\n    \nversion\n:\n \n7u45\n\n    \nversion_padded\n:\n \n1.7.0_45\n\n    \nversion_text\n:\n \n7\n \nUpdate\n \n45\n\n  \ntasks\n:\n\n  \n-\n \ntemplate\n:\n \nsrc=templates/usr_local_src_remove-programs.vbs dest=/usr/local/src/remove-programs.vbs\n\n\n  \n-\n \nshell\n:\n \ncscript remove-programs.vbs |grep \nJava {{version_text}} (64-bit)\n chdir=/usr/local/src\n\n    \nregister\n:\n \nresult\n\n    \nignore_errors\n:\n \ntrue\n\n\n  \n-\n \ncommand\n:\n \ncscript remove-programs.vbs /uninstall \nJava {{version_text}} (64-bit)\n chdir=/usr/local/src\n\n    \nwhen\n:\n \nresult|success\n\n\n  \n-\n \nshell\n:\n \ncscript remove-programs.vbs |grep \nJava SE Development Kit {{version_text}} (64-bit)\n chdir=/usr/local/src\n\n    \nregister\n:\n \nresult\n\n    \nignore_errors\n:\n \ntrue\n\n\n  \n-\n \ncommand\n:\n \ncscript remove-programs.vbs /uninstall \nJava SE Development Kit {{version_text}} (64-bit)\n chdir=/usr/local/src\n\n    \nwhen\n:\n \nresult|success\n\n\n  \n-\n \nfile\n:\n \npath={{item}} state=absent\n\n    \nwith_items\n:\n\n    \n-\n \n/usr/local/src/jdk-{{version}}-windows-x64.exe\n\n    \n-\n \n/usr/local/src/jdk-install-log.txt\n\n    \n-\n \n/usr/local/src/remove-programs.vbs\n\n\n\n\n\n\nThe \nremove-programs.vbs\n helper script:\n\n\nIf Wscript.Arguments.Count = 0 Then\n  inventory_software()\nElseIf Wscript.Arguments.Count = 2 Then\n  If Wscript.Arguments(0) = \n/uninstall\n Then\n    \nExpecting: cscript remove-programs.vbs /uninstall \nJava(TM) 6 Update 26\n\n    \nExpecting: cscript remove-programs.vbs /uninstall \nJava(TM) SE Development Kit 6 Update 26\n\n    uninstall_software(Wscript.Arguments(1))\n  Else\n    Wscript.Echo \nUsage: remove-programs.vbs [/uninstall \nsoftware\n]\n\n  End If\nElse\n  Wscript.Echo \nUsage: remove-programs.vbs [/uninstall \nsoftware\n]\n\nEnd If\n\nSub inventory_software()\n  strComputer = \n.\n\n\n  Set objWMIService = GetObject(\nwinmgmts:\n _\n    \n \n{impersonationLevel=impersonate}!\\\\\n _\n    \n strComputer \n \n\\root\\cimv2\n)\n  Set colSoftware = objWMIService.ExecQuery _\n    (\nSelect * from Win32_Product\n)\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo \nName: \n \n objSoftware.Name\n    \nWscript.Echo \nVersion: \n \n objSoftware.Version\n  Next\nEnd Sub\n\nSub inventory_java_software()\n  strComputer = \n.\n\n\n  Set objWMIService = GetObject(\nwinmgmts:\n _\n    \n \n{impersonationLevel=impersonate}!\\\\\n _\n    \n strComputer \n \n\\root\\cimv2\n)\n  Set colSoftware = objWMIService.ExecQuery _\n    (\nSelect * from Win32_Product \n _\n        \n \nWhere Name Like \nJava%\n)\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo \nName: \n \n objSoftware.Name\n    Wscript.Echo \nVersion: \n \n objSoftware.Version\n  Next\nEnd Sub\n\nSub uninstall_software(strApplicationName)\n  \nMake sure to run this with Administrator privileges\n  strComputer = \n.\n\n\n  Set objWMIService = GetObject(\nwinmgmts:\n _\n    \n \n{impersonationLevel=impersonate}!\\\\\n _\n    \n strComputer \n \n\\root\\cimv2\n)\n  Set colSoftware = objWMIService.ExecQuery _\n    (\nSelect * From Win32_Product Where Name = \n _\n    \n strApplicationName \n \n)\n\n  For Each objSoftware in colSoftware\n    objSoftware.Uninstall()\n  Next\nEnd Sub\n\n\n\n\n\nDemonstration\n\n\nFile system state before install:\n\n\nadministrator@W7X64-JF~\n$ ls -l /cygdrive/c \n|\negrep \njdk|jre\n\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Jun \n24\n \n2010\n jdk5\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk6\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk7\n\n\n\n\n\nInstall JDK7 on Windows:\n\n\ncopperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_install.yml\n\nPLAY \n[\ninstall oracle jdk7\n]\n ****************************************************\n\nTASK: \n[\ncommand\n wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24\n=\nhttp%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/7u45-b18/jdk-7u45-windows-x64.exe \ncreates\n=\n/usr/local/src/jdk-7u45-windows-x64.exe\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nfile \npath\n=\n/usr/local/src/jdk-7u45-windows-x64.exe \nmode\n=\n0755\n]\n ***********\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nshell /usr/local/src/jdk-7u45-windows-x64.exe /s \nINSTALLDIR\n=\nc:\n\\\\\njdk1.7.0_45 /INSTALLDIRPUBJRE\n=\nc:\n\\\\\njre1.7.0_45 \nREBOOT\n=\nSuppress \nADDLOCAL\n=\nToolsFeature,SourceFeature,PublicjreFeature \nAUTOUPDATE\n=\n0\n \nSYSTRAY\n=\n0\n \nSYSTRAY\n=\n0\n /L c:\n\\\\\ncygwin\n\\\\\nusr\n\\\\\nlocal\n\\\\\nsrc\n\\\\\njdk-install-log.txt \ncreates\n=\n/cygdrive/c/jdk1.7.0_45\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncopy \nsrc\n=\n../certs/dod.root-ca-2.pem \ndest\n=\n/cygdrive/c/jdk1.7.0_45/jre/lib/security/dod.root-ca-2.pem\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncommand\n /cygdrive/c/jdk1.7.0_45/bin/keytool -import -trustcacerts -alias dodrootca2 -file c:\n\\\\\njdk1.7.0_45\n\\\\\njre\n\\\\\nlib\n\\\\\nsecurity\n\\\\\ndod.root-ca-2.pem -keystore c:\n\\\\\njdk1.7.0_45\n\\\\\njre\n\\\\\nlib\n\\\\\nsecurity\n\\\\\ncacerts -storepass changeit -noprompt\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncopy \nsrc\n=\nfiles/cygdrive_c_java_jre_lib_security_US_export_policy.jar \ndest\n=\n/cygdrive/c/jdk1.7.0_45/jre/lib/security/US_export_policy.jar\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncopy \nsrc\n=\nfiles/cygdrive_c_java_jre_lib_security_local_policy.jar \ndest\n=\n/cygdrive/c/jdk1.7.0_45/jre/lib/security/local_policy.jar\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nshell /cygdrive/c/jdk1.7.0_45/bin/java -version \n2\n1\n \n|\nhead -1 \n|\nawk \n{print $3}\n \n|\nsed -e \ns/\n//g\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nfail \nmsg\n=\nThe Java version does not match the expected value 1.7.0_45.\n]\n ***\nskipping: \n[\nw7x64-jf.pd.local\n]\n\n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local : \nok\n=\n8\n \nchanged\n=\n8\n \nunreachable\n=\n0\n \nfailed\n=\n0\n\n\n\n\n\n\nFile system state after install and before uninstall:\n\n\nadministrator@W7X64-JF ~\n$ ls -l /cygdrive/c \n|\negrep \njdk|jre\n\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Dec \n12\n \n23\n:17 jdk1.7.0_45\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Jun \n24\n \n2010\n jdk5\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk6\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk7\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Dec \n12\n \n23\n:17 jre1.7.0_45\n\n\n\n\n\nUninstall JDK7 on Windows:\n\n\ncopperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_uninstall.yml\n\nPLAY \n[\nuninstall oracle jdk7\n]\n **************************************************\n\nTASK: \n[\ntemplate \nsrc\n=\ntemplates/usr_local_src_remove-programs.vbs \ndest\n=\n/usr/local/src/remove-programs.vbs\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nshell cscript remove-programs.vbs \n|\ngrep \nJava 7 Update 45 (64-bit)\n \nchdir\n=\n/usr/local/src\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncommand\n cscript remove-programs.vbs /uninstall \nJava 7 Update 45 (64-bit)\n \nchdir\n=\n/usr/local/src\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nshell cscript remove-programs.vbs \n|\ngrep \nJava SE Development Kit 7 Update 45 (64-bit)\n \nchdir\n=\n/usr/local/src\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\ncommand\n cscript remove-programs.vbs /uninstall \nJava SE Development Kit 7 Update 45 (64-bit)\n \nchdir\n=\n/usr/local/src\n]\n ***\nchanged: \n[\nw7x64-jf.pd.local\n]\n\n\nTASK: \n[\nfile \npath\n=\n$item\n \nstate\n=\nabsent\n]\n ******************************************\nchanged: \n[\nw7x64-jf.pd.local\n]\n \n=\n \n(\nitem\n=\n/usr/local/src/jdk-7u45-windows-x64.exe\n)\n\nchanged: \n[\nw7x64-jf.pd.local\n]\n \n=\n \n(\nitem\n=\n/usr/local/src/jdk-install-log.txt\n)\n\nchanged: \n[\nw7x64-jf.pd.local\n]\n \n=\n \n(\nitem\n=\n/usr/local/src/remove-programs.vbs\n)\n\n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local : \nok\n=\n6\n \nchanged\n=\n6\n \nunreachable\n=\n0\n \nfailed\n=\n0\n\n\n\n\n\n\nFile system state after uninstall:\n\n\nadministrator@W7X64-JF ~\n$ ls -l /cygdrive/c \n|\negrep \njdk|jre\n\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Jun \n24\n \n2010\n jdk5\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk6\ndrwx------+ \n1\n SYSTEM SYSTEM \n0\n Oct \n4\n \n2012\n jdk7", 
            "title": "Running Ansible Playbooks on Windows"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#running-ansible-playbooks-on-windows", 
            "text": "2014-06-29", 
            "title": "Running Ansible Playbooks on Windows"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#but-first-some-history", 
            "text": "In early 2006, running almost a thousand servers for Blackboard Product Development that were evenly\ndistributed across Windows, Linux and Solaris, we needed an automation tool that would allow us to\nquickly deploy and configure the Blackboard Learning Management System (LMS).  The real sticking\npoint for us was managing the Windows ecosystem.  The  first commit \nof PuppetLabs Puppet occurred in April 2005 and the  first tagged release \nwas on Jan 3, 2006.  The  first commit \nof OpsCode Chef occurred in March 2008 and the  first tagged release \nwas on Jan 31, 2009.  Needless to say, the modern configuration management ecosystem was much sparser\nin 2006 then as compared to today.  Puppet was still very new and in the process of gaining mind share\nand adding functionality; it did not support Windows at first.   CFEngine  was\navailable, but it also did not support Windows in the open source version.  I worked with Dave Carter at the beginning of 2006 to develop our own in-house configuration\nmanagement system that we called Fusion.  It was based on  Ant  and Ant-Contrib .  Since Blackboard was a\nJava based application, we always had one or more versions of JDKs installed on our systems as a\npart of the imaging or virtual machine cloning process, so picking a tool that ran on the JDK made\nsense and offered us the platform independence we needed.  Dave developed a state machine with a\nsocket listener that would accept XML-formatted messages and then kick off various tasks.  I\ndeveloped a library and property inheritance hierarchy for the the system, along with a parallel job\nexecution client and added the set of scripts that deployed and configured the Blackboard LMS.  I\nfigured out that by cherry-picking a few key utilities out of the  UnxUtils distribution ,\nI could write Windows batch scripts in a manner similar to Linux bash scripts and this lent to a\nsomewhat manageable level of consistency between the disparate operating systems without completely\nabandoning the hooks we needed for Windows.", 
            "title": "But First, Some History"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#motivation", 
            "text": "Anyone who has spoken with me in the past year about my work knows that  Ansible \nis hands-down my favorite piece of software tooling.  Using Ansible, I was able to effectively\nmanage 500-odd Ubuntu Linux systems at Blackboard, half of which were deployed in a VPC at AWS and\nhalf of which were deployed in the Blackboard Managed Hosting data centers.  Part of the challenge\nthat we had at Blackboard in the Product Development department is that 30-40% of the several\nthousand servers used for development and testing were Windows, which made it difficult to choose a\nsingle configuration management system to rule them all.  For the better part of a year, I kept\nsaying that it was a terrible shame that Ansible did not support Windows and that Opscode Chef would\nprobably be the best choice for configuration management of all systems, since it arguably had the\nbest support for that platform in 2013.  After meeting with  Michael DeHaan ,\ncreator and CTO of Ansible, at Blackboard headquarters, we talked through their rough plans for\nWindows support.  In short, it was something they wanted to be thoughtful about.  Some time later,\nwe had a hack day organized at Blackboard and I decided that I would attempt to develop an Ansible\nplaybook that could install and uninstall a JDK on Windows, using my previous experience with\nbuilding the Fusion configuration management system.  It turns out that this works. Quite well. Even though it wasn't intended to.", 
            "title": "Motivation"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#ansible-roadmap-update", 
            "text": "On June 19, 2014, Michael DeHaan announced  Windows Is Coming .\nPowerShell remoting is a far cleaner solution and I am looking forward to seeing it hit the release\nbranch, although I don't have to worry about Windows machines so much these days.  I learned this\nnifty fact from  Ansible Weekly Issue 38 ;\nthis is not a bad way to keep up on the latest Ansible news.", 
            "title": "Ansible Roadmap Update"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#pre-requisites", 
            "text": "Windows + Cygwin + SSHd + Python", 
            "title": "Pre-Requisites"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#playbooks", 
            "text": "Ansible inventory file  hosts :  [w7x64-jf]  w7x64-jf.pd.local   JDK7 installation playbook  jdk7_install.yml :  -   name :   install oracle jdk7 \n   hosts : \n   -   w7x64-jf \n   user :   administrator \n   gather_facts :   false \n   vars : \n     version :   7u45 \n     build :   b18 \n     version_padded :   1.7.0_45 \n     dodrootca2 :   c:\\\\jdk\\{{version_padded}}\\\\jre\\\\lib\\\\security\\\\dod.root-ca-2.pem \n     cacerts :   c:\\\\jdk{{version_padded}}\\\\jre\\\\lib\\\\security\\\\cacerts \n   tasks : \n   -   command :   wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/{{version}}-{{build}}/jdk-{{version}}-windows-x64.exe creates=/usr/local/src/jdk-{{version}}-windows-x64.exe \n\n   -   file :   path=/usr/local/src/jdk-{{version}}-windows-x64.exe mode=0755 \n\n   -   shell :   /usr/local/src/jdk-{{version}}-windows-x64.exe /s INSTALLDIR=c:\\\\jdk{{version_padded}} /INSTALLDIRPUBJRE=c:\\\\jre{{version_padded}} REBOOT=Suppress ADDLOCAL=ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE=0 SYSTRAY=0 SYSTRAY=0 /L c:\\\\cygwin\\\\usr\\\\local\\\\src\\\\jdk-install-log.txt creates=/cygdrive/c/jdk{{version_padded}} \n\n   -   copy :   src=../certs/dod.root-ca-2.pem dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/dod.root-ca-2.pem \n\n   -   command :   /cygdrive/c/jdk{{version_padded}}/bin/keytool -import -trustcacerts -alias dodrootca2 -file {{dodrootca2}} -keystore $cacerts -storepass changeit -noprompt \n     ignore_errors :   true \n\n   -   copy :   src=files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/US_export_policy.jar \n\n   -   copy :   src=files/cygdrive_c_java_jre_lib_security_local_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/local_policy.jar \n\n   -   shell :   /cygdrive/c/jdk{{version_padded}}/bin/java -version 2 1 |head -1 |awk  {print $3}  |sed -e  s/ //g \n     register :   java_version \n\n   -   fail :   msg= The Java version does not match the expected value {{ version_padded }}. \n     when :   {{   java_version.stdout   }}   !=   {{   version_padded   }}   JDK7 uninstallation playbook  jdk7_uninstall.yml :  -   name :   uninstall oracle jdk7 \n   hosts : \n   -   w7x64-jf \n   user :   administrator \n   gather_facts :   false \n   vars : \n     version :   7u45 \n     version_padded :   1.7.0_45 \n     version_text :   7   Update   45 \n   tasks : \n   -   template :   src=templates/usr_local_src_remove-programs.vbs dest=/usr/local/src/remove-programs.vbs \n\n   -   shell :   cscript remove-programs.vbs |grep  Java {{version_text}} (64-bit)  chdir=/usr/local/src \n     register :   result \n     ignore_errors :   true \n\n   -   command :   cscript remove-programs.vbs /uninstall  Java {{version_text}} (64-bit)  chdir=/usr/local/src \n     when :   result|success \n\n   -   shell :   cscript remove-programs.vbs |grep  Java SE Development Kit {{version_text}} (64-bit)  chdir=/usr/local/src \n     register :   result \n     ignore_errors :   true \n\n   -   command :   cscript remove-programs.vbs /uninstall  Java SE Development Kit {{version_text}} (64-bit)  chdir=/usr/local/src \n     when :   result|success \n\n   -   file :   path={{item}} state=absent \n     with_items : \n     -   /usr/local/src/jdk-{{version}}-windows-x64.exe \n     -   /usr/local/src/jdk-install-log.txt \n     -   /usr/local/src/remove-programs.vbs   The  remove-programs.vbs  helper script:  If Wscript.Arguments.Count = 0 Then\n  inventory_software()\nElseIf Wscript.Arguments.Count = 2 Then\n  If Wscript.Arguments(0) =  /uninstall  Then\n     Expecting: cscript remove-programs.vbs /uninstall  Java(TM) 6 Update 26 \n     Expecting: cscript remove-programs.vbs /uninstall  Java(TM) SE Development Kit 6 Update 26 \n    uninstall_software(Wscript.Arguments(1))\n  Else\n    Wscript.Echo  Usage: remove-programs.vbs [/uninstall  software ] \n  End If\nElse\n  Wscript.Echo  Usage: remove-programs.vbs [/uninstall  software ] \nEnd If\n\nSub inventory_software()\n  strComputer =  . \n\n  Set objWMIService = GetObject( winmgmts:  _\n       {impersonationLevel=impersonate}!\\\\  _\n      strComputer    \\root\\cimv2 )\n  Set colSoftware = objWMIService.ExecQuery _\n    ( Select * from Win32_Product )\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo  Name:     objSoftware.Name\n     Wscript.Echo  Version:     objSoftware.Version\n  Next\nEnd Sub\n\nSub inventory_java_software()\n  strComputer =  . \n\n  Set objWMIService = GetObject( winmgmts:  _\n       {impersonationLevel=impersonate}!\\\\  _\n      strComputer    \\root\\cimv2 )\n  Set colSoftware = objWMIService.ExecQuery _\n    ( Select * from Win32_Product   _\n           Where Name Like  Java% )\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo  Name:     objSoftware.Name\n    Wscript.Echo  Version:     objSoftware.Version\n  Next\nEnd Sub\n\nSub uninstall_software(strApplicationName)\n   Make sure to run this with Administrator privileges\n  strComputer =  . \n\n  Set objWMIService = GetObject( winmgmts:  _\n       {impersonationLevel=impersonate}!\\\\  _\n      strComputer    \\root\\cimv2 )\n  Set colSoftware = objWMIService.ExecQuery _\n    ( Select * From Win32_Product Where Name =   _\n      strApplicationName    )\n\n  For Each objSoftware in colSoftware\n    objSoftware.Uninstall()\n  Next\nEnd Sub", 
            "title": "Playbooks"
        }, 
        {
            "location": "/ansible/running-ansible-playbooks-on-windows/#demonstration", 
            "text": "File system state before install:  administrator@W7X64-JF~\n$ ls -l /cygdrive/c  | egrep  jdk|jre \ndrwx------+  1  SYSTEM SYSTEM  0  Jun  24   2010  jdk5\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk6\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk7  Install JDK7 on Windows:  copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_install.yml\n\nPLAY  [ install oracle jdk7 ]  ****************************************************\n\nTASK:  [ command  wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24 = http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/7u45-b18/jdk-7u45-windows-x64.exe  creates = /usr/local/src/jdk-7u45-windows-x64.exe ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ file  path = /usr/local/src/jdk-7u45-windows-x64.exe  mode = 0755 ]  ***********\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ shell /usr/local/src/jdk-7u45-windows-x64.exe /s  INSTALLDIR = c: \\\\ jdk1.7.0_45 /INSTALLDIRPUBJRE = c: \\\\ jre1.7.0_45  REBOOT = Suppress  ADDLOCAL = ToolsFeature,SourceFeature,PublicjreFeature  AUTOUPDATE = 0   SYSTRAY = 0   SYSTRAY = 0  /L c: \\\\ cygwin \\\\ usr \\\\ local \\\\ src \\\\ jdk-install-log.txt  creates = /cygdrive/c/jdk1.7.0_45 ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ copy  src = ../certs/dod.root-ca-2.pem  dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/dod.root-ca-2.pem ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ command  /cygdrive/c/jdk1.7.0_45/bin/keytool -import -trustcacerts -alias dodrootca2 -file c: \\\\ jdk1.7.0_45 \\\\ jre \\\\ lib \\\\ security \\\\ dod.root-ca-2.pem -keystore c: \\\\ jdk1.7.0_45 \\\\ jre \\\\ lib \\\\ security \\\\ cacerts -storepass changeit -noprompt ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ copy  src = files/cygdrive_c_java_jre_lib_security_US_export_policy.jar  dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/US_export_policy.jar ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ copy  src = files/cygdrive_c_java_jre_lib_security_local_policy.jar  dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/local_policy.jar ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ shell /cygdrive/c/jdk1.7.0_45/bin/java -version  2 1   | head -1  | awk  {print $3}   | sed -e  s/ //g ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ fail  msg = The Java version does not match the expected value 1.7.0_45. ]  ***\nskipping:  [ w7x64-jf.pd.local ] \n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local :  ok = 8   changed = 8   unreachable = 0   failed = 0   File system state after install and before uninstall:  administrator@W7X64-JF ~\n$ ls -l /cygdrive/c  | egrep  jdk|jre \ndrwx------+  1  SYSTEM SYSTEM  0  Dec  12   23 :17 jdk1.7.0_45\ndrwx------+  1  SYSTEM SYSTEM  0  Jun  24   2010  jdk5\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk6\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk7\ndrwx------+  1  SYSTEM SYSTEM  0  Dec  12   23 :17 jre1.7.0_45  Uninstall JDK7 on Windows:  copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_uninstall.yml\n\nPLAY  [ uninstall oracle jdk7 ]  **************************************************\n\nTASK:  [ template  src = templates/usr_local_src_remove-programs.vbs  dest = /usr/local/src/remove-programs.vbs ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ shell cscript remove-programs.vbs  | grep  Java 7 Update 45 (64-bit)   chdir = /usr/local/src ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ command  cscript remove-programs.vbs /uninstall  Java 7 Update 45 (64-bit)   chdir = /usr/local/src ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ shell cscript remove-programs.vbs  | grep  Java SE Development Kit 7 Update 45 (64-bit)   chdir = /usr/local/src ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ command  cscript remove-programs.vbs /uninstall  Java SE Development Kit 7 Update 45 (64-bit)   chdir = /usr/local/src ]  ***\nchanged:  [ w7x64-jf.pd.local ] \n\nTASK:  [ file  path = $item   state = absent ]  ******************************************\nchanged:  [ w7x64-jf.pd.local ]   =   ( item = /usr/local/src/jdk-7u45-windows-x64.exe ) \nchanged:  [ w7x64-jf.pd.local ]   =   ( item = /usr/local/src/jdk-install-log.txt ) \nchanged:  [ w7x64-jf.pd.local ]   =   ( item = /usr/local/src/remove-programs.vbs ) \n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local :  ok = 6   changed = 6   unreachable = 0   failed = 0   File system state after uninstall:  administrator@W7X64-JF ~\n$ ls -l /cygdrive/c  | egrep  jdk|jre \ndrwx------+  1  SYSTEM SYSTEM  0  Jun  24   2010  jdk5\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk6\ndrwx------+  1  SYSTEM SYSTEM  0  Oct  4   2012  jdk7", 
            "title": "Demonstration"
        }, 
        {
            "location": "/ansible/testing-ansible-galaxy-roles-with-docker/", 
            "text": "Testing Ansible Galaxy Roles with Docker\n\n\n2014-10-14\n\n\nI learned a neat trick for testing \nAnsible Galaxy\n\n\nroles\n at\n\nAnsibleFest 2014\n.\n\n\nTo get started with Docker on your Mac, install \nVirtualBox\n and then\ninstall \nboot2docker\n, using \nHomebrew\n.\nThe boot2docker package will install docker as a dependency and it runs a small (24MB) Linux\nVirtualBox virtual machine that provides a platform for running Docker images:\n\n\nbrew install boot2docker\nboot2docker init\nboot2docker up\n$(boot2docker shellinit)\n\n\n\n\n\nThe Ansible Team (thanks, \nToshio!\n) has\n\nreleased\n\nDocker images that are included in the \nDocker Hub Registry\n,\nwhich can be used for testing:\n\n\ndocker search ansible |grep ^ansible\nansible/ubuntu14.04-ansible                    Ubuntu 14.04 LTS with ansible                   12\nansible/centos7-ansible                        Ansible on Centos7                              11\n\n\n\n\n\nThere are two tags associated with each of these images: latest and devel.  The latest contains a\nlayered Ansible stable and devel contains a layered Ansible HEAD.  Pull one of these images from the\nDocker Hub:\n\n\ndocker pull ansible/ubuntu14.04-ansible\n\n\n\n\n\nThis technique will allow you to rinse and repeat installs with different roles, which will help you\nfigure out which ones deliver the most suitable functionality for your use cases.\n\n\nLaunch the docker image and leave a shell process running:\n\n\ndocker run -i -t ansible/ubuntu14.04-ansible:stable /bin/bash\n\n\n\n\n\nDownload a role from Ansible Galaxy:\n\n\nansible-galaxy install geerlingguy.memcached\n\n\n\n\n\nCreate a local site.yml playbook to run the role:\n\n\n- hosts: localhost\n  roles:\n      - role: geerlingguy.memcached\n\n\n\n\n\nExecute the playbook:\n\n\nroot@ccca9e7f365f:/# ansible-playbook site.yml -c local\n\nPLAY [localhost] **************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nskipping: [localhost]\n\nTASK: [geerlingguy.memcached | Update apt cache.] *****************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Copy Memcached configuration.] *****************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Ensure Memcached is started and set to run on startup.] ***\nchanged: [localhost]\n\nNOTIFIED: [geerlingguy.memcached | restart memcached] *************************\nchanged: [localhost]\n\nPLAY RECAP ********************************************************************\nlocalhost                  : ok=6    changed=4    unreachable=0    failed=0\n\nroot@ccca9e7f365f:/#", 
            "title": "Testing Ansible Galaxy Roles with Docker"
        }, 
        {
            "location": "/ansible/testing-ansible-galaxy-roles-with-docker/#testing-ansible-galaxy-roles-with-docker", 
            "text": "2014-10-14  I learned a neat trick for testing  Ansible Galaxy  roles  at AnsibleFest 2014 .  To get started with Docker on your Mac, install  VirtualBox  and then\ninstall  boot2docker , using  Homebrew .\nThe boot2docker package will install docker as a dependency and it runs a small (24MB) Linux\nVirtualBox virtual machine that provides a platform for running Docker images:  brew install boot2docker\nboot2docker init\nboot2docker up\n$(boot2docker shellinit)  The Ansible Team (thanks,  Toshio! ) has released \nDocker images that are included in the  Docker Hub Registry ,\nwhich can be used for testing:  docker search ansible |grep ^ansible\nansible/ubuntu14.04-ansible                    Ubuntu 14.04 LTS with ansible                   12\nansible/centos7-ansible                        Ansible on Centos7                              11  There are two tags associated with each of these images: latest and devel.  The latest contains a\nlayered Ansible stable and devel contains a layered Ansible HEAD.  Pull one of these images from the\nDocker Hub:  docker pull ansible/ubuntu14.04-ansible  This technique will allow you to rinse and repeat installs with different roles, which will help you\nfigure out which ones deliver the most suitable functionality for your use cases.  Launch the docker image and leave a shell process running:  docker run -i -t ansible/ubuntu14.04-ansible:stable /bin/bash  Download a role from Ansible Galaxy:  ansible-galaxy install geerlingguy.memcached  Create a local site.yml playbook to run the role:  - hosts: localhost\n  roles:\n      - role: geerlingguy.memcached  Execute the playbook:  root@ccca9e7f365f:/# ansible-playbook site.yml -c local\n\nPLAY [localhost] **************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nskipping: [localhost]\n\nTASK: [geerlingguy.memcached | Update apt cache.] *****************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Copy Memcached configuration.] *****************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Ensure Memcached is started and set to run on startup.] ***\nchanged: [localhost]\n\nNOTIFIED: [geerlingguy.memcached | restart memcached] *************************\nchanged: [localhost]\n\nPLAY RECAP ********************************************************************\nlocalhost                  : ok=6    changed=4    unreachable=0    failed=0\n\nroot@ccca9e7f365f:/#", 
            "title": "Testing Ansible Galaxy Roles with Docker"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/", 
            "text": "AWS Credential Files for Java and Python\n\n\n2014-07-14\n\n\nEach of the AWS tools has slightly different expectations about the location and naming of the\ncredentials file and the various properties within it.  It seems like the Python tools are moving\ncloser to the Java standard as they iterate through releases, but it is still necessary to use a\npatchwork solution to be able to have a unified credentials file.\n\n\nJava SDK\n\n\n\n    \n \nVersion: \n\"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\"\n    \n \nInstallation: \nsbt\n    \n \nLink: \nAWS Java SDK Class ProfilesConfigFile\n\n\n\n\n\nThe standard location for the credentials file is \n~/.aws/credentials\n, which can be overridden with\nthe \nAWS_CREDENTIAL_PROFILES_FILE\n environment variable or by specifying an alternate file location\nin the constructor.  The format of this file is described below:\n\n\n[default]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\n\n[test-user]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\n\n\n\n\nJava Command Line Tools\n\n\n\n    \n \nVersion: \n1.6.13.0\n    \n \nInstallation: \nbrew install ec2-api-tools\n    \n \nLink: \nSetting Up the Amazon EC2 Command Line Interface Tools on Linux/Unix and Mac OS X\n\n\n\n\n\nThe standard configuration is to use environment variables, since these tools have not been updated\nto read the standard AWS credentials file.  Add the following to your \n~/.bash_profile\n, to link the\nrequired data to the standard credentials file and allow for session tokens:\n\n\nexport AWS_CREDENTIAL_FILE=\n$HOME/.aws/credentials\n\nexport AWS_ACCESS_KEY=\n$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\nexport AWS_SECRET_KEY=\n$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\nexport AWS_DELEGATION_TOKEN=\n$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\n\n\n\n\n\nPython Boto\n\n\n\n    \n \nVersion: \nboto==2.31.1 \n botocore==0.56.0\n    \n \nInstallation: \npip install boto\n    \n \nLink: \nBoto Config\n\n\n\n\n\nThe latest version of boto needs to have \naws_security_token\n defined, rather than\n\naws_session_token\n, in the credentials file.  The simplest solution for this is to duplicate the\ntoken between both names; the Java SDK will throw the following log message when reading the extra\nproperty, but will work as expected: \nINFO: Skip unsupported property name aws_security_token in\nprofile [default].\n  Boto will not throw log messages about the existence of the \naws_session_token\n\nproperty.\n\n\nAWS CLI\n\n\n\n    \n \nVersion: \naws-cli/1.3.22\n    \n \nInstallation: \npip install awscli\n    \n \nLink: \nConfiguring the AWS Command Line Interface\n\n\n\n\n\nThe standard location for the credentials file is \n~/.aws/config\n, which can be overridden with the\n\nAWS_CREDENTIAL_FILE\n environment variable.  The latest version of this tool accepts the Java SDK\ncredential file format as-is, including the use of \naws_session_token\n, whereas previous versions\nwanted \naws_security_token\n instead.  When you have multiple profiles in the credentials file, you\ncan select a profile with the tool like so:\n\n\naws --profile test-user s3 ls\n\n\n\n\n\nUnified Solution\n\n\nThe best approach for creating a unified credentials file is to follow the Java credentials file\nformat as closely as possible, while redirecting the Python tools to that file and adding properties\nto cover the corner cases.\n\n\nTo do this, create a \n~/.aws/credentials\n file that duplicates the necessary properties:\n\n\n[default]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\naws_security_token\n=\ntestSessionToken\n\n\n\n[test-user]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\naws_security_token\n=\ntestSessionToken\n\n\n\n[prod-user]\n\n\naws_access_key_id\n=\ntestAccessKey\n\n\naws_secret_access_key\n=\ntestSecretKey\n\n\naws_session_token\n=\ntestSessionToken\n\n\naws_security_token\n=\ntestSessionToken\n\n\n\n\n\n\nAnd add a section to your \n~/.bash_profile\n:\n\n\nexport AWS_CREDENTIAL_FILE=\n$HOME/.aws/credentials\n\nexport AWS_ACCESS_KEY=\n$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\nexport AWS_SECRET_KEY=\n$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\nexport AWS_DELEGATION_TOKEN=\n$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\n\n\n\n\n\n\nWith this configuration, you should be able to move seamlessly between the various Java and Python\ntools available for AWS.", 
            "title": "AWS Credential Files for Java and Python"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#aws-credential-files-for-java-and-python", 
            "text": "2014-07-14  Each of the AWS tools has slightly different expectations about the location and naming of the\ncredentials file and the various properties within it.  It seems like the Python tools are moving\ncloser to the Java standard as they iterate through releases, but it is still necessary to use a\npatchwork solution to be able to have a unified credentials file.", 
            "title": "AWS Credential Files for Java and Python"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#java-sdk", 
            "text": "Version:  \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\"\n       Installation:  sbt\n       Link:  AWS Java SDK Class ProfilesConfigFile   The standard location for the credentials file is  ~/.aws/credentials , which can be overridden with\nthe  AWS_CREDENTIAL_PROFILES_FILE  environment variable or by specifying an alternate file location\nin the constructor.  The format of this file is described below:  [default]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken  [test-user]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken", 
            "title": "Java SDK"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#java-command-line-tools", 
            "text": "Version:  1.6.13.0\n       Installation:  brew install ec2-api-tools\n       Link:  Setting Up the Amazon EC2 Command Line Interface Tools on Linux/Unix and Mac OS X   The standard configuration is to use environment variables, since these tools have not been updated\nto read the standard AWS credentials file.  Add the following to your  ~/.bash_profile , to link the\nrequired data to the standard credentials file and allow for session tokens:  export AWS_CREDENTIAL_FILE= $HOME/.aws/credentials \nexport AWS_ACCESS_KEY= $(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2) \nexport AWS_SECRET_KEY= $(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2) \nexport AWS_DELEGATION_TOKEN= $(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)", 
            "title": "Java Command Line Tools"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#python-boto", 
            "text": "Version:  boto==2.31.1   botocore==0.56.0\n       Installation:  pip install boto\n       Link:  Boto Config   The latest version of boto needs to have  aws_security_token  defined, rather than aws_session_token , in the credentials file.  The simplest solution for this is to duplicate the\ntoken between both names; the Java SDK will throw the following log message when reading the extra\nproperty, but will work as expected:  INFO: Skip unsupported property name aws_security_token in\nprofile [default].   Boto will not throw log messages about the existence of the  aws_session_token \nproperty.", 
            "title": "Python Boto"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#aws-cli", 
            "text": "Version:  aws-cli/1.3.22\n       Installation:  pip install awscli\n       Link:  Configuring the AWS Command Line Interface   The standard location for the credentials file is  ~/.aws/config , which can be overridden with the AWS_CREDENTIAL_FILE  environment variable.  The latest version of this tool accepts the Java SDK\ncredential file format as-is, including the use of  aws_session_token , whereas previous versions\nwanted  aws_security_token  instead.  When you have multiple profiles in the credentials file, you\ncan select a profile with the tool like so:  aws --profile test-user s3 ls", 
            "title": "AWS CLI"
        }, 
        {
            "location": "/aws/aws-credential-files-for-java-and-python/#unified-solution", 
            "text": "The best approach for creating a unified credentials file is to follow the Java credentials file\nformat as closely as possible, while redirecting the Python tools to that file and adding properties\nto cover the corner cases.  To do this, create a  ~/.aws/credentials  file that duplicates the necessary properties:  [default]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken  aws_security_token = testSessionToken  [test-user]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken  aws_security_token = testSessionToken  [prod-user]  aws_access_key_id = testAccessKey  aws_secret_access_key = testSecretKey  aws_session_token = testSessionToken  aws_security_token = testSessionToken   And add a section to your  ~/.bash_profile :  export AWS_CREDENTIAL_FILE= $HOME/.aws/credentials \nexport AWS_ACCESS_KEY= $(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2) \nexport AWS_SECRET_KEY= $(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2) \nexport AWS_DELEGATION_TOKEN= $(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)   With this configuration, you should be able to move seamlessly between the various Java and Python\ntools available for AWS.", 
            "title": "Unified Solution"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/", 
            "text": "Build a Fake Instance Metadata Server for Ubuntu on Vagrant\n\n\n2014-07-22\n\n\nLet's say that you have a proper build/bake/deploy pipeline in place for running appications in AWS.\nThis is a reliable way to deploy applications at scale and move traffic between different versions\nof applications as a part of the deployment pipeline.  However, the whole process may take 20-40\nminutes or so to complete for any particular build.  If you want to iterate more rapidly on your\ndevelopment efforts, you could skip the full process with a quickpatch ssh/rsync deployment to a\nsingle server or you could stand up a local Vagrant base image and iterate on that.  Now let's say\nthat the application you are working on is intended to work with instance metadata, particularly for\nthe purpose of obtaining a rotating set of access and secret keys.  It might be nice to have a fake\nmetadata service running on your local Vagrant image so that you can test your application in a\nmanner similar to how it will be running in the cloud.  In this post, I describe how to build and\nconfigure a fake metadata service for an Ubuntu image running on Vagrant.\n\n\nPackage Layout\n\n\nThis layout assumes that you will be installing custom packages to the \n/apps\n directory, and there\nis a \ndaemontools\n service hierarchy located at \n/service\n.  The\n\nfake-metadata/service/run\n file is a script suitable for use with daemontools.\n\n\nroot/\n\u251c\u2500\u2500 apps\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 app.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 service\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 run\n\u2514\u2500\u2500 etc\n    \u251c\u2500\u2500 init.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u251c\u2500\u2500 logrotate.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u2514\u2500\u2500 network\n        \u2514\u2500\u2500 iptables.rules\n\n\n\n\n\nBuilding and Packaging\n\n\nFor cross-platform build packaging, it will be easiest to use the\n\nnebula ospackage\n plugin with Gradle.\nWith this plugin available, your build script will look something like this:\n\n\napply\n \nplugin:\n \nnebula-ospackage\n\n\n\nospackage\n \n{\n\n    \nversion\n=\n1.0\n\n    \npackageName\n=\nfake-metadata\n\n\n    \nrequires\n(\npython-flask\n)\n\n\n    \nlink\n(\n/apps/fake-metadata/logs\n,\n \n/mnt/logs/fake-metadata\n)\n\n    \nlink\n(\n/service/fake-metadata\n,\n \n/apps/fake-metadata/service\n)\n\n\n}\n\n\n\nbuildDeb\n \n{\n\n    \npostInstall\n \nfile\n(\nscripts/postInstall.sh\n)\n\n    \npreUninstall\n \nsvc -d /service/fake-metadata\n\n    \npostUninstall\n \nfile\n(\nscripts/postUninstall.sh\n)\n\n\n}\n\n\n\ntask\n \nbuild\n(\ndependsOn:\n \n[\nbuildDeb\n])\n\n\n\n\n\n\nFake Metadata Application\n\n\nThe simplest approach to building the service is to create a \nFlask\n\nservice and have it run bare on the Vagrant instance.  Given how small it will be and limited amount\nof traffic it will need to serve, there is no need to run this behind a dedicated static webserver\nlike nginx or Apache.  The nice thing about using Flask and having a basic structure in place is\nthat it is then easy to extend the application to add other endpoints when needed.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n#!/usr/bin/env python\n\n\n\nfrom\n \nflask\n \nimport\n \nFlask\n,\n \njsonify\n,\n \nabort\n,\n \nmake_response\n,\n \nrequest\n\n\nimport\n \nos\n\n\nimport\n \nsys\n\n\n\n\napp\n \n=\n \nFlask\n(\n__name__\n)\n\n\n\n\nBaseIAMRole\n \n=\n \n{\n\n  \nCode\n:\n \nSuccess\n,\n\n  \nLastUpdated\n \n:\n \n,\n\n  \nType\n:\n \nAWS-HMAC\n,\n\n  \nAccessKeyId\n:\n \n,\n\n  \nSecretAccessKey\n:\n \n,\n\n  \nToken\n:\n \n,\n\n  \nExpiration\n:\n \n\n\n}\n\n\n\n\n@app.route\n(\n/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nindex\n():\n\n    \nreturn\n \nlatest\n\n\n\n\n@app.route\n(\n/latest/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nlatest\n():\n\n    \nreturn\n \nmeta-data\n\n\n\n\n@app.route\n(\n/latest/meta-data/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nmeta_data\n():\n\n    \nendpoints\n \n=\n \n[\n\n        \niam\n,\n\n        \npublic-hostname\n,\n\n        \npublic-ipv4\n\n    \n]\n\n    \nreturn\n \n(\n\\n\n)\n.\njoin\n(\nendpoints\n)\n\n\n\n\n@app.route\n(\n/latest/meta-data/iam/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \niam\n():\n\n    \nreturn\n \nsecurity-credentials\n\n\n\n\n@app.route\n(\n/latest/meta-data/iam/security-credentials/\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nsecurity_credentials\n():\n\n    \nreturn\n \nBaseIAMRole\n\n\n\n\n@app.route\n(\n/latest/meta-data/iam/security-credentials/BaseIAMRole\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \nbase_iam_role\n():\n\n    \n# update the payload to contain a current set of accesss and secrey keys\n\n    \nreturn\n \njsonify\n(\nBaseIAMRole\n)\n\n\n\n\n@app.route\n(\n/latest/meta-data/public-hostname\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \npublic_hostname\n():\n\n    \nreturn\n \nos\n.\nenviron\n[\nEC2_LOCAL_HOSTNAME\n]\n\n\n\n\n@app.route\n(\n/latest/meta-data/public-ipv4\n,\n \nmethods\n \n=\n \n[\nGET\n])\n\n\ndef\n \npublic_ipv4\n():\n\n    \nreturn\n \nos\n.\nenviron\n[\nEC2_LOCAL_IPV4\n]\n\n\n\n\n@app.errorhandler\n(\n400\n)\n\n\ndef\n \nnot_found\n(\nerror\n):\n\n    \nreturn\n \nmake_response\n(\njsonify\n(\n \n{\n \nerror\n:\n \nbad request\n \n}\n \n),\n \n400\n)\n\n\n\n\n@app.errorhandler\n(\n404\n)\n\n\ndef\n \nnot_found\n(\nerror\n):\n\n    \nreturn\n \nmake_response\n(\njsonify\n(\n \n{\n \nerror\n:\n \nnot found\n \n}\n \n),\n \n404\n)\n\n\n\n\n@app.errorhandler\n(\n500\n)\n\n\ndef\n \nnot_found\n(\nerror\n):\n\n    \nreturn\n \nmake_response\n(\njsonify\n(\n \n{\n \nerror\n:\n \ninternal server error\n \n}\n \n),\n \n500\n)\n\n\n\n\nif\n \n__name__\n \n==\n \n__main__\n:\n\n    \nif\n \nlen\n(\nsys\n.\nargv\n)\n \n \n1\n:\n\n        \nif\n \n:\n \nin\n \nsys\n.\nargv\n[\n1\n]:\n\n            \nhost\n=\nsys\n.\nargv\n[\n1\n]\n.\nsplit\n(\n:\n)[\n0\n]\n\n            \nport\n=\nint\n(\nsys\n.\nargv\n[\n1\n]\n.\nsplit\n(\n:\n)[\n1\n])\n\n            \napp\n.\nrun\n(\nhost\n=\nhost\n,\n \nport\n=\nport\n)\n\n        \nelse\n:\n\n            \napp\n.\nrun\n(\nhost\n=\nsys\n.\nargv\n[\n1\n])\n\n    \nelse\n:\n\n        \napp\n.\nrun\n(\ndebug\n=\nTrue\n)\n\n\n\n\n\n\n\nDaemontools Run Script\n\n\nThis script is watched by the supervise process, which then starts (or restarts) the application if\nit is not running.  Switching to a non-root user and redirecting output to the log file occurs here.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n#!/bin/bash\n\n\n\nulimit\n -n \n32768\n\n\n\nsource\n /etc/profile.d/environment.sh\n\n\nexport\n \nPATH\n=\n/command:/usr/local/bin:/usr/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/scripts\n\n\nif\n \n[\n ! -d \n/mnt/logs/fake-metadata\n \n]\n;\n \nthen\n\n  mkdir -p /mnt/logs/fake-metadata\n  chmod \n0777\n /mnt/logs/fake-metadata\n\nfi\n\n\n\nUSER\n=\nsomeuser\n\nPYTHON\n=\n/usr/bin/python\n\nAPP\n=\n/apps/fake-metadata/app.py\n\nOPTS\n=\n127\n.0.0.1:8000\n\nLOG\n=\n/mnt/logs/fake-metadata/server.log\n\n\necho\n \nstarting fake-metadata\n\n\nexec\n setuidgid \n$USER\n \n$PYTHON\n \n$APP\n \n$OPTS\n \n \n$LOG\n \n2\n1\n\n\n\n\n\n\n\nPostInstall Script\n\n\nThis is where most of the trickiness occurs.\n\n\nThe post install script is responsible for modifying the \n/etc/network/interfaces\n file, adding the\nmetadata server IP address and configuring iptables in an idempotent manner.  When the post install\nscript is packaged by the nebula ospackage plugin into a Debian package, it gets a \n#!/bin/sh -e\n\nshebang invocation, which means that the script will halt execution at any point where it evaluates\na non-zero return code.  This means that the script needs to be written such a way that the\nenvironment state testing being done always returns a true value so that the script does not fail,\nhence the \n||true\n constructs.\n\n\nWe are attaching an extra IP address to the loopback interface, so we need to redirect traffic from\n169.254.169.254:80 to the location where the fake metadata server is running.  We are dealing with\nthe loopback interface, which means that the PREROUTING nat table is never hit and we must use the\nOUTPUT table instead.  You cannot DNAT packets destined for the loopback interface, because the\nkernel will treat them as martians and drop them, so you must REDIRECT the packets to the desired\nport.  When performing the redirection from port 80 to 8000 on the loopback interface, it sends the\npackets to 127.0.0.1:8000, not 169.254.169.254:8000, so the fake metadata server must be listening\non localhost port 8000.\n\n\nIf for some reason, you need to troubleshoot the post install script, it can be found at\n\n/var/lib/dpkg/info/fake-metadata.postinst\n following an attempted package installation.\n\n\nLINE\n=\n$(\n grep \n169\n.254.169.254/32 /etc/network/interfaces \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n ! \n$LINE\n \n==\n *169.254.169.254/32*  \n]]\n;\n \nthen\n\n    sed -i \n/iface lo inet loopback/a up ip addr add 169.254.169.254/32 dev lo scope host\n /etc/network/interfaces\n\nfi\n\n\n\nLINE\n=\n$(\n /sbin/ip addr \n|\ngrep \n169\n.254.169.254/32 \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n ! \n$LINE\n \n==\n *169.254.169.254/32*  \n]]\n;\n \nthen\n\n    /sbin/ip addr add \n169\n.254.169.254/32 dev lo scope host\n\nfi\n\n\n\nLINE\n=\n$(\n grep iptables-restore /etc/network/interfaces \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n ! \n$LINE\n \n==\n *iptables-restore*  \n]]\n;\n \nthen\n\n    sed -i \n/up ip addr add/a pre-up iptables-restore \n /etc/network/iptables.rules\n /etc/network/interfaces\n\nfi\n\n\n\nLINE\n=\n$(\n iptables -t nat -L \n|\ngrep \n8000\n \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n ! \n$LINE\n \n==\n *8000*  \n]]\n;\n \nthen\n\n    iptables -t nat -A OUTPUT -p tcp -d \n169\n.254.169.254/32 --dport \n80\n -j REDIRECT --to-ports \n8000\n\n\nfi\n\n\n/usr/sbin/update-rc.d fake-metadata defaults\n\n\n\n\n\nPostUninstall Script\n\n\nThis script is the inverse of the post install script; it returns the system to its previous state.\n\n\nLINE\n=\n$(\n grep \n169\n.254.169.254/32 /etc/network/interfaces \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n \n$LINE\n \n==\n *169.254.169.254/32*  \n]]\n;\n \nthen\n\n    sed -i \n/up ip addr add 169.254.169.254\\/32 dev lo scope host/d\n /etc/network/interfaces\n\nfi\n\n\n\nLINE\n=\n$(\n /sbin/ip addr \n|\ngrep \n169\n.254.169.254/32 \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n \n$LINE\n \n==\n *169.254.169.254/32*  \n]]\n;\n \nthen\n\n    /sbin/ip addr delete \n169\n.254.169.254/32 dev lo scope host\n\nfi\n\n\n\nLINE\n=\n$(\n grep iptables-restore /etc/network/interfaces \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n \n$LINE\n \n==\n *iptables-restore*  \n]]\n;\n \nthen\n\n    sed -i \n/pre-up iptables-restore \n \\/etc\\/network\\/iptables.rules/d\n /etc/network/interfaces\n\nfi\n\n\n\nLINE\n=\n$(\n iptables -t nat -L \n|\ngrep \n8000\n \n||\n \ntrue\n \n)\n\n\nif\n \n[[\n \n$LINE\n \n==\n *8000*  \n]]\n;\n \nthen\n\n    iptables -t nat -D OUTPUT -p tcp -d \n169\n.254.169.254/32 --dport \n80\n -j REDIRECT --to-ports \n8000\n\n\nfi\n\n\n/usr/sbin/update-rc.d -f fake-metadata remove\nrm -rf /apps/fake-metadata\npkill -f \nsupervise fake-metadata\n\n\n\n\n\n\nLog Rotation\n\n\nTo keep the local Vagrant instance clean, it is useful to configure log rotation.  Sending a HUP\nsignal to the service allows it to continue writing to the new logfile.\n\n\n/mnt/logs/fake-metadata/server.log \n{\n\n    daily\n    rotate \n7\n\n    compress\n    missingok\n    notifempty\n    create \n644\n root root\n    postrotate\n        svc -h /service/fake-metadata\n    endscript\n\n}", 
            "title": "Build a Fake Instance Metadata Server for Ubuntu on Vagrant"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant", 
            "text": "2014-07-22  Let's say that you have a proper build/bake/deploy pipeline in place for running appications in AWS.\nThis is a reliable way to deploy applications at scale and move traffic between different versions\nof applications as a part of the deployment pipeline.  However, the whole process may take 20-40\nminutes or so to complete for any particular build.  If you want to iterate more rapidly on your\ndevelopment efforts, you could skip the full process with a quickpatch ssh/rsync deployment to a\nsingle server or you could stand up a local Vagrant base image and iterate on that.  Now let's say\nthat the application you are working on is intended to work with instance metadata, particularly for\nthe purpose of obtaining a rotating set of access and secret keys.  It might be nice to have a fake\nmetadata service running on your local Vagrant image so that you can test your application in a\nmanner similar to how it will be running in the cloud.  In this post, I describe how to build and\nconfigure a fake metadata service for an Ubuntu image running on Vagrant.", 
            "title": "Build a Fake Instance Metadata Server for Ubuntu on Vagrant"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#package-layout", 
            "text": "This layout assumes that you will be installing custom packages to the  /apps  directory, and there\nis a  daemontools  service hierarchy located at  /service .  The fake-metadata/service/run  file is a script suitable for use with daemontools.  root/\n\u251c\u2500\u2500 apps\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 app.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 service\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 run\n\u2514\u2500\u2500 etc\n    \u251c\u2500\u2500 init.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u251c\u2500\u2500 logrotate.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u2514\u2500\u2500 network\n        \u2514\u2500\u2500 iptables.rules", 
            "title": "Package Layout"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#building-and-packaging", 
            "text": "For cross-platform build packaging, it will be easiest to use the nebula ospackage  plugin with Gradle.\nWith this plugin available, your build script will look something like this:  apply   plugin:   nebula-ospackage  ospackage   { \n     version = 1.0 \n     packageName = fake-metadata \n\n     requires ( python-flask ) \n\n     link ( /apps/fake-metadata/logs ,   /mnt/logs/fake-metadata ) \n     link ( /service/fake-metadata ,   /apps/fake-metadata/service )  }  buildDeb   { \n     postInstall   file ( scripts/postInstall.sh ) \n     preUninstall   svc -d /service/fake-metadata \n     postUninstall   file ( scripts/postUninstall.sh )  }  task   build ( dependsOn:   [ buildDeb ])", 
            "title": "Building and Packaging"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#fake-metadata-application", 
            "text": "The simplest approach to building the service is to create a  Flask \nservice and have it run bare on the Vagrant instance.  Given how small it will be and limited amount\nof traffic it will need to serve, there is no need to run this behind a dedicated static webserver\nlike nginx or Apache.  The nice thing about using Flask and having a basic structure in place is\nthat it is then easy to extend the application to add other endpoints when needed.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92 #!/usr/bin/env python  from   flask   import   Flask ,   jsonify ,   abort ,   make_response ,   request  import   os  import   sys  app   =   Flask ( __name__ )  BaseIAMRole   =   { \n   Code :   Success , \n   LastUpdated   :   , \n   Type :   AWS-HMAC , \n   AccessKeyId :   , \n   SecretAccessKey :   , \n   Token :   , \n   Expiration :    }  @app.route ( / ,   methods   =   [ GET ])  def   index (): \n     return   latest  @app.route ( /latest/ ,   methods   =   [ GET ])  def   latest (): \n     return   meta-data  @app.route ( /latest/meta-data/ ,   methods   =   [ GET ])  def   meta_data (): \n     endpoints   =   [ \n         iam , \n         public-hostname , \n         public-ipv4 \n     ] \n     return   ( \\n ) . join ( endpoints )  @app.route ( /latest/meta-data/iam/ ,   methods   =   [ GET ])  def   iam (): \n     return   security-credentials  @app.route ( /latest/meta-data/iam/security-credentials/ ,   methods   =   [ GET ])  def   security_credentials (): \n     return   BaseIAMRole  @app.route ( /latest/meta-data/iam/security-credentials/BaseIAMRole ,   methods   =   [ GET ])  def   base_iam_role (): \n     # update the payload to contain a current set of accesss and secrey keys \n     return   jsonify ( BaseIAMRole )  @app.route ( /latest/meta-data/public-hostname ,   methods   =   [ GET ])  def   public_hostname (): \n     return   os . environ [ EC2_LOCAL_HOSTNAME ]  @app.route ( /latest/meta-data/public-ipv4 ,   methods   =   [ GET ])  def   public_ipv4 (): \n     return   os . environ [ EC2_LOCAL_IPV4 ]  @app.errorhandler ( 400 )  def   not_found ( error ): \n     return   make_response ( jsonify (   {   error :   bad request   }   ),   400 )  @app.errorhandler ( 404 )  def   not_found ( error ): \n     return   make_response ( jsonify (   {   error :   not found   }   ),   404 )  @app.errorhandler ( 500 )  def   not_found ( error ): \n     return   make_response ( jsonify (   {   error :   internal server error   }   ),   500 )  if   __name__   ==   __main__ : \n     if   len ( sys . argv )     1 : \n         if   :   in   sys . argv [ 1 ]: \n             host = sys . argv [ 1 ] . split ( : )[ 0 ] \n             port = int ( sys . argv [ 1 ] . split ( : )[ 1 ]) \n             app . run ( host = host ,   port = port ) \n         else : \n             app . run ( host = sys . argv [ 1 ]) \n     else : \n         app . run ( debug = True )", 
            "title": "Fake Metadata Application"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#daemontools-run-script", 
            "text": "This script is watched by the supervise process, which then starts (or restarts) the application if\nit is not running.  Switching to a non-root user and redirecting output to the log file occurs here.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 #!/bin/bash  ulimit  -n  32768  source  /etc/profile.d/environment.sh export   PATH = /command:/usr/local/bin:/usr/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/scripts if   [  ! -d  /mnt/logs/fake-metadata   ] ;   then \n  mkdir -p /mnt/logs/fake-metadata\n  chmod  0777  /mnt/logs/fake-metadata fi  USER = someuser PYTHON = /usr/bin/python APP = /apps/fake-metadata/app.py OPTS = 127 .0.0.1:8000 LOG = /mnt/logs/fake-metadata/server.log echo   starting fake-metadata  exec  setuidgid  $USER   $PYTHON   $APP   $OPTS     $LOG   2 1", 
            "title": "Daemontools Run Script"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#postinstall-script", 
            "text": "This is where most of the trickiness occurs.  The post install script is responsible for modifying the  /etc/network/interfaces  file, adding the\nmetadata server IP address and configuring iptables in an idempotent manner.  When the post install\nscript is packaged by the nebula ospackage plugin into a Debian package, it gets a  #!/bin/sh -e \nshebang invocation, which means that the script will halt execution at any point where it evaluates\na non-zero return code.  This means that the script needs to be written such a way that the\nenvironment state testing being done always returns a true value so that the script does not fail,\nhence the  ||true  constructs.  We are attaching an extra IP address to the loopback interface, so we need to redirect traffic from\n169.254.169.254:80 to the location where the fake metadata server is running.  We are dealing with\nthe loopback interface, which means that the PREROUTING nat table is never hit and we must use the\nOUTPUT table instead.  You cannot DNAT packets destined for the loopback interface, because the\nkernel will treat them as martians and drop them, so you must REDIRECT the packets to the desired\nport.  When performing the redirection from port 80 to 8000 on the loopback interface, it sends the\npackets to 127.0.0.1:8000, not 169.254.169.254:8000, so the fake metadata server must be listening\non localhost port 8000.  If for some reason, you need to troubleshoot the post install script, it can be found at /var/lib/dpkg/info/fake-metadata.postinst  following an attempted package installation.  LINE = $(  grep  169 .254.169.254/32 /etc/network/interfaces  ||   true   )  if   [[  !  $LINE   ==  *169.254.169.254/32*   ]] ;   then \n    sed -i  /iface lo inet loopback/a up ip addr add 169.254.169.254/32 dev lo scope host  /etc/network/interfaces fi  LINE = $(  /sbin/ip addr  | grep  169 .254.169.254/32  ||   true   )  if   [[  !  $LINE   ==  *169.254.169.254/32*   ]] ;   then \n    /sbin/ip addr add  169 .254.169.254/32 dev lo scope host fi  LINE = $(  grep iptables-restore /etc/network/interfaces  ||   true   )  if   [[  !  $LINE   ==  *iptables-restore*   ]] ;   then \n    sed -i  /up ip addr add/a pre-up iptables-restore   /etc/network/iptables.rules  /etc/network/interfaces fi  LINE = $(  iptables -t nat -L  | grep  8000   ||   true   )  if   [[  !  $LINE   ==  *8000*   ]] ;   then \n    iptables -t nat -A OUTPUT -p tcp -d  169 .254.169.254/32 --dport  80  -j REDIRECT --to-ports  8000  fi \n\n/usr/sbin/update-rc.d fake-metadata defaults", 
            "title": "PostInstall Script"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#postuninstall-script", 
            "text": "This script is the inverse of the post install script; it returns the system to its previous state.  LINE = $(  grep  169 .254.169.254/32 /etc/network/interfaces  ||   true   )  if   [[   $LINE   ==  *169.254.169.254/32*   ]] ;   then \n    sed -i  /up ip addr add 169.254.169.254\\/32 dev lo scope host/d  /etc/network/interfaces fi  LINE = $(  /sbin/ip addr  | grep  169 .254.169.254/32  ||   true   )  if   [[   $LINE   ==  *169.254.169.254/32*   ]] ;   then \n    /sbin/ip addr delete  169 .254.169.254/32 dev lo scope host fi  LINE = $(  grep iptables-restore /etc/network/interfaces  ||   true   )  if   [[   $LINE   ==  *iptables-restore*   ]] ;   then \n    sed -i  /pre-up iptables-restore   \\/etc\\/network\\/iptables.rules/d  /etc/network/interfaces fi  LINE = $(  iptables -t nat -L  | grep  8000   ||   true   )  if   [[   $LINE   ==  *8000*   ]] ;   then \n    iptables -t nat -D OUTPUT -p tcp -d  169 .254.169.254/32 --dport  80  -j REDIRECT --to-ports  8000  fi \n\n/usr/sbin/update-rc.d -f fake-metadata remove\nrm -rf /apps/fake-metadata\npkill -f  supervise fake-metadata", 
            "title": "PostUninstall Script"
        }, 
        {
            "location": "/aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#log-rotation", 
            "text": "To keep the local Vagrant instance clean, it is useful to configure log rotation.  Sending a HUP\nsignal to the service allows it to continue writing to the new logfile.  /mnt/logs/fake-metadata/server.log  { \n    daily\n    rotate  7 \n    compress\n    missingok\n    notifempty\n    create  644  root root\n    postrotate\n        svc -h /service/fake-metadata\n    endscript }", 
            "title": "Log Rotation"
        }, 
        {
            "location": "/aws/configuring-multiple-interfaces-on-the-same-network-in-ec2/", 
            "text": "Configuring Multiple Interfaces on the Same Network in EC2\n\n\n2014-07-09\n\n\nI've been reading a bit lately on Linux policy and source-based routing for the purpose of\nconfiguring multiple NICs on the same network in EC2.  I found the following links helpful:\n\n\n\n\nMultiple IPs and ENIs on EC2 in a VPC\n\n\nRouting for Multiple Uplinks/Providers\n\n\nA Quick Introduction to Linux Policy Routing", 
            "title": "Configuring Multiple Interfaces on the Same Network in EC2"
        }, 
        {
            "location": "/aws/configuring-multiple-interfaces-on-the-same-network-in-ec2/#configuring-multiple-interfaces-on-the-same-network-in-ec2", 
            "text": "2014-07-09  I've been reading a bit lately on Linux policy and source-based routing for the purpose of\nconfiguring multiple NICs on the same network in EC2.  I found the following links helpful:   Multiple IPs and ENIs on EC2 in a VPC  Routing for Multiple Uplinks/Providers  A Quick Introduction to Linux Policy Routing", 
            "title": "Configuring Multiple Interfaces on the Same Network in EC2"
        }, 
        {
            "location": "/github/how-i-built-my-blog/", 
            "text": "How I Built My Blog\n\n\n2014-06-28\n\n\nI have settled on \nGitHub Pages\n and\n\nJekyll Bootstrap\n as the fastest way to get started with blogging in\na way that gives me some reasonable control over the formatting and design process.  There are\nseveral things that I like about this combination:\n\n\n\n\nGitHub handles the source, backups, hosting and scaling\n\n\nJekyll adds the structure to make it viable as a blog (and serves locally for prototyping!)\n\n\nMarkdown is fast and easy to write for formatting purposes\n\n\n\n\nI am pleased with how easy it is to get started:\n\n\ngit clone https://github.com/plusjade/jekyll-bootstrap.git USERNAME.github.com\n\ncd\n USERNAME.github.com\ngit remote set-url origin git@github.com:USERNAME/USERNAME.github.com.git\ngit push origin master\n\n\n\n\n\nI had originally started configuring my GitHub Pages repo a few months ago and the code was slightly\nout of sync with the JB repo.  I want to be able to pickup changes from the JB upstream master, so I\nconfigured that:\n\n\ngit remote add upstream git@github.com:plusjade/jekyll-bootstrap.git\ngit pull upstream master\ngit push origin\n\n\n\n\n\nIn the process of configuring my blog, I found the following links helpful:\n\n\n\n\nHow I Built My Blog in One Day\n by Eric Jones\n\n\nLet Me Introduce My Blog\n by Malchio\n\n\nJekyll Bootstrap Theme Browser\n\n\n\n\nEric's blog has several tricks for integrating with various other web services like social sites and\nwhat not that seems like it would be useful for increasing engagement with posts.\n\n\nConfiguration Details\n\n\nI like the modern fonts and white-on-black look of the Hooligan theme, so I added that:\n\n\nrake theme:install \ngit\n=\nhttps://github.com/dhulihan/hooligan.git\n\n\n\n\n\n\nI fixed a deprecation warning in \n_config.yml\n, changing \npygments: true\n to \nhighlighter: true\n.\n\n\nThe new method of using GitHub Pages \nlocks the version of Jekyll\n\nthrough a \nGemfile\n specified like so:\n\n\nsource\n \nhttps://rubygems.org\n\n\ngem\n \ngithub-pages\n\n\n\n\n\n\nThis is important, since they are using a fairly old version of Jekyll (1.5.1 versus 2.1.0) and a\nfew other gems.  From here, you install the github-pages gem which brings in the dependencies:\n\n\ngem install bundler\nbundle install\n\n\n\n\n\nFollowing these steps will result in downloading the \nspecific gem versions\n\nused for GitHub pages.  This will make it a lot easier to figure out why pages fail to compile\n(email notifications are sent for these failures).  In my case, it turns out that GitHub Pages was\ndefaulting to using the \nmaruku\n markdown interpreter, which is now end of life, and which was also\nfailing to compile my markdown.  The solution is to switch to the newer \nkramdown\n markdown\ninterpreter by adding the following to \n_config.yml\n, noting that this field is not part of the\nlatest JB configuration:\n\n\nmarkdown\n:\n \nkramdown\n\n\n\n\n\n\nBlog Post Development Workflow\n\n\nThe basic post development workflow looks like this:\n\n\n# start serving the site locally\n\nbundle \nexec\n jekyll serve\n\n\n# start a new post\n\nrake post \ntitle\n=\nHello World\n\n\n\n# write the post\n\n\n\n# rebuild the site\n\nbundle \nexec\n jekyll build\n\n\n# review post at http://localhost:4000\n\n\n\n\n\n\nTyping \njekyll build\n frequently becomes old fast, so I added \nguard-jekyll\n\nto the bundle and ran it like so:\n\n\nbundle install guard-jekyll\nbundle \nexec\n guard init jekyll\nbundle \nexec\n guard\n\n\n\n\n\nThis will monitor the directory for file changes and automatically rebuild the site, so I can stay\nfocused in Sublime Text and refresh the page in the browser.", 
            "title": "How I Built My Blog"
        }, 
        {
            "location": "/github/how-i-built-my-blog/#how-i-built-my-blog", 
            "text": "2014-06-28  I have settled on  GitHub Pages  and Jekyll Bootstrap  as the fastest way to get started with blogging in\na way that gives me some reasonable control over the formatting and design process.  There are\nseveral things that I like about this combination:   GitHub handles the source, backups, hosting and scaling  Jekyll adds the structure to make it viable as a blog (and serves locally for prototyping!)  Markdown is fast and easy to write for formatting purposes   I am pleased with how easy it is to get started:  git clone https://github.com/plusjade/jekyll-bootstrap.git USERNAME.github.com cd  USERNAME.github.com\ngit remote set-url origin git@github.com:USERNAME/USERNAME.github.com.git\ngit push origin master  I had originally started configuring my GitHub Pages repo a few months ago and the code was slightly\nout of sync with the JB repo.  I want to be able to pickup changes from the JB upstream master, so I\nconfigured that:  git remote add upstream git@github.com:plusjade/jekyll-bootstrap.git\ngit pull upstream master\ngit push origin  In the process of configuring my blog, I found the following links helpful:   How I Built My Blog in One Day  by Eric Jones  Let Me Introduce My Blog  by Malchio  Jekyll Bootstrap Theme Browser   Eric's blog has several tricks for integrating with various other web services like social sites and\nwhat not that seems like it would be useful for increasing engagement with posts.", 
            "title": "How I Built My Blog"
        }, 
        {
            "location": "/github/how-i-built-my-blog/#configuration-details", 
            "text": "I like the modern fonts and white-on-black look of the Hooligan theme, so I added that:  rake theme:install  git = https://github.com/dhulihan/hooligan.git   I fixed a deprecation warning in  _config.yml , changing  pygments: true  to  highlighter: true .  The new method of using GitHub Pages  locks the version of Jekyll \nthrough a  Gemfile  specified like so:  source   https://rubygems.org  gem   github-pages   This is important, since they are using a fairly old version of Jekyll (1.5.1 versus 2.1.0) and a\nfew other gems.  From here, you install the github-pages gem which brings in the dependencies:  gem install bundler\nbundle install  Following these steps will result in downloading the  specific gem versions \nused for GitHub pages.  This will make it a lot easier to figure out why pages fail to compile\n(email notifications are sent for these failures).  In my case, it turns out that GitHub Pages was\ndefaulting to using the  maruku  markdown interpreter, which is now end of life, and which was also\nfailing to compile my markdown.  The solution is to switch to the newer  kramdown  markdown\ninterpreter by adding the following to  _config.yml , noting that this field is not part of the\nlatest JB configuration:  markdown :   kramdown", 
            "title": "Configuration Details"
        }, 
        {
            "location": "/github/how-i-built-my-blog/#blog-post-development-workflow", 
            "text": "The basic post development workflow looks like this:  # start serving the site locally \nbundle  exec  jekyll serve # start a new post \nrake post  title = Hello World  # write the post  # rebuild the site \nbundle  exec  jekyll build # review post at http://localhost:4000   Typing  jekyll build  frequently becomes old fast, so I added  guard-jekyll \nto the bundle and ran it like so:  bundle install guard-jekyll\nbundle  exec  guard init jekyll\nbundle  exec  guard  This will monitor the directory for file changes and automatically rebuild the site, so I can stay\nfocused in Sublime Text and refresh the page in the browser.", 
            "title": "Blog Post Development Workflow"
        }, 
        {
            "location": "/linux/linux-ftrace-delivers-dtrace-like-functionality/", 
            "text": "Linux ftrace Delivers dtrace-like Functionality\n\n\n2014-07-22\n\n\nBrendan Gregg\n recently released\n\nperf-tools\n, which is a collection of low-level\nperformance observability scripts akin to the \nDTraceToolkit\n.\n\n\nHe has a \nnew article\n posted on the tools at LWN.", 
            "title": "Linux ftrace Delivers dtrace-like Functionality"
        }, 
        {
            "location": "/linux/linux-ftrace-delivers-dtrace-like-functionality/#linux-ftrace-delivers-dtrace-like-functionality", 
            "text": "2014-07-22  Brendan Gregg  recently released perf-tools , which is a collection of low-level\nperformance observability scripts akin to the  DTraceToolkit .  He has a  new article  posted on the tools at LWN.", 
            "title": "Linux ftrace Delivers dtrace-like Functionality"
        }, 
        {
            "location": "/python/building-rest-apis-with-python-flask/", 
            "text": "Building REST APIs with Python Flask\n\n\n2014-07-15\n\n\nMiguel Grinberg\n, author of\n\nFlask Web Development\n put together an excellent blog post on\n\nDesigning a RESTful API with Python and Flask\n.", 
            "title": "Building REST APIs with Python Flask"
        }, 
        {
            "location": "/python/building-rest-apis-with-python-flask/#building-rest-apis-with-python-flask", 
            "text": "2014-07-15  Miguel Grinberg , author of Flask Web Development  put together an excellent blog post on Designing a RESTful API with Python and Flask .", 
            "title": "Building REST APIs with Python Flask"
        }, 
        {
            "location": "/python/stop-using-print-for-debugging/", 
            "text": "Stop Using \"print\" for Debugging\n\n\n2017-10-12\n\n\nMy favorite quickstart guide to the Python logging module, by \nAl Sweigart\n:\n\n\nStop Using \"print\" for Debugging: A 5 Minute Quickstart Guide to Python\u2019s logging Module\n\n\nimport\n \nlogging\n\n\nlogging\n.\nbasicConfig\n(\nlevel\n=\nlogging\n.\nDEBUG\n,\n \nformat\n=\n%(asctime)s\n - \n%(levelname)s\n - \n%(message)s\n)\n\n\nlogging\n.\ndebug\n(\nThis is a log message.\n)", 
            "title": "Stop Using \"print\" for Debugging"
        }, 
        {
            "location": "/python/stop-using-print-for-debugging/#stop-using-print-for-debugging", 
            "text": "2017-10-12  My favorite quickstart guide to the Python logging module, by  Al Sweigart :  Stop Using \"print\" for Debugging: A 5 Minute Quickstart Guide to Python\u2019s logging Module  import   logging  logging . basicConfig ( level = logging . DEBUG ,   format = %(asctime)s  -  %(levelname)s  -  %(message)s )  logging . debug ( This is a log message. )", 
            "title": "Stop Using \"print\" for Debugging"
        }, 
        {
            "location": "/scala/starting-scala-repls-with-gradle-and-sbt/", 
            "text": "Starting Scala REPLs with Gradle and SBT\n\n\n2015-07-04\n\n\nThese examples are tailored for usage with the Atomic Scala exercises, but they demonstrate the\nprinciples.\n\n\nbuild.gradle\n\n\napply\n \nplugin:\n \nscala\n\n\n\nrepositories\n \n{\n\n    \nmavenCentral\n()\n\n\n}\n\n\n\next\n \n{\n\n    \nversions\n \n=\n \n[\n\n        \ncommons_math3:\n \n3.5\n,\n\n        \njline:\n \n2.12.1\n,\n\n        \nscala:\n \n2.11.6\n\n    \n]\n\n\n}\n\n\n\ndependencies\n \n{\n\n    \ncompile\n \norg.apache.commons:commons-math3:${versions.commons_math3}\n\n    \ncompile\n \norg.scala-lang:scala-library:${versions.scala}\n\n\n    \nruntime\n \njline:jline:${versions.jline}\n\n    \nruntime\n \norg.scala-lang:scala-compiler:${versions.scala}\n\n\n}\n\n\n\nsourceSets\n \n{\n\n    \nmain\n \n{\n\n        \nscala\n \n{\n\n            \nsrcDirs\n \n=\n \n[\n.\n]\n\n            \ninclude\n \nAtomicTest.scala\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\nscalaConsole\n.\ndependsOn\n(\nbuild\n)\n\n\nscalaConsole\n.\nclasspath\n \n+=\n \nsourceSets\n.\nmain\n.\nruntimeClasspath\n\n\n\n// usage: gradlew scalaConsole -q\n\n\n\n\n\n\nbuild.sbt\n\n\nscalaVersion\n \n:=\n \n2.11.6\n\n\n\nsources\n \nin\n \nCompile\n \n=\n \n(\nsources\n \nin\n \nCompile\n).\nmap\n(\n_\n \nfilter\n(\n_\n.\nname\n \n==\n \nAtomicTest.scala\n))\n\n\n\nlibraryDependencies\n \n+=\n \norg.apache.commons\n \n%\n \ncommons-math3\n \n%\n \n3.5\n\n\n\ninitialCommands\n \nin\n \nconsole\n \n:=\n \n\n\n    |import org.apache.commons.math3._\n\n\n    |import com.atomicscala.AtomicTest._\n\n\n    |\n.\nstripMargin\n\n\n\n// usage: sbt console", 
            "title": "Starting Scala REPLs with Gradle and SBT"
        }, 
        {
            "location": "/scala/starting-scala-repls-with-gradle-and-sbt/#starting-scala-repls-with-gradle-and-sbt", 
            "text": "2015-07-04  These examples are tailored for usage with the Atomic Scala exercises, but they demonstrate the\nprinciples.  build.gradle  apply   plugin:   scala  repositories   { \n     mavenCentral ()  }  ext   { \n     versions   =   [ \n         commons_math3:   3.5 , \n         jline:   2.12.1 , \n         scala:   2.11.6 \n     ]  }  dependencies   { \n     compile   org.apache.commons:commons-math3:${versions.commons_math3} \n     compile   org.scala-lang:scala-library:${versions.scala} \n\n     runtime   jline:jline:${versions.jline} \n     runtime   org.scala-lang:scala-compiler:${versions.scala}  }  sourceSets   { \n     main   { \n         scala   { \n             srcDirs   =   [ . ] \n             include   AtomicTest.scala \n         } \n     }  }  scalaConsole . dependsOn ( build )  scalaConsole . classpath   +=   sourceSets . main . runtimeClasspath  // usage: gradlew scalaConsole -q   build.sbt  scalaVersion   :=   2.11.6  sources   in   Compile   =   ( sources   in   Compile ). map ( _   filter ( _ . name   ==   AtomicTest.scala ))  libraryDependencies   +=   org.apache.commons   %   commons-math3   %   3.5  initialCommands   in   console   :=        |import org.apache.commons.math3._      |import com.atomicscala.AtomicTest._      | . stripMargin  // usage: sbt console", 
            "title": "Starting Scala REPLs with Gradle and SBT"
        }, 
        {
            "location": "/scala/using-the-scala-repl-to-configure-amazon-ses-notifications/", 
            "text": "Using the Scala REPL to Configure Amazon SES Notifications\n\n\n2014-06-30\n\n\nLet's say you want to configure bounce notifications for SES emails using the Scala REPL.  How do\nyou go about accessing the AWS API to make it happen?  Make sure to have the\n\nAWS SDK for Java API Reference\n available\nfor additional details.\n\n\nInstall Scala and SBT:\n\n\nbrew install scala sbt\n\n\n\n\n\nCreate an AWS credentials file at \n~/.aws/credentials\n that is formatted like so:\n\n\n[default]\n\n\naws_access_key_id\n=\n...\n\n\naws_secret_access_key\n=\n...\n\n\naws_session_token\n=\n...\n\n\n\n\n\n\nCreate a \nbuild.sbt\n file that looks like:\n\n\nname\n \n:=\n \naws-sdk-client\n\n\n\nversion\n \n:=\n \n1.0\n\n\n\nscalaVersion\n \n:=\n \n2.11.0\n\n\n\nlibraryDependencies\n \n++=\n \nSeq\n(\n\n    \ncommons-logging\n \n%\n \ncommons-logging\n \n%\n \n1.1.3\n,\n\n    \ncom.amazonaws\n \n%\n \naws-java-sdk\n \n%\n \n1.8.2\n\n\n)\n\n\n\n\n\n\nYou can then engage the Scala REPL to talk to the AWS API like so (Scala will download the AWS SDK\nfrom Maven Central automatically):\n\n\nsbt\nconsole\n\n\n\n\n\nimport\n \ncom.amazonaws.auth._\n\n\nimport\n \ncom.amazonaws.auth.profile._\n\n\nimport\n \ncom.amazonaws.services.simpleemail._\n\n\nimport\n \ncom.amazonaws.services.simpleemail.model._\n\n\nimport\n \ncom.amazonaws.services.sns._\n\n\nimport\n \ncom.amazonaws.services.sns.model._\n\n\n\nval\n \nc\n \n=\n \nnew\n \nAmazonSNSClient\n(\n\n    \nnew\n \nAWSCredentialsProviderChain\n(\n\n        \n// attempt on-instance credentials first\n\n        \nnew\n \nInstanceProfileCredentialsProvider\n(),\n\n        \n// fallback to aws credentials file\n\n        \nnew\n \nProfileCredentialsProvider\n()\n\n    \n)\n\n\n)\n\n\n\nc\n.\nsetEndpoint\n(\nsns.us-east-1.amazonaws.com\n)\n\n\n\nc\n.\ncreateTopic\n(\nses-email-bounce\n)\n\n\n\nres0\n:\n \ncom.amazonaws.services.sns.model.CreateTopicResult\n \n=\n \n{\nTopicArn\n:\n \narn:aws:sns:us-east-\n1\n:\n000000000000\n:ses-email-bounce\n}\n\n\n\nval\n \nc\n \n=\n \nnew\n \nAmazonSimpleEmailServiceClient\n(\n\n    \nnew\n \nAWSCredentialsProviderChain\n(\n\n        \n// attempt on-instance credentials first\n\n        \nnew\n \nInstanceProfileCredentialsProvider\n(),\n\n        \n// fallback to aws credentials file\n\n        \nnew\n \nProfileCredentialsProvider\n()\n\n    \n)\n\n\n)\n\n\n\nc\n.\nsetEndpoint\n(\nemail.us-east-1.amazonaws.com\n)\n\n\n\nc\n.\ngetSendQuota\n()\n\n\n\nres1\n:\n \ncom.amazonaws.services.simpleemail.model.GetSendQuotaResult\n \n=\n \n{\nMax24HourSend\n:\n \n100\n.\n0\n,\nMaxSendRate\n:\n \n10\n.\n0\n,\nSentLast24Hours\n:\n \n1\n.\n0\n}\n\n\n\nc\n.\ngetSendStatistics\n()\n\n\n\nres2\n:\n \ncom.amazonaws.services.simpleemail.model.GetSendStatisticsResult\n \n=\n \n{\nSendDataPoints\n:\n \n[\n{\nTimestamp:\n \n...\n,\nDeliveryAttempts:\n \n0\n,\nBounces\n:\n \n0\n,\nComplaints\n:\n \n0\n,\nRejects\n:\n \n0\n},\n \n...\n\n\n\nc\n.\ngetIdentityVerificationAttributes\n(\n\n    \nnew\n \nGetIdentityVerificationAttributesRequest\n()\n\n        \n.\nwithIdentities\n(\nsome.email@mydomain.com\n)\n\n\n)\n\n\n\nres3\n:\n \ncom.amazonaws.services.simpleemail.model.GetIdentityVerificationAttributesResult\n \n=\n \n{\nVerificationAttributes\n:\n \n{\nsome.email@mydomain.com\n={\nVerificationStatus:\n \nSuccess\n,}}}\n\n\n\nc\n.\nsetIdentityNotificationTopic\n(\n\n    \nnew\n \nSetIdentityNotificationTopicRequest\n()\n\n        \n.\nwithIdentity\n(\nsome.email@mydomain.com\n)\n\n        \n.\nwithNotificationType\n(\nBounce\n)\n\n        \n.\nwithSnsTopic\n(\narn:aws:sns:us-east-1:000000000000:ses-email-bounce\n)\n\n\n)\n\n\n\nc\n.\ngetIdentityNotificationAttributes\n(\n\n    \nnew\n \nGetIdentityNotificationAttributesRequest\n()\n\n        \n.\nwithIdentities\n(\nsome.email@mydomain.com\n)\n\n\n)\n\n\n\n\n\n\nOne of the nice things about using the Scala REPL is that you get some useful tab completions (SES\nobject here):\n\n\nscala\n c.\naddRequestHandler                   sendEmail\nasInstanceOf                        sendRawEmail\ndeleteIdentity                      setConfiguration\ndeleteVerifiedEmailAddress          setEndpoint\ngetCachedResponseMetadata           setIdentityDkimEnabled\ngetIdentityDkimAttributes           setIdentityFeedbackForwardingEnabled\ngetIdentityNotificationAttributes   setIdentityNotificationTopic\ngetIdentityVerificationAttributes   setRegion\ngetRequestMetricsCollector          setServiceNameIntern\ngetSendQuota                        setSignerRegionOverride\ngetSendStatistics                   setTimeOffset\ngetServiceName                      shutdown\ngetSignerByURI                      toString\ngetSignerRegionOverride             verifyDomainDkim\ngetTimeOffset                       verifyDomainIdentity\nisInstanceOf                        verifyEmailAddress\nlistIdentities                      verifyEmailIdentity\nlistVerifiedEmailAddresses          withTimeOffset\nremoveRequestHandler\n\n\n\n\n\nOnce you have a feel for how the Scala REPL works with a library, you can create alternate entry\npoint for sbt called \nscalas\n which allows you to write full Scala scripts with library\ndependencies.  See \nScripts, REPL, and Dependencies\n\nfor additional details.  In brief, create a \nscalas\n script and make it available on your \nPATH\n:\n\n\n#!/bin/sh\n\n\ntest\n -f ~/.sbtconfig \n . ~/.sbtconfig\n\nexec\n java -Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize\n=\n256M \n${\nSBT_OPTS\n}\n -jar /usr/local/Cellar/sbt/0.13.2/libexec/sbt-launch.jar -Dsbt.main.class\n=\nsbt.ScriptMain \n$@\n\n\n\n\n\n\nYou can then write Scala scripts like so:\n\n\n#!/\nusr\n/\nbin\n/\nenv\n \nscalas\n\n\n\n/***\n\n\n    scalaVersion := \n2.11.0\n\n\n\n    libraryDependencies ++= Seq(\n\n\n        \ncommons-logging\n % \ncommons-logging\n % \n1.1.3\n,\n\n\n        \ncom.amazonaws\n % \naws-java-sdk\n % \n1.8.2\n\n\n    )\n\n\n*/\n\n\n\nimport\n \ncom.amazonaws.auth._\n\n\nimport\n \ncom.amazonaws.auth.profile._\n\n\nimport\n \ncom.amazonaws.services.simpleemail._\n\n\nimport\n \ncom.amazonaws.services.simpleemail.model._\n\n\n\nval\n \nc\n \n=\n \nnew\n \nAmazonSimpleEmailServiceClient\n(\n\n    \nnew\n \nAWSCredentialsProviderChain\n(\n\n        \n// attempt on-instance credentials first\n\n        \nnew\n \nInstanceProfileCredentialsProvider\n(),\n\n        \n// fallback to aws credentials file\n\n        \nnew\n \nProfileCredentialsProvider\n()\n\n    \n)\n\n\n)\n\n\n\nc\n.\nsetEndpoint\n(\nemail.us-east-1.amazonaws.com\n)\n\n\n\nprintln\n(\nc\n.\ngetSendQuota\n())\n\n\n\nprintln\n(\nc\n.\ngetSendStatistics\n().\ngetSendDataPoints\n.\nget\n(\n0\n))\n\n\n\nprintln\n(\nc\n.\ngetSendStatistics\n().\ngetSendDataPoints\n.\nsize\n)\n\n\n\n\n\n\nExecute the script as follows:\n\n\n$ ./demo.scala\n.\n.\n.\n\n{\nTimestamp: Sat Jul \n05\n \n04\n:01:00 PDT \n2014\n,DeliveryAttempts: \n3\n,Bounces: \n0\n,Complaints: \n0\n,Rejects: \n0\n}\n\n\n1301", 
            "title": "Using the Scala REPL to Configure Amazon SES Notifications"
        }, 
        {
            "location": "/scala/using-the-scala-repl-to-configure-amazon-ses-notifications/#using-the-scala-repl-to-configure-amazon-ses-notifications", 
            "text": "2014-06-30  Let's say you want to configure bounce notifications for SES emails using the Scala REPL.  How do\nyou go about accessing the AWS API to make it happen?  Make sure to have the AWS SDK for Java API Reference  available\nfor additional details.  Install Scala and SBT:  brew install scala sbt  Create an AWS credentials file at  ~/.aws/credentials  that is formatted like so:  [default]  aws_access_key_id = ...  aws_secret_access_key = ...  aws_session_token = ...   Create a  build.sbt  file that looks like:  name   :=   aws-sdk-client  version   :=   1.0  scalaVersion   :=   2.11.0  libraryDependencies   ++=   Seq ( \n     commons-logging   %   commons-logging   %   1.1.3 , \n     com.amazonaws   %   aws-java-sdk   %   1.8.2  )   You can then engage the Scala REPL to talk to the AWS API like so (Scala will download the AWS SDK\nfrom Maven Central automatically):  sbt\nconsole  import   com.amazonaws.auth._  import   com.amazonaws.auth.profile._  import   com.amazonaws.services.simpleemail._  import   com.amazonaws.services.simpleemail.model._  import   com.amazonaws.services.sns._  import   com.amazonaws.services.sns.model._  val   c   =   new   AmazonSNSClient ( \n     new   AWSCredentialsProviderChain ( \n         // attempt on-instance credentials first \n         new   InstanceProfileCredentialsProvider (), \n         // fallback to aws credentials file \n         new   ProfileCredentialsProvider () \n     )  )  c . setEndpoint ( sns.us-east-1.amazonaws.com )  c . createTopic ( ses-email-bounce )  res0 :   com.amazonaws.services.sns.model.CreateTopicResult   =   { TopicArn :   arn:aws:sns:us-east- 1 : 000000000000 :ses-email-bounce }  val   c   =   new   AmazonSimpleEmailServiceClient ( \n     new   AWSCredentialsProviderChain ( \n         // attempt on-instance credentials first \n         new   InstanceProfileCredentialsProvider (), \n         // fallback to aws credentials file \n         new   ProfileCredentialsProvider () \n     )  )  c . setEndpoint ( email.us-east-1.amazonaws.com )  c . getSendQuota ()  res1 :   com.amazonaws.services.simpleemail.model.GetSendQuotaResult   =   { Max24HourSend :   100 . 0 , MaxSendRate :   10 . 0 , SentLast24Hours :   1 . 0 }  c . getSendStatistics ()  res2 :   com.amazonaws.services.simpleemail.model.GetSendStatisticsResult   =   { SendDataPoints :   [ { Timestamp:   ... , DeliveryAttempts:   0 , Bounces :   0 , Complaints :   0 , Rejects :   0 },   ...  c . getIdentityVerificationAttributes ( \n     new   GetIdentityVerificationAttributesRequest () \n         . withIdentities ( some.email@mydomain.com )  )  res3 :   com.amazonaws.services.simpleemail.model.GetIdentityVerificationAttributesResult   =   { VerificationAttributes :   { some.email@mydomain.com ={ VerificationStatus:   Success ,}}}  c . setIdentityNotificationTopic ( \n     new   SetIdentityNotificationTopicRequest () \n         . withIdentity ( some.email@mydomain.com ) \n         . withNotificationType ( Bounce ) \n         . withSnsTopic ( arn:aws:sns:us-east-1:000000000000:ses-email-bounce )  )  c . getIdentityNotificationAttributes ( \n     new   GetIdentityNotificationAttributesRequest () \n         . withIdentities ( some.email@mydomain.com )  )   One of the nice things about using the Scala REPL is that you get some useful tab completions (SES\nobject here):  scala  c.\naddRequestHandler                   sendEmail\nasInstanceOf                        sendRawEmail\ndeleteIdentity                      setConfiguration\ndeleteVerifiedEmailAddress          setEndpoint\ngetCachedResponseMetadata           setIdentityDkimEnabled\ngetIdentityDkimAttributes           setIdentityFeedbackForwardingEnabled\ngetIdentityNotificationAttributes   setIdentityNotificationTopic\ngetIdentityVerificationAttributes   setRegion\ngetRequestMetricsCollector          setServiceNameIntern\ngetSendQuota                        setSignerRegionOverride\ngetSendStatistics                   setTimeOffset\ngetServiceName                      shutdown\ngetSignerByURI                      toString\ngetSignerRegionOverride             verifyDomainDkim\ngetTimeOffset                       verifyDomainIdentity\nisInstanceOf                        verifyEmailAddress\nlistIdentities                      verifyEmailIdentity\nlistVerifiedEmailAddresses          withTimeOffset\nremoveRequestHandler  Once you have a feel for how the Scala REPL works with a library, you can create alternate entry\npoint for sbt called  scalas  which allows you to write full Scala scripts with library\ndependencies.  See  Scripts, REPL, and Dependencies \nfor additional details.  In brief, create a  scalas  script and make it available on your  PATH :  #!/bin/sh  test  -f ~/.sbtconfig   . ~/.sbtconfig exec  java -Xms512M -Xmx1536M -Xss1M -XX:+CMSClassUnloadingEnabled -XX:MaxPermSize = 256M  ${ SBT_OPTS }  -jar /usr/local/Cellar/sbt/0.13.2/libexec/sbt-launch.jar -Dsbt.main.class = sbt.ScriptMain  $@   You can then write Scala scripts like so:  #!/ usr / bin / env   scalas  /***      scalaVersion :=  2.11.0      libraryDependencies ++= Seq(           commons-logging  %  commons-logging  %  1.1.3 ,           com.amazonaws  %  aws-java-sdk  %  1.8.2      )  */  import   com.amazonaws.auth._  import   com.amazonaws.auth.profile._  import   com.amazonaws.services.simpleemail._  import   com.amazonaws.services.simpleemail.model._  val   c   =   new   AmazonSimpleEmailServiceClient ( \n     new   AWSCredentialsProviderChain ( \n         // attempt on-instance credentials first \n         new   InstanceProfileCredentialsProvider (), \n         // fallback to aws credentials file \n         new   ProfileCredentialsProvider () \n     )  )  c . setEndpoint ( email.us-east-1.amazonaws.com )  println ( c . getSendQuota ())  println ( c . getSendStatistics (). getSendDataPoints . get ( 0 ))  println ( c . getSendStatistics (). getSendDataPoints . size )   Execute the script as follows:  $ ./demo.scala\n.\n.\n. { Timestamp: Sat Jul  05   04 :01:00 PDT  2014 ,DeliveryAttempts:  3 ,Bounces:  0 ,Complaints:  0 ,Rejects:  0 }  1301", 
            "title": "Using the Scala REPL to Configure Amazon SES Notifications"
        }, 
        {
            "location": "/shell/measuring-transfer-speed-over-time-with-curl/", 
            "text": "Measuring Transfer Speed Over Time with cURL\n\n\n2014-07-02\n\n\nOrdinarily when you run the \ncURL\n command to download a file, you see a\nprogress meter that updates every second.\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   346  100   346    0     0    422      0 --:--:-- --:--:-- --:--:--   422\n  4  635M    4 29.8M    0     0  1793k      0  0:06:02  0:00:17  0:05:45 2394k\n\n\n\n\n\nThis progress meter is written to stderr and if you were to redirect both stderr and stdout to a\nfile and then run \ntail -f\n on that file, you would see the exact same progress meter being updated\nonce per second, with no running log of download speed.  The reason that this output updates in\nplace is because the program is writing a carriage return \n\\r\n at the end of the progress line\ninstead of a newline \n\\n\n.  This causes the cursor to return to the beginning of the line without\nadvancing.\n\n\nWith the knowledge of how this operates, it is possible to alter the output of the cURL command to\nsave the per-second speed of a download.  If you further send the results of a large file download\nto \n/dev/null\n, then you have a reasonable approximation of of a speedtest tool and you can graph\nthe download speed over time.  The command below uses \ntr\n to rewrite carriage returns as newlines\nin an unbuffered manner, so that data is instantly available in the output file.\n\n\nAs an aside on the power of the \ntr command\n, the\n\nMore Shell, Less Egg\n blog post by\n\nDr. Drang\n discusses a programming\nchallenge proposed to \nDonald Knuth\n, who solved it with\n~10 pages of literate Pascal, and \nDoug McIlroy\n who\ncritiqued the solution and provided an alternative solution in six shell commands.\n\n\nURL\n=\nhttp://cdimage.debian.org/debian-cd/7.5.0/amd64/iso-cd/debian-7.5.0-amd64-CD-1.iso\n\n\ncurl -L -o /dev/null \n$URL\n \n2\n1\n \n\\\n\n  \n|\ntr -u \n\\r\n \n\\n\n \n curl.out\n\n\n\n\n\nThis results in an output file that looks like this:\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   346  100   346    0     0    295      0  0:00:01  0:00:01 --:--:--   295\n\n  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n  0  635M    0 70871    0     0  12988      0 14:14:26  0:00:05 14:14:21 17260\n  0  635M    0  608k    0     0  97534      0  1:53:46  0:00:06  1:53:40  120k\n  0  635M    0 1489k    0     0   201k      0  0:53:41  0:00:07  0:53:34  296k\n  0  635M    0 2742k    0     0   328k      0  0:33:00  0:00:08  0:32:52  548k\n  0  635M    0 4297k    0     0   456k      0  0:23:43  0:00:09  0:23:34  849k\n  0  635M    0 6015k    0     0   580k      0  0:18:40  0:00:10  0:18:30 1210k\n  1  635M    1 8014k    0     0   701k      0  0:15:27  0:00:11  0:15:16 1471k\n  1  635M    1 10.0M    0     0   827k      0  0:13:05  0:00:12  0:12:53 1749k\n  1  635M    1 11.0M    0     0   841k      0  0:12:52  0:00:13  0:12:39 1682k\n  .\n  .\n  .\n\n\n\n\n\nWrite a Python script \nplot_curl_data.py\n to process the data to convert it into a format useful for\n\ngnuplot\n and render a plot:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n#!/usr/bin/env python\n\n\n\nimport\n \nos\n,\n \nsys\n\n\n\n\ndef\n \nreadCurlData\n(\nfname\n):\n\n    \nlines\n \n=\n \n[]\n\n    \nwith\n \nopen\n(\nfname\n)\n \nas\n \nf\n:\n\n        \nfor\n \nline\n \nin\n \nf\n:\n\n            \nlines\n.\nappend\n(\nline\n.\nsplit\n())\n\n    \nreturn\n \nlines\n[\n3\n:]\n\n\n\n\ndef\n \nconvertUnits\n(\nlines\n):\n\n    \nconverted\n \n=\n \n[]\n\n    \nfor\n \nline\n \nin\n \nlines\n:\n\n        \nif\n \nlen\n(\nline\n)\n \n==\n \n12\n \nand\n \nnot\n \n--\n \nin\n \nline\n[\n9\n]:\n\n            \n# curl reports speed in bytes per second\n\n            \nif\n \nk\n \nin\n \nline\n[\n11\n]:\n\n                \nline\n[\n11\n]\n \n=\n \nstr\n(\nint\n(\nline\n[\n11\n]\n.\nreplace\n(\nk\n,\n))\n \n*\n \n8\n \n*\n \n1024\n)\n\n            \nelif\n \nM\n \nin\n \nline\n[\n11\n]:\n\n                \nline\n[\n11\n]\n \n=\n \nstr\n(\nint\n(\nline\n[\n11\n]\n.\nreplace\n(\nM\n,\n))\n \n*\n \n8\n \n*\n \n1048576\n)\n\n            \nelif\n \nG\n \nin\n \nline\n[\n11\n]:\n\n                \nline\n[\n11\n]\n \n=\n \nstr\n(\nint\n(\nline\n[\n11\n]\n.\nreplace\n(\nG\n,\n))\n \n*\n \n8\n \n*\n \n1073741824\n)\n\n            \nconverted\n.\nappend\n([\nline\n[\n9\n],\n \nline\n[\n11\n]])\n\n    \nreturn\n \nconverted\n\n\n\n\ndef\n \nwriteGnuplotData\n(\nfname\n,\n \nlines\n):\n\n    \nfname\n \n=\n \nfname\n \n+\n \n.gnuplot.data\n\n    \nwith\n \nopen\n(\nfname\n,\n \nw\n)\n \nas\n \nf\n:\n\n        \nfor\n \nline\n \nin\n \nlines\n:\n\n            \nf\n.\nwrite\n(\n,\n.\njoin\n(\nline\n)\n \n+\n \n\\n\n)\n\n\n\n\ndef\n \nplot\n(\nfname\n):\n\n    \ngp_fname\n \n=\n \nfname\n \n+\n \n.gp\n\n    \ngpdata_fname\n \n=\n \nfname\n \n+\n \n.gnuplot.data\n\n    \npng_fname\n \n=\n \nfname\n \n+\n \n.png\n\n\n    \nf\n \n=\n \nopen\n(\ngp_fname\n,\n \nw\n)\n\n    \nf\n.\nwrite\n(\nset output \n%s\n\\n\n \n%\n \npng_fname\n)\n\n    \nf\n.\nwrite\n(\nset datafile separator \n,\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset terminal png size 1400,800\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset title \nDownload Speed\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset ylabel \nSpeed (Mbits/s)\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset xlabel \nTime (seconds)\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset xdata time\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset timefmt \n%H:%M:%S\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset key outside\n\\n\n)\n\n    \nf\n.\nwrite\n(\nset grid\n\\n\n)\n\n    \nf\n.\nwrite\n(\nplot \n\\\\\\n\n)\n\n    \nf\n.\nwrite\n(\n%s\n using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \nspeed\n\\n\n \n%\n \ngpdata_fname\n)\n\n    \nf\n.\nclose\n()\n\n\n    \nos\n.\nsystem\n(\ngnuplot \n%s\n \n%\n \ngp_fname\n)\n\n\n\n\nif\n \nlen\n(\nsys\n.\nargv\n)\n \n \n2\n:\n\n    \nprint\n \nUsage: \n%s\n [curl_data_filename]\n \n%\n \nsys\n.\nargv\n[\n0\n]\n\n    \nexit\n(\n1\n)\n\n\nelse\n:\n\n    \nlines\n \n=\n \nreadCurlData\n(\nsys\n.\nargv\n[\n1\n])\n\n    \nlines\n \n=\n \nconvertUnits\n(\nlines\n)\n\n    \nwriteGnuplotData\n(\nsys\n.\nargv\n[\n1\n],\n \nlines\n)\n\n    \nplot\n(\nsys\n.\nargv\n[\n1\n])\n\n\n\n\n\n\n\nRun this script like so:\n\n\n./plot_curl_data.py curl.out\n\n\n\n\n\nYou will end up with data (\ncurl.out.gp.data\n) and configuration files (\ncurl.out.gp\n) like so:\n\n\n0:00:01,295\n0:00:01,0\n0:00:02,0\n0:00:03,0\n0:00:04,0\n0:00:05,17260\n0:00:06,983040\n0:00:07,2424832\n0:00:08,4489216\n0:00:09,6955008\n0:00:10,9912320\n0:00:11,12050432\n0:00:12,14327808\n0:00:13,13778944\n0:00:14,12173312\n.\n.\n.\n\n\n\n\n\nset output \ncurl.out.png\n\nset datafile separator \n,\n\nset terminal png size 1400,800\nset title \nDownload Speed\n\nset ylabel \nSpeed (Mbits/s)\n\nset xlabel \nTime (seconds)\n\nset xdata time\nset timefmt \n%H:%M:%S\n\nset key outside\nset grid\nplot \\\n\ncurl.out.gp.data\n using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \nspeed\n\n\n\n\n\n\nThe graph will be rendered in PNG format:", 
            "title": "Measuring Transfer Speed Over Time with cURL"
        }, 
        {
            "location": "/shell/measuring-transfer-speed-over-time-with-curl/#measuring-transfer-speed-over-time-with-curl", 
            "text": "2014-07-02  Ordinarily when you run the  cURL  command to download a file, you see a\nprogress meter that updates every second.    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   346  100   346    0     0    422      0 --:--:-- --:--:-- --:--:--   422\n  4  635M    4 29.8M    0     0  1793k      0  0:06:02  0:00:17  0:05:45 2394k  This progress meter is written to stderr and if you were to redirect both stderr and stdout to a\nfile and then run  tail -f  on that file, you would see the exact same progress meter being updated\nonce per second, with no running log of download speed.  The reason that this output updates in\nplace is because the program is writing a carriage return  \\r  at the end of the progress line\ninstead of a newline  \\n .  This causes the cursor to return to the beginning of the line without\nadvancing.  With the knowledge of how this operates, it is possible to alter the output of the cURL command to\nsave the per-second speed of a download.  If you further send the results of a large file download\nto  /dev/null , then you have a reasonable approximation of of a speedtest tool and you can graph\nthe download speed over time.  The command below uses  tr  to rewrite carriage returns as newlines\nin an unbuffered manner, so that data is instantly available in the output file.  As an aside on the power of the  tr command , the More Shell, Less Egg  blog post by Dr. Drang  discusses a programming\nchallenge proposed to  Donald Knuth , who solved it with\n~10 pages of literate Pascal, and  Doug McIlroy  who\ncritiqued the solution and provided an alternative solution in six shell commands.  URL = http://cdimage.debian.org/debian-cd/7.5.0/amd64/iso-cd/debian-7.5.0-amd64-CD-1.iso \n\ncurl -L -o /dev/null  $URL   2 1   \\ \n   | tr -u  \\r   \\n    curl.out  This results in an output file that looks like this:    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   346  100   346    0     0    295      0  0:00:01  0:00:01 --:--:--   295\n\n  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n  0  635M    0 70871    0     0  12988      0 14:14:26  0:00:05 14:14:21 17260\n  0  635M    0  608k    0     0  97534      0  1:53:46  0:00:06  1:53:40  120k\n  0  635M    0 1489k    0     0   201k      0  0:53:41  0:00:07  0:53:34  296k\n  0  635M    0 2742k    0     0   328k      0  0:33:00  0:00:08  0:32:52  548k\n  0  635M    0 4297k    0     0   456k      0  0:23:43  0:00:09  0:23:34  849k\n  0  635M    0 6015k    0     0   580k      0  0:18:40  0:00:10  0:18:30 1210k\n  1  635M    1 8014k    0     0   701k      0  0:15:27  0:00:11  0:15:16 1471k\n  1  635M    1 10.0M    0     0   827k      0  0:13:05  0:00:12  0:12:53 1749k\n  1  635M    1 11.0M    0     0   841k      0  0:12:52  0:00:13  0:12:39 1682k\n  .\n  .\n  .  Write a Python script  plot_curl_data.py  to process the data to convert it into a format useful for gnuplot  and render a plot:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66 #!/usr/bin/env python  import   os ,   sys  def   readCurlData ( fname ): \n     lines   =   [] \n     with   open ( fname )   as   f : \n         for   line   in   f : \n             lines . append ( line . split ()) \n     return   lines [ 3 :]  def   convertUnits ( lines ): \n     converted   =   [] \n     for   line   in   lines : \n         if   len ( line )   ==   12   and   not   --   in   line [ 9 ]: \n             # curl reports speed in bytes per second \n             if   k   in   line [ 11 ]: \n                 line [ 11 ]   =   str ( int ( line [ 11 ] . replace ( k , ))   *   8   *   1024 ) \n             elif   M   in   line [ 11 ]: \n                 line [ 11 ]   =   str ( int ( line [ 11 ] . replace ( M , ))   *   8   *   1048576 ) \n             elif   G   in   line [ 11 ]: \n                 line [ 11 ]   =   str ( int ( line [ 11 ] . replace ( G , ))   *   8   *   1073741824 ) \n             converted . append ([ line [ 9 ],   line [ 11 ]]) \n     return   converted  def   writeGnuplotData ( fname ,   lines ): \n     fname   =   fname   +   .gnuplot.data \n     with   open ( fname ,   w )   as   f : \n         for   line   in   lines : \n             f . write ( , . join ( line )   +   \\n )  def   plot ( fname ): \n     gp_fname   =   fname   +   .gp \n     gpdata_fname   =   fname   +   .gnuplot.data \n     png_fname   =   fname   +   .png \n\n     f   =   open ( gp_fname ,   w ) \n     f . write ( set output  %s \\n   %   png_fname ) \n     f . write ( set datafile separator  , \\n ) \n     f . write ( set terminal png size 1400,800 \\n ) \n     f . write ( set title  Download Speed \\n ) \n     f . write ( set ylabel  Speed (Mbits/s) \\n ) \n     f . write ( set xlabel  Time (seconds) \\n ) \n     f . write ( set xdata time \\n ) \n     f . write ( set timefmt  %H:%M:%S \\n ) \n     f . write ( set key outside \\n ) \n     f . write ( set grid \\n ) \n     f . write ( plot  \\\\\\n ) \n     f . write ( %s  using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title  speed \\n   %   gpdata_fname ) \n     f . close () \n\n     os . system ( gnuplot  %s   %   gp_fname )  if   len ( sys . argv )     2 : \n     print   Usage:  %s  [curl_data_filename]   %   sys . argv [ 0 ] \n     exit ( 1 )  else : \n     lines   =   readCurlData ( sys . argv [ 1 ]) \n     lines   =   convertUnits ( lines ) \n     writeGnuplotData ( sys . argv [ 1 ],   lines ) \n     plot ( sys . argv [ 1 ])    Run this script like so:  ./plot_curl_data.py curl.out  You will end up with data ( curl.out.gp.data ) and configuration files ( curl.out.gp ) like so:  0:00:01,295\n0:00:01,0\n0:00:02,0\n0:00:03,0\n0:00:04,0\n0:00:05,17260\n0:00:06,983040\n0:00:07,2424832\n0:00:08,4489216\n0:00:09,6955008\n0:00:10,9912320\n0:00:11,12050432\n0:00:12,14327808\n0:00:13,13778944\n0:00:14,12173312\n.\n.\n.  set output  curl.out.png \nset datafile separator  , \nset terminal png size 1400,800\nset title  Download Speed \nset ylabel  Speed (Mbits/s) \nset xlabel  Time (seconds) \nset xdata time\nset timefmt  %H:%M:%S \nset key outside\nset grid\nplot \\ curl.out.gp.data  using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title  speed   The graph will be rendered in PNG format:", 
            "title": "Measuring Transfer Speed Over Time with cURL"
        }, 
        {
            "location": "/shell/parsing-json-on-the-command-line/", 
            "text": "Parsing JSON on the Command Line\n\n\n2014-07-16\n\n\nWith more APIs moving to JSON, being able to parse it at the command line allows you to write more\nsophisticated shell scripts that can interact with your favorite services.  Most first attempts at\nJSON parsing using some variation of the following to get the job done, which initially seems\nreasonable:\n\n\ncurl -s http://endpoint.info \n\\\n\n    \n|\npython -mjson.tool \n\\\n\n    \n|\ngrep foo \n\\\n\n    \n|\ncut -d: -f2 \n\\\n\n    \n|\nsed -e \ns/\n//g\n\n\n\n\n\n\nHowever, this can rapidly get out of hand, if you have key duplication, complex nested structures or\nyou need to pull in all of the elements of a list.  For awhile,\n\nunderscore-cli\n was my favorite fully-featured JSON\nparser, but I found its documentation somewhat lacking and it hasn't seen serious development since\nNovember 2012.  Since then, I found \njq\n which has a beautiful,\nwell-written \nmanual\n with many usage examples and it is under\nactive development.  It also has the benefit of being written in C, which helps speed and it has a\nfairly concise descriptor language.  To install:\n\n\nbrew install jq\n\n\n\n\n\nThen you can do things like flattening a complex JSON structure into a simple CSV:\n\n\nPAYLOADS\n=\n$(\n curl -s \n$URL\n \n|\njq \n.payloads\n \n)\n\n\nif\n \n[\n \n$PAYLOADS\n !\n=\n \n[]\n \n]\n;\n \nthen\n\n    \necho\n \n$PAYLOADS\n \n\\\n\n        \n|\n jq -r \n.[] | .minutes[].payload.items[] | [.actions.actionTime, (.actions | {actions} | .actions.email.state), .sourceInstance, (.actions | {actions} | .actions.email.info )] | @csv\n\n\nfi\n\n\n\n\n\n\nAs a bonus feature, if you have to deal in XML rather than JSON, then\n\nxmlstarlet\n is a good choice for handling it.  Naturally,\ninstallation:\n\n\nbrew install xmlstarlet\n\n\n\n\n\nOnce you have xmlstarlet, you can do things like pull the build number out of an Atlassian Bamboo\nHTML page, which was necessary when they did not have an API call available to report the version\nnumber of the last known good build for a project:\n\n\ncurl -s --insecure https://bamboo.local/browse/\n${\nBUILDKEY\n}${\nBUILD\n}\n \n\\\n\n    \n|\ntidy -asxhtml -numeric --force-output \ntrue\n \n2\n/dev/null \n\\\n\n    \n|\nxmlstarlet sel -N \nx\n=\nhttp://www.w3.org/1999/xhtml\n -t -m \n//x:div[@id=\nsr-build\n]/x:h2/x:a\n -v \n.\n \n\\\n\n    \n|\nsed -e \ns/#//g", 
            "title": "Parsing JSON on the Command Line"
        }, 
        {
            "location": "/shell/parsing-json-on-the-command-line/#parsing-json-on-the-command-line", 
            "text": "2014-07-16  With more APIs moving to JSON, being able to parse it at the command line allows you to write more\nsophisticated shell scripts that can interact with your favorite services.  Most first attempts at\nJSON parsing using some variation of the following to get the job done, which initially seems\nreasonable:  curl -s http://endpoint.info  \\ \n     | python -mjson.tool  \\ \n     | grep foo  \\ \n     | cut -d: -f2  \\ \n     | sed -e  s/ //g   However, this can rapidly get out of hand, if you have key duplication, complex nested structures or\nyou need to pull in all of the elements of a list.  For awhile, underscore-cli  was my favorite fully-featured JSON\nparser, but I found its documentation somewhat lacking and it hasn't seen serious development since\nNovember 2012.  Since then, I found  jq  which has a beautiful,\nwell-written  manual  with many usage examples and it is under\nactive development.  It also has the benefit of being written in C, which helps speed and it has a\nfairly concise descriptor language.  To install:  brew install jq  Then you can do things like flattening a complex JSON structure into a simple CSV:  PAYLOADS = $(  curl -s  $URL   | jq  .payloads   )  if   [   $PAYLOADS  ! =   []   ] ;   then \n     echo   $PAYLOADS   \\ \n         |  jq -r  .[] | .minutes[].payload.items[] | [.actions.actionTime, (.actions | {actions} | .actions.email.state), .sourceInstance, (.actions | {actions} | .actions.email.info )] | @csv  fi   As a bonus feature, if you have to deal in XML rather than JSON, then xmlstarlet  is a good choice for handling it.  Naturally,\ninstallation:  brew install xmlstarlet  Once you have xmlstarlet, you can do things like pull the build number out of an Atlassian Bamboo\nHTML page, which was necessary when they did not have an API call available to report the version\nnumber of the last known good build for a project:  curl -s --insecure https://bamboo.local/browse/ ${ BUILDKEY }${ BUILD }   \\ \n     | tidy -asxhtml -numeric --force-output  true   2 /dev/null  \\ \n     | xmlstarlet sel -N  x = http://www.w3.org/1999/xhtml  -t -m  //x:div[@id= sr-build ]/x:h2/x:a  -v  .   \\ \n     | sed -e  s/#//g", 
            "title": "Parsing JSON on the Command Line"
        }, 
        {
            "location": "/talks/netflix-atlas-telemetry-a-platform-begets-an-ecosystem/", 
            "text": "Netflix Atlas Telemetry - A Platform Begets an Ecosystem", 
            "title": "Netflix Atlas Telemetry - A Platform Begets an Ecosystem"
        }, 
        {
            "location": "/talks/netflix-atlas-telemetry-a-platform-begets-an-ecosystem/#netflix-atlas-telemetry-a-platform-begets-an-ecosystem", 
            "text": "", 
            "title": "Netflix Atlas Telemetry - A Platform Begets an Ecosystem"
        }
    ]
}